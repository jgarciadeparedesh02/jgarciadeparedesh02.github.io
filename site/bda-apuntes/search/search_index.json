{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83c\udf89 \u00a1Bienvenidos a Big Data Aplicado! \ud83c\udf1f","text":"<p>\ud83d\ude80 Especializaci\u00f3n en Inteligencia Artificial y Big Data</p> <p>\u00a1Hola a todos! \ud83d\udc4b Me complace darles la bienvenida al m\u00f3dulo de Big Data Aplicado dentro del curso de especializaci\u00f3n en Inteligencia Artificial y Big Data. En este m\u00f3dulo, nos adentraremos en el fascinante mundo del Big Data y aprenderemos a usar herramientas clave como Apache Hadoop para manejar y analizar grandes vol\u00famenes de datos. \ud83c\udf10\ud83d\udcca</p> <p></p>"},{"location":"#que-aprenderemos","title":"\ud83c\udfaf \u00bfQu\u00e9 aprenderemos?","text":"<p>En este curso, exploraremos c\u00f3mo almacenar, procesar y analizar grandes cantidades de informaci\u00f3n utilizando Hadoop y su extenso ecosistema. Estas tecnolog\u00edas est\u00e1n revolucionando industrias, permitiendo que las empresas aprovechen al m\u00e1ximo sus datos. \u00a1Prep\u00e1rate para convertirte en un experto en Big Data! \ud83d\ude80</p> <p>\ud83d\udcda \u00cdndice de Contenidos A lo largo de este m\u00f3dulo, profundizaremos en varias \u00e1reas clave del Big Data, desde conceptos introductorios hasta aplicaciones pr\u00e1cticas. Aqu\u00ed te dejamos una visi\u00f3n general de los contenidos:</p>"},{"location":"#unidad-1-introduccion-a-apache-hadoop","title":"Unidad 1: Introducci\u00f3n a Apache Hadoop","text":"<p>Comenzamos por lo b\u00e1sico: \u00bfQu\u00e9 es Hadoop y por qu\u00e9 es tan importante para el procesamiento de grandes datos?</p> <ul> <li>Motivaci\u00f3n y origen: Exploramos los problemas que dieron origen a Hadoop y c\u00f3mo se convirti\u00f3 en una soluci\u00f3n para el manejo de grandes vol\u00famenes de datos.</li> <li>Apache Hadoop a alto nivel: Nos familiarizamos con los conceptos clave de Hadoop y sus componentes principales.</li> <li>\u00bfQu\u00e9 es Apache Hadoop? Conoce qu\u00e9 es y por qu\u00e9 es crucial para el almacenamiento y procesamiento de grandes cantidades de datos.</li> <li>Ecosistema Hadoop y distribuciones: Descubre c\u00f3mo Hadoop es solo el n\u00facleo de un ecosistema m\u00e1s amplio, que incluye herramientas como Hive, Pig, y HBase.</li> <li>Arquitectura de Hadoop: Examinamos c\u00f3mo Hadoop organiza el procesamiento distribuido a trav\u00e9s de HDFS (Hadoop Distributed File System) y MapReduce.</li> <li>Beneficios, desventajas y dificultades: Analizamos las ventajas que ofrece Hadoop, pero tambi\u00e9n sus limitaciones y retos.</li> </ul>"},{"location":"#unidad-2-almacenamiento-y-procesamiento-en-hadoop","title":"Unidad 2: Almacenamiento y Procesamiento en Hadoop","text":"<p>Profundizamos en c\u00f3mo Hadoop almacena y procesa datos utilizando HDFS, YARN y MapReduce.</p> <ul> <li>HDFS (Hadoop Distributed File System): Es el sistema de archivos distribuido que permite almacenar grandes vol\u00famenes de datos de manera eficiente. Explicaremos c\u00f3mo funciona la lectura y escritura de datos en HDFS.</li> <li>Arquitectura de HDFS: Conoceremos la estructura interna de HDFS y c\u00f3mo los NameNodes y DataNodes se comunican para mantener la integridad de los datos.</li> <li>YARN (Yet Another Resource Negotiator): Esta herramienta gestiona los recursos dentro del cl\u00faster de Hadoop, permitiendo ejecutar m\u00faltiples tareas en paralelo.</li> <li>MapReduce: Modelo de programaci\u00f3n distribuida que divide las tareas en \"Map\" y \"Reduce\". Veremos c\u00f3mo se utiliza para procesar grandes cantidades de datos de manera eficiente y escalable.</li> </ul>"},{"location":"#unidad-3-ecosistema-hadoop","title":"Unidad 3: Ecosistema Hadoop","text":"<p>El ecosistema Hadoop es vasto y est\u00e1 lleno de herramientas dise\u00f1adas para facilitar el procesamiento y an\u00e1lisis de datos.</p> <ul> <li>Apache Pig: Un lenguaje de scripting dise\u00f1ado para analizar grandes conjuntos de datos de manera simple y eficiente.</li> <li>Apache Hive: Un sistema de data warehousing que permite realizar consultas sobre datos almacenados en Hadoop mediante un lenguaje similar a SQL, llamado HQL.</li> <li>Apache Impala: Herramienta de an\u00e1lisis que permite realizar consultas en tiempo real sobre grandes vol\u00famenes de datos.</li> <li>Apache HBase: Una base de datos NoSQL que permite el almacenamiento y acceso a datos estructurados de manera distribuida.</li> <li>Apache Spark: Un motor de procesamiento r\u00e1pido y general para grandes vol\u00famenes de datos, ideal para aplicaciones en tiempo real.</li> <li>Componentes de Ingesta de Datos: Exploraremos herramientas como Apache Sqoop y Apache Flume, dise\u00f1adas para la ingesta eficiente de datos desde fuentes externas a Hadoop.</li> <li>Apache Oozie: Una herramienta de flujo de trabajo que permite coordinar y programar trabajos dentro del ecosistema Hadoop.</li> <li>Procesamiento en Streaming: Veremos c\u00f3mo tecnolog\u00edas como Apache Spark (Structured Streaming), Apache Flink, y Apache Storm manejan datos en tiempo real, permitiendo an\u00e1lisis continuos.</li> </ul>"},{"location":"#unidad-4-administracion-y-monitorizacion-de-sistemas-hadoop","title":"Unidad 4: Administraci\u00f3n y Monitorizaci\u00f3n de Sistemas Hadoop","text":"<p>Gestionar un cl\u00faster Hadoop es crucial para asegurar que todo funcione correctamente, y en esta unidad aprenderemos las mejores pr\u00e1cticas y herramientas.</p> <ul> <li>Interfaz de HDFS y YARN: Exploraremos las interfaces de usuario que permiten monitorizar el funcionamiento del sistema de archivos y la gesti\u00f3n de recursos.</li> <li>Apache Ambari y Cloudera Manager: Dos herramientas poderosas que facilitan la administraci\u00f3n de cl\u00fasteres Hadoop.</li> <li>Ganglia: Un sistema de monitorizaci\u00f3n dise\u00f1ado para analizar el rendimiento y la eficiencia de un cl\u00faster Hadoop en tiempo real.</li> </ul>"},{"location":"#unidad-5-aplicacion-practica-de-tecnologias-big-data","title":"Unidad 5: Aplicaci\u00f3n Pr\u00e1ctica de Tecnolog\u00edas Big Data","text":"<p>Finalmente, aplicaremos los conocimientos adquiridos para resolver problemas reales utilizando tecnolog\u00edas Big Data.</p> <ul> <li>Arquitecturas y Modelos de Despliegue: Exploraremos diferentes arquitecturas de sistemas Big Data y sus modelos de despliegue, incluyendo soluciones on-premise y en la nube.</li> <li>Hadoop en la Pr\u00e1ctica: Veremos ejemplos de implementaci\u00f3n pr\u00e1ctica de Hadoop en diversos sectores y casos de uso.</li> <li>Soluciones Hadoop-as-a-Service: Exploraremos plataformas como Amazon EMR y Microsoft Azure HDInsight, que permiten implementar y gestionar soluciones Hadoop en la nube.</li> </ul>"},{"location":"#a-por-ello","title":"\u00a1A por ello!","text":"<p>Este curso te proporcionar\u00e1 las habilidades y conocimientos necesarios para dise\u00f1ar, implementar y administrar sistemas de Big Data. Al finalizar, ser\u00e1s capaz de gestionar grandes vol\u00famenes de datos de manera eficiente, desde el almacenamiento hasta el procesamiento avanzado. \ud83c\udfc6</p> <p>\ud83d\udca1 Consejo del d\u00eda: El mundo del Big Data est\u00e1 lleno de oportunidades. No dudes en preguntar, explorar y experimentar a lo largo del curso. \u00a1Estamos aqu\u00ed para ayudarte a cada paso del camino!</p> <p>\ud83d\udd25 \u00a1Vamos a dominar el Big Data, un cl\u00faster a la vez! \ud83d\udcaa Espero que est\u00e9s tan emocionado como yo para comenzar este viaje en el mundo del Big Data. \u00a1Es hora de hacer que los datos trabajen para nosotros! \ud83c\udf0d\ud83c\udf93</p>"},{"location":"docker/","title":"Introducci\u00f3n a Docker y Docker-compose \ud83d\udea2","text":""},{"location":"docker/#que-es-docker","title":"\ud83d\ude80 \u00bfQu\u00e9 es Docker?","text":"<p>Docker es una plataforma dise\u00f1ada para desarrollar, desplegar y ejecutar aplicaciones en contenedores. Un contenedor es un paquete liviano, port\u00e1til y aut\u00f3nomo que incluye todo lo necesario para ejecutar una aplicaci\u00f3n: c\u00f3digo, runtime, bibliotecas y configuraciones. La principal ventaja de Docker es que permite garantizar que la aplicaci\u00f3n se ejecutar\u00e1 de la misma manera en cualquier entorno, ya sea tu m\u00e1quina local o un servidor en la nube.</p> <pre><code># Ejemplo de comando para ejecutar un contenedor Docker:\ndocker run -d -p 8080:80 --name mi_contenedor nginx\n</code></pre> <p>En el ejemplo anterior:</p> <ul> <li><code>-d</code> indica que el contenedor se ejecutar\u00e1 en segundo plano (modo detached).</li> <li><code>-p 8080:80</code> enlaza el puerto 8080 de tu m\u00e1quina local al puerto 80 del contenedor.</li> <li><code>--name mi_contenedor</code> le asigna un nombre al contenedor.</li> <li><code>nginx</code> es la imagen que estamos usando para crear el contenedor.</li> </ul>"},{"location":"docker/#componentes-clave-de-docker","title":"\ud83d\udee0 Componentes clave de Docker","text":"<ol> <li>Imagen: Es una plantilla de solo lectura que se utiliza para crear contenedores. Piensa en una imagen como una instant\u00e1nea de un sistema operativo m\u00e1s una aplicaci\u00f3n.</li> </ol> <p>Ejemplo: La imagen <code>nginx</code> es una plantilla que contiene el servidor web NGINX.</p> <ol> <li> <p>Contenedor: Es una instancia ejecutable de una imagen. Cada contenedor es aut\u00f3nomo y aislado.</p> </li> <li> <p>Dockerfile: Es un archivo de texto que contiene todas las instrucciones necesarias para construir una imagen. </p> </li> </ol> <p><pre><code># Ejemplo de un Dockerfile simple:\nFROM node:14\nWORKDIR /app\nCOPY . .\nRUN npm install\nCMD [\"npm\", \"start\"]\n</code></pre> El Dockerfile anterior:</p> <ul> <li>Usa la imagen base <code>node:14</code>.</li> <li>Copia el contenido del directorio actual en el contenedor.</li> <li>Instala las dependencias de Node.js.</li> <li> <p>Ejecuta el comando <code>npm start</code> para iniciar la aplicaci\u00f3n.</p> </li> <li> <p>Docker Hub: Es un registro p\u00fablico donde puedes encontrar im\u00e1genes creadas por otros usuarios o subir las tuyas propias.</p> </li> </ul>"},{"location":"docker/#que-es-docker-compose","title":"\ud83d\udce6 \u00bfQu\u00e9 es Docker-compose?","text":"<p>Docker-compose es una herramienta que te permite definir y ejecutar aplicaciones multi-contenedor. En lugar de ejecutar m\u00faltiples contenedores de forma manual, puedes utilizar un archivo <code>docker-compose.yml</code> para definir c\u00f3mo se deben configurar y ejecutar.</p> <pre><code># Ejemplo b\u00e1sico de un archivo docker-compose.yml:\nversion: '3'\nservices:\n  web:\n    image: nginx\n    ports:\n      - \"8080:80\"\n  db:\n    image: mysql\n    environment:\n      MYSQL_ROOT_PASSWORD: example\n</code></pre> <p>Este archivo YAML define dos servicios:</p> <ul> <li>web: que usa la imagen <code>nginx</code> y mapea el puerto 80 del contenedor al 8080 de la m\u00e1quina local.</li> <li>db: que usa la imagen <code>mysql</code> y define una contrase\u00f1a para el usuario root.</li> </ul> <p>Para iniciar los contenedores definidos en el archivo <code>docker-compose.yml</code>, simplemente ejecutas:</p> <pre><code>docker-compose up\n</code></pre>"},{"location":"docker/#ventajas-de-docker-y-docker-compose","title":"\ud83c\udf00 Ventajas de Docker y Docker-compose","text":"<ol> <li>Portabilidad: Los contenedores se ejecutan de la misma manera en cualquier entorno.</li> <li>Aislamiento: Cada contenedor es independiente, evitando conflictos entre aplicaciones.</li> <li>Escalabilidad: Docker-compose facilita la creaci\u00f3n de aplicaciones complejas al permitir la orquestaci\u00f3n de m\u00faltiples contenedores.</li> <li>Reproducibilidad: Con un <code>Dockerfile</code> y <code>docker-compose</code>, puedes recrear un entorno exacto f\u00e1cilmente.</li> </ol>"},{"location":"docker/#diferencia-entre-imagenes-y-contenedores-en-docker","title":"\ud83d\uddbc\ufe0f Diferencia entre Im\u00e1genes y Contenedores en Docker \ud83d\udea2","text":"<p>Una de las confusiones m\u00e1s comunes al empezar con Docker es la diferencia entre im\u00e1genes y contenedores. Vamos a aclarar esto:</p>"},{"location":"docker/#imagenes","title":"Im\u00e1genes \ud83d\udcf8","text":"<p>Una imagen en Docker es como una plantilla de solo lectura que contiene todo lo necesario para ejecutar una aplicaci\u00f3n, incluyendo:</p> <ul> <li>Sistema operativo (por ejemplo, Ubuntu, Alpine).</li> <li>C\u00f3digo de la aplicaci\u00f3n.</li> <li>Dependencias o librer\u00edas necesarias para que la aplicaci\u00f3n funcione.</li> <li>Configuraciones o variables de entorno.</li> </ul> <p>Las im\u00e1genes son est\u00e1ticas, lo que significa que no cambian una vez que se crean. Son utilizadas para generar contenedores, pero por s\u00ed solas no hacen nada. </p> <p>Piensa en una imagen como una receta o blueprint que no se ejecuta, pero que contiene todas las instrucciones necesarias para crear algo que s\u00ed lo har\u00e1.</p> <pre><code># Ejemplo: Listar las im\u00e1genes disponibles en tu sistema\ndocker images\n</code></pre> <p>Salida t\u00edpica:</p> <pre><code>REPOSITORY       TAG       IMAGE ID       CREATED        SIZE\nnginx            latest    d1a364dc548d   2 weeks ago    133MB\nmysql            5.7       2a3174d2e2f7   1 month ago    450MB\n</code></pre> <p>En el ejemplo anterior:</p> <ul> <li>La imagen <code>nginx</code> est\u00e1 lista para ser utilizada para crear un servidor web.</li> <li>La imagen <code>mysql</code> contiene la base de datos MySQL en su versi\u00f3n 5.7.</li> </ul>"},{"location":"docker/#contenedores","title":"Contenedores \ud83d\udef3\ufe0f","text":"<p>Un contenedor es una instancia en ejecuci\u00f3n de una imagen. Es como si tomaras la plantilla (imagen) y la ejecutaras para crear un entorno real donde tu aplicaci\u00f3n corre. Un contenedor:</p> <ul> <li>Est\u00e1 basado en una imagen.</li> <li>Es din\u00e1mico: puede ejecutar procesos, guardar datos, recibir tr\u00e1fico de red, etc.</li> <li>Puede ser creado, iniciado, detenido y destruido.</li> </ul> <p>Cada contenedor tiene su propio sistema de archivos y entorno de ejecuci\u00f3n aislado del resto. Esto significa que puedes tener m\u00faltiples contenedores ejecutando la misma imagen sin que interfieran entre s\u00ed.</p> <pre><code># Ejemplo: Listar los contenedores en ejecuci\u00f3n\ndocker ps\n</code></pre> <p>Salida t\u00edpica:</p> <pre><code>CONTAINER ID   IMAGE     COMMAND                  CREATED         STATUS         PORTS                  NAMES\na1b2c3d4e5f6   nginx     \"nginx -g 'daemon off\u2026   5 minutes ago   Up 5 minutes   0.0.0.0:8080-&gt;80/tcp   webserver\n</code></pre> <p>En este caso, el contenedor <code>webserver</code> fue creado a partir de la imagen <code>nginx</code> y est\u00e1 en ejecuci\u00f3n mapeando el puerto 80 al 8080 de la m\u00e1quina local.</p>"},{"location":"docker/#diferencia-esencial","title":"\ud83d\udcca Diferencia Esencial","text":"Im\u00e1genes Contenedores Son plantillas est\u00e1ticas. Son instancias en ejecuci\u00f3n basadas en una imagen. No pueden modificarse una vez creadas. Pueden cambiar mientras est\u00e1n en ejecuci\u00f3n (p.ej. archivos creados dentro del contenedor). Se almacenan localmente o en repositorios como Docker Hub. Pueden ser creados, iniciados, detenidos y destruidos. Son inmutables (no cambian). Son din\u00e1micos y pueden ejecutar procesos."},{"location":"docker/#metafora-restaurante-y-recetas","title":"\ud83e\uddd1\u200d\ud83c\udf73 Met\u00e1fora: Restaurante y Recetas","text":"<ul> <li>Imagen: Es como una receta. Tienes todos los ingredientes y pasos necesarios para preparar un plato, pero no puedes comer la receta.</li> <li>Contenedor: Es el plato servido en la mesa. Ya se ha preparado a partir de la receta (imagen) y est\u00e1 listo para que lo disfrutes (o en nuestro caso, para que la aplicaci\u00f3n se ejecute).</li> </ul> <p>\ud83d\udca1 Conclusi\u00f3n:</p> <ul> <li>Imagen = Plantilla (Receta).</li> <li>Contenedor = Ejecuci\u00f3n de la Imagen (Plato Listo).</li> </ul> <p>Las im\u00e1genes son el punto de partida, pero son los contenedores los que realmente ejecutan y manejan tu aplicaci\u00f3n. Cada vez que usas Docker para ejecutar algo, est\u00e1s creando uno o m\u00e1s contenedores basados en im\u00e1genes preexistentes.</p>"},{"location":"docker/#conclusion","title":"\ud83c\udfaf Conclusi\u00f3n","text":"<p>Docker y Docker-compose son herramientas poderosas que simplifican el desarrollo y despliegue de aplicaciones en diferentes entornos. No importa si eres un desarrollador que quiere probar su aplicaci\u00f3n localmente o si administras un servidor en producci\u00f3n, estas herramientas te ayudar\u00e1n a crear entornos confiables y escalables.</p> <p>\ud83d\udca1 Consejo: Comienza por crear un peque\u00f1o contenedor con Docker y luego explora c\u00f3mo usar Docker-compose para manejar aplicaciones m\u00e1s complejas con m\u00faltiples servicios.</p>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/","title":"Big Data: Motivaci\u00f3n, Almacenamiento y Procesamiento \ud83d\ude80\ud83d\udcca","text":"<p>El t\u00e9rmino Big Data ha ganado una gran relevancia en la era digital moderna. No se trata solo de manejar grandes vol\u00famenes de datos, sino de extraer valor y conocimiento de ellos para tomar decisiones informadas y estrat\u00e9gicas. Desde sus or\u00edgenes hasta su integraci\u00f3n con tecnolog\u00edas avanzadas como el Cloud Computing y la Inteligencia Artificial, el Big Data ha revolucionado la forma en que las organizaciones operan en m\u00faltiples sectores.</p>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#motivacion-del-big-data-y-su-origen","title":"\ud83c\udf1f Motivaci\u00f3n del Big Data y su Origen","text":"<p>El Big Data surgi\u00f3 como una respuesta natural a la creciente cantidad de datos generados por dispositivos digitales, redes sociales, sensores IoT, smartphones, y una multitud de otros dispositivos conectados a la red. Este crecimiento exponencial de datos oblig\u00f3 a las organizaciones a buscar soluciones para almacenar, procesar y analizar eficientemente esta informaci\u00f3n masiva.</p> <p>Empresas pioneras como Google, Facebook y Amazon fueron las primeras en desarrollar e implementar infraestructuras capaces de manejar cantidades inmensas de datos. Al hacerlo, comenzaron a descubrir patrones, comportamientos y tendencias que ofrec\u00edan insights valiosos para la toma de decisiones.</p>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#por-que-es-importante-el-big-data","title":"\u00bfPor qu\u00e9 es Importante el Big Data?","text":"<p>El Big Data ha sido un motor clave para la transformaci\u00f3n digital de las empresas. A trav\u00e9s del an\u00e1lisis masivo de datos, las organizaciones pueden descubrir patrones y relaciones que no eran visibles antes, optimizando su rendimiento y abriendo nuevas oportunidades de crecimiento. Las razones clave de su importancia incluyen:</p> <ol> <li> <p>Tomar Decisiones Basadas en Datos:     Gracias al an\u00e1lisis de grandes vol\u00famenes de datos, las empresas pueden identificar patrones y tendencias que antes eran dif\u00edciles de detectar. Esto permite tomar decisiones m\u00e1s r\u00e1pidas y acertadas. Por ejemplo, una empresa minorista puede analizar millones de transacciones de clientes para identificar qu\u00e9 productos tienen una mayor probabilidad de compra en ciertas temporadas.</p> </li> <li> <p>Optimizaci\u00f3n de Procesos:     El Big Data permite optimizar procesos en todos los niveles. Desde la cadena de suministro hasta el marketing, el an\u00e1lisis de datos permite identificar ineficiencias y \u00e1reas de mejora. Un ejemplo claro es el uso de datos en la log\u00edstica, donde el an\u00e1lisis de rutas en tiempo real puede reducir los tiempos de entrega y los costos de transporte.</p> </li> <li> <p>Innovaci\u00f3n y Desarrollo de Nuevos Productos:     El an\u00e1lisis de datos masivos permite identificar nuevas oportunidades de mercado y desarrollar productos personalizados para satisfacer mejor las necesidades del cliente. Por ejemplo, empresas del sector de la salud pueden analizar grandes cantidades de datos m\u00e9dicos para identificar tendencias de consumo en productos saludables, lo que puede llevar al desarrollo de productos m\u00e1s alineados con los intereses del consumidor.</p> </li> </ol>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#almacenamiento-masivo-de-datos-las-vs-del-big-data","title":"\ud83c\udfe2 Almacenamiento Masivo de Datos: Las Vs del Big Data","text":"<p>El almacenamiento de grandes vol\u00famenes de datos es un pilar fundamental en el ecosistema de Big Data. Tradicionalmente, se mencionan las \"4Vs\" para describir las caracter\u00edsticas de Big Data, pero en realidad, existen muchas m\u00e1s Vs que a\u00f1aden complejidad y potencial a su an\u00e1lisis:</p> <ul> <li> <p>Volumen \ud83d\udce6: Se refiere a la enorme cantidad de datos generados y almacenados en sistemas digitales cada segundo. Desde redes sociales hasta dispositivos IoT, cada actividad genera datos que se deben almacenar y analizar. Empresas como Facebook y YouTube generan petabytes de datos diariamente. El reto es manejar este volumen de datos de manera eficiente sin perder velocidad de procesamiento.</p> </li> <li> <p>Velocidad \u26a1: Es la rapidez con la que se generan, recopilan y procesan los datos. Los sistemas de Big Data deben ser capaces de procesar la informaci\u00f3n casi en tiempo real para generar valor. Por ejemplo, empresas financieras utilizan datos en tiempo real para tomar decisiones sobre inversiones.</p> </li> <li> <p>Variedad \ud83c\udf08: Los datos provienen de diferentes fuentes y formatos, como texto, im\u00e1genes, videos, audio, transacciones, sensores IoT, etc. Esto hace necesario el uso de tecnolog\u00edas avanzadas capaces de analizar tanto datos estructurados (como bases de datos) como no estructurados (como publicaciones en redes sociales).</p> </li> <li> <p>Veracidad \ud83d\udee1\ufe0f: La calidad de los datos es crucial para tomar decisiones acertadas. Los datos incorrectos, duplicados o incompletos pueden llevar a conclusiones err\u00f3neas. Garantizar la veracidad de los datos implica establecer controles de calidad y limpieza antes de procesarlos para asegurar su fiabilidad.</p> </li> <li> <p>Variabilidad: No solo los vol\u00famenes de datos crecen, sino que tambi\u00e9n var\u00edan constantemente. Las tendencias y patrones de comportamiento cambian a lo largo del tiempo, y el an\u00e1lisis debe adaptarse a estos cambios.</p> </li> <li> <p>Valor \ud83d\udcb0: Es la capacidad de extraer insights \u00fatiles de grandes vol\u00famenes de datos. De nada sirve almacenar cantidades masivas de datos si no se puede extraer valor de ellos. Las empresas que logran transformar los datos en informaci\u00f3n valiosa pueden mejorar su posici\u00f3n competitiva y tomar mejores decisiones.</p> </li> </ul> <p>Para m\u00e1s informaci\u00f3n sobre las m\u00faltiples Vs del Big Data, puedes consultar esta infograf\u00eda: Infograf\u00eda sobre Big Data.</p>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#sistemas-de-almacenamiento-de-datos","title":"\ud83d\udcc2 Sistemas de Almacenamiento de Datos","text":"<p>El almacenamiento de grandes vol\u00famenes de datos debe cumplir con una serie de requisitos clave para manejar el crecimiento de la informaci\u00f3n de manera eficiente. Estos requisitos son:</p> <ol> <li>Capacidad: Los sistemas de almacenamiento deben ser escalables para soportar el crecimiento continuo de datos sin comprometer el rendimiento del sistema.</li> <li>Rendimiento: El acceso r\u00e1pido y eficiente a los datos es esencial para asegurar un procesamiento eficaz. Los sistemas de almacenamiento deben estar optimizados para acceder a los datos de manera \u00e1gil, sin cuellos de botella.</li> <li>Fiabilidad: Es crucial asegurar que los datos est\u00e9n protegidos contra p\u00e9rdidas y fallos del sistema. La replicaci\u00f3n y redundancia de datos ayudan a garantizar la disponibilidad continua de la informaci\u00f3n.</li> <li>Recuperabilidad: En caso de un fallo o p\u00e9rdida accidental de datos, los sistemas deben facilitar su recuperaci\u00f3n de manera r\u00e1pida y eficiente, minimizando el tiempo de inactividad.</li> </ol>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#dispositivos-mas-usados","title":"\ud83d\ude80 Dispositivos M\u00e1s Usados","text":"<ol> <li> <p>Discos (HDD, SSD, RAID): </p> <ul> <li>Los discos duros (HDD) ofrecen gran capacidad de almacenamiento a bajo costo, pero son m\u00e1s lentos en comparaci\u00f3n con los SSD.</li> <li>Los discos de estado s\u00f3lido (SSD) son mucho m\u00e1s r\u00e1pidos y eficientes, lo que los hace ideales para aplicaciones que requieren procesamiento a alta velocidad.</li> <li>Los arreglos RAID mejoran tanto la fiabilidad como el rendimiento al combinar varios discos en configuraciones redundantes. Esto permite la continuidad del servicio en caso de fallos de un disco individual.</li> </ul> <p></p> </li> <li> <p>Cintas Magn\u00e9ticas \ud83e\uddf2: Aunque puede parecer una tecnolog\u00eda antigua, las cintas magn\u00e9ticas siguen siendo una opci\u00f3n popular para el archivado a largo plazo debido a su bajo costo. Se utilizan com\u00fanmente para almacenar grandes vol\u00famenes de datos que no necesitan ser accedidos con frecuencia, como copias de seguridad.</p> </li> <li> <p>Almacenamiento en Red (NAS, SAN) \ud83c\udf10: </p> <ul> <li>NAS (Network Attached Storage) y SAN (Storage Area Network) permiten compartir almacenamiento a trav\u00e9s de una red, facilitando el acceso a los datos desde m\u00faltiples dispositivos. Estas soluciones son comunes en empresas que manejan grandes cantidades de datos de forma colaborativa.</li> </ul> </li> <li> <p>Almacenamiento en la Nube \u2601\ufe0f: El almacenamiento en la nube se ha convertido en una de las soluciones m\u00e1s populares debido a su escalabilidad, flexibilidad y capacidad para facilitar la recuperaci\u00f3n de datos ante desastres. Adem\u00e1s, permite a las empresas reducir costos al no tener que invertir en infraestructura propia.</p> </li> </ol>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#metodos-avanzados-de-almacenamiento-clusters","title":"\ud83d\udee0\ufe0f M\u00e9todos Avanzados de Almacenamiento: Clusters","text":"<p>Los sistemas distribuidos y clusters permiten gestionar grandes vol\u00famenes de datos de manera m\u00e1s eficiente, segura y escalable. Estos sistemas distribuyen los datos en varios nodos o servidores, asegurando redundancia y mejorando el rendimiento.</p> <ul> <li> <p>Tipos de RAID: Los diferentes niveles de RAID (como RAID 0, RAID 1, RAID 5, RAID 10) ofrecen diversas combinaciones de redundancia y rendimiento. RAID 5, por ejemplo, ofrece un equilibrio entre protecci\u00f3n de datos y rendimiento, siendo ideal para entornos que necesitan redundancia sin sacrificar velocidad.</p> </li> <li> <p>GlusterFS y MooseFS: Son sistemas de archivos distribuidos dise\u00f1ados para manejar grandes vol\u00famenes de datos. Estos sistemas permiten a las organizaciones administrar sus datos a trav\u00e9s de m\u00faltiples servidores, garantizando la disponibilidad y la redundancia de la informaci\u00f3n.</p> </li> <li> <p>CephFileSystem: Es un sistema de almacenamiento distribuido y altamente escalable que ofrece capacidades avanzadas de auto-reparaci\u00f3n y recuperaci\u00f3n. Es utilizado por grandes empresas que necesitan manejar petabytes de datos.</p> </li> <li> <p>DRBD (Distributed Replicated Block Device): Proporciona replicaci\u00f3n de datos en tiempo real entre servidores, asegurando que los datos est\u00e9n siempre disponibles y sincronizados en m\u00faltiples ubicaciones. Esto es vital para sistemas de alta disponibilidad.</p> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#procesamiento-de-datos-de-la-recoleccion-a-la-visualizacion","title":"\ud83d\udd0d Procesamiento de Datos: De la Recolecci\u00f3n a la Visualizaci\u00f3n","text":"<p>El procesamiento de datos en Big Data abarca una serie de etapas clave que transforman los datos brutos en informaci\u00f3n \u00fatil y aplicable. Cada etapa es esencial para obtener insights valiosos. A continuaci\u00f3n, ilustramos cada fase utilizando un ejemplo en el sector de la salud, donde se analizan datos de millones de pacientes para detectar patrones relacionados con enfermedades cr\u00f3nicas como la diabetes o la hipertensi\u00f3n.</p>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#etapas-de-procesamiento","title":"\ud83d\udcdd Etapas de Procesamiento","text":"<pre><code>graph TB\n    A[Recolecci\u00f3n de Datos] --&gt; B[Recopilaci\u00f3n]\n    B --&gt; C[Preprocesamiento o Limpieza de Datos]\n    C --&gt; D[Procesamiento]\n    D --&gt; E[Interpretaci\u00f3n y Visualizaci\u00f3n]\n    E --&gt; F[An\u00e1lisis]\n    F --&gt; G[Almacenamiento]</code></pre> <ol> <li> <p>Recolecci\u00f3n de Datos \ud83d\udce5:     En el caso de la salud, los datos provienen de diversas fuentes como:</p> <ul> <li>Historiales m\u00e9dicos electr\u00f3nicos (EMR).</li> <li>Dispositivos port\u00e1tiles como pulseras de actividad o relojes inteligentes que monitorean constantes vitales.</li> <li>Encuestas y cuestionarios de salud.</li> <li>Bases de datos gen\u00e9ticas.</li> </ul> <p>Ejemplo: Recolectamos datos de los niveles de glucosa, actividad f\u00edsica y dieta de millones de pacientes que utilizan dispositivos m\u00e9dicos y de bienestar.</p> </li> <li> <p>Recopilaci\u00f3n: Una vez recolectados, los datos de diferentes fuentes se consolidan en un almac\u00e9n de datos distribuido (como un sistema Hadoop o un almac\u00e9n en la nube como Amazon S3 o Azure Blob Storage) para su an\u00e1lisis posterior.</p> <p>Ejemplo: Los datos de pacientes de varios hospitales y dispositivos m\u00e9dicos son centralizados en una plataforma de almacenamiento en la nube para ser procesados de manera unificada.</p> </li> <li> <p>Preprocesamiento o Limpieza de Datos \ud83e\uddf9: En esta fase, se eliminan los datos duplicados, inconsistentes o incompletos para asegurar la calidad del an\u00e1lisis. Se estandarizan los formatos de datos para que todas las fuentes utilicen las mismas unidades de medida y estructura.</p> <p>Ejemplo: Se eliminan las entradas duplicadas y se estandarizan las unidades de medida (por ejemplo, convertir los niveles de glucosa de mg/dL a mmol/L) para que los datos sean coherentes en todo el conjunto.</p> </li> <li> <p>Procesamiento \ud83d\udda5\ufe0f: Se aplican algoritmos avanzados como machine learning y t\u00e9cnicas de miner\u00eda de datos para analizar patrones dentro de los datos de los pacientes y predecir la probabilidad de desarrollar enfermedades cr\u00f3nicas.</p> <p>Ejemplo: Un algoritmo de regresi\u00f3n log\u00edstica analiza los datos y predice la probabilidad de que un paciente desarrolle diabetes en los pr\u00f3ximos cinco a\u00f1os en funci\u00f3n de sus niveles de glucosa, actividad f\u00edsica y gen\u00e9tica.</p> </li> <li> <p>Interpretaci\u00f3n y Visualizaci\u00f3n \ud83d\udcca: Los resultados se presentan mediante gr\u00e1ficos interactivos, dashboards y reportes comprensibles que ayudan a los m\u00e9dicos a entender los patrones y tendencias.</p> <p>Ejemplo: Un dashboard interactivo muestra gr\u00e1ficos sobre c\u00f3mo diferentes factores como la obesidad, la falta de ejercicio y los antecedentes familiares influyen en el riesgo de desarrollar diabetes. Los m\u00e9dicos pueden ver f\u00e1cilmente c\u00f3mo var\u00edan estos factores seg\u00fan la regi\u00f3n geogr\u00e1fica o la edad del paciente.</p> </li> <li> <p>An\u00e1lisis \ud83e\udde0: En esta fase se profundiza en los resultados obtenidos para descubrir insights valiosos. Por ejemplo, el an\u00e1lisis puede revelar correlaciones inesperadas entre los h\u00e1bitos alimenticios y la aparici\u00f3n de enfermedades.</p> <p>Ejemplo: El an\u00e1lisis revela que el 80% de los pacientes con obesidad y antecedentes familiares tienen una alta probabilidad de desarrollar diabetes tipo 2 dentro de los pr\u00f3ximos cinco a\u00f1os.</p> </li> <li> <p>Almacenamiento: Finalmente, los datos analizados y sus resultados se almacenan para usos futuros, auditor\u00edas o para ser comparados con nuevos datos en investigaciones posteriores.</p> <p>Ejemplo: Los resultados se almacenan en una base de datos distribuida para su posterior an\u00e1lisis y comparaci\u00f3n con nuevos pacientes a lo largo del tiempo, lo que permite un monitoreo continuo de las tendencias de salud p\u00fablica.</p> </li> </ol>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#analitica-en-tiempo-real","title":"\ud83d\udcc8 Anal\u00edtica en Tiempo Real","text":"<p>Uno de los mayores beneficios del Big Data es la capacidad de realizar an\u00e1lisis en tiempo real. La anal\u00edtica en tiempo real permite a las empresas reaccionar inmediatamente ante eventos que ocurren en el momento. Algunos ejemplos incluyen:</p> <ul> <li> <p>Servicios Financieros: Las instituciones financieras utilizan an\u00e1lisis en tiempo real para monitorear transacciones y detectar posibles fraudes en el momento en que ocurren. Esto permite bloquear transacciones sospechosas antes de que se completen.</p> </li> <li> <p>Plataformas de Streaming: Empresas como Netflix y Spotify analizan en tiempo real el comportamiento de sus usuarios para ofrecer recomendaciones personalizadas sobre qu\u00e9 series, pel\u00edculas o canciones ver o escuchar a continuaci\u00f3n.</p> </li> <li> <p>Smart Cities: Las ciudades inteligentes utilizan sensores distribuidos en toda la infraestructura urbana para monitorear el tr\u00e1fico, los niveles de contaminaci\u00f3n y el consumo de energ\u00eda en tiempo real. Esto permite ajustes autom\u00e1ticos para optimizar el uso de recursos y mejorar la calidad de vida de los ciudadanos.</p> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#big-data-y-cloud-computing","title":"\u2601\ufe0f Big Data y Cloud Computing","text":"<p>El Cloud Computing ha abierto un nuevo mundo de posibilidades para el Big Data. Al combinar ambas tecnolog\u00edas, las empresas pueden escalar sus operaciones sin necesidad de costosas inversiones en infraestructura f\u00edsica.</p>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#ventajas-del-cloud-computing-para-big-data","title":"Ventajas del Cloud Computing para Big Data","text":"<ul> <li> <p>Escalabilidad Ilimitada: El cloud computing permite a las empresas ajustar su capacidad de procesamiento y almacenamiento seg\u00fan sea necesario. Esto significa que pueden escalar vertical y horizontalmente sin comprometer el rendimiento.</p> </li> <li> <p>Costos Bajo Demanda: Las empresas solo pagan por los recursos que utilizan, lo que optimiza los costos operativos y evita gastos innecesarios en infraestructura f\u00edsica.</p> </li> <li> <p>Accesibilidad Global: El almacenamiento en la nube permite que los datos y las aplicaciones sean accesibles desde cualquier parte del mundo, facilitando la colaboraci\u00f3n entre equipos distribuidos geogr\u00e1ficamente.</p> </li> <li> <p>Seguridad y Recuperaci\u00f3n: Las soluciones en la nube ofrecen avanzadas medidas de seguridad y recuperaci\u00f3n ante desastres, asegurando que los datos est\u00e9n protegidos y disponibles incluso en situaciones de emergencia.</p> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#conclusion","title":"\ud83d\ude80 Conclusi\u00f3n","text":"<p>El Big Data ha transformado la forma en que las organizaciones capturan, almacenan, procesan y analizan datos. Desde sus or\u00edgenes hasta las avanzadas soluciones de Cloud Computing y Inteligencia Artificial, el Big Data ha proporcionado una plataforma poderosa para la innovaci\u00f3n y la toma de decisiones estrat\u00e9gicas. La combinaci\u00f3n de almacenamiento masivo, procesamiento distribuido y an\u00e1lisis en tiempo real est\u00e1 remodelando industrias enteras, creando nuevas oportunidades y optimizando las operaciones empresariales. \u00a1Es el momento de aprovechar el poder del Big Data para llevar tu organizaci\u00f3n al siguiente nivel!</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/","title":"2.1. \u00bfQu\u00e9 es Apache Hadoop?","text":""},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#que-es-apache-hadoop","title":"\u00bfQu\u00e9 es Apache Hadoop? \ud83d\ude80","text":"<p>Apache Hadoop es un marco de software de c\u00f3digo abierto dise\u00f1ado para el almacenamiento y procesamiento masivo de datos en cl\u00fasteres de computadoras. Gracias a su arquitectura distribuida, Hadoop es capaz de manejar grandes cantidades de informaci\u00f3n de manera eficiente y rentable, convirti\u00e9ndose en un pilar esencial en el mundo del Big Data.</p> <p>Hadoop no solo almacena datos, sino que tambi\u00e9n facilita su procesamiento en paralelo, lo que permite analizar grandes vol\u00famenes de informaci\u00f3n de manera r\u00e1pida. Su capacidad para escalar desde unos pocos servidores hasta miles lo convierte en una herramienta flexible y poderosa para empresas de todos los tama\u00f1os.</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#como-funciona-hadoop","title":"\ud83e\udde0 \u00bfC\u00f3mo Funciona Hadoop?","text":"<p>Hadoop se compone principalmente de cuatro m\u00f3dulos que trabajan en conjunto para proporcionar un ecosistema completo de Big Data:</p> <ol> <li> <p>HDFS (Hadoop Distributed File System) \ud83d\udcc2: Almacena grandes vol\u00famenes de datos distribuidos a trav\u00e9s de m\u00faltiples nodos, garantizando alta disponibilidad y resistencia a fallos.</p> </li> <li> <p>YARN (Yet Another Resource Negotiator) \ud83c\udfaf: Act\u00faa como un administrador de recursos, asignando tareas y gestionando recursos de manera eficiente dentro del cl\u00faster.</p> </li> <li> <p>MapReduce \ud83d\udee0\ufe0f: Es el motor de procesamiento de datos que divide las tareas en subtareas m\u00e1s peque\u00f1as, permitiendo el procesamiento en paralelo de grandes conjuntos de datos.</p> </li> <li> <p>Hadoop Common \u2699\ufe0f: Proporciona las herramientas y utilidades b\u00e1sicas que soportan los dem\u00e1s m\u00f3dulos, facilitando la integraci\u00f3n y el funcionamiento del ecosistema.</p> </li> </ol>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#por-que-elegir-hadoop","title":"\ud83d\udea6 \u00bfPor Qu\u00e9 Elegir Hadoop?","text":""},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#1-escalabilidad-infinita","title":"1. Escalabilidad Infinita \ud83c\udfd7\ufe0f","text":"<p>Hadoop est\u00e1 dise\u00f1ado para crecer junto con tus necesidades. Desde unos pocos nodos hasta miles de m\u00e1quinas, puede manejar crecimientos exponenciales de datos sin perder rendimiento. Su arquitectura permite la adici\u00f3n de nodos sin necesidad de reconfigurar el sistema, lo que facilita la expansi\u00f3n continua.</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#2-rentabilidad","title":"2. Rentabilidad \ud83d\udcb0","text":"<p>El uso de hardware b\u00e1sico y de bajo costo hace que Hadoop sea una soluci\u00f3n asequible para las empresas que necesitan manejar grandes vol\u00famenes de datos. A diferencia de otros sistemas de datos que requieren hardware especializado, Hadoop se ejecuta en servidores comunes, reduciendo significativamente los costos de infraestructura.</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#3-flexibilidad-y-adaptabilidad","title":"3. Flexibilidad y Adaptabilidad \ud83d\udd04","text":"<p>No importa si tus datos son estructurados, no estructurados o semiestructurados; Hadoop puede almacenar y procesar cualquier tipo de informaci\u00f3n. Esto lo hace ideal para un amplio rango de aplicaciones, desde an\u00e1lisis de redes sociales hasta procesamiento de registros de sensores.</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#4-resistencia-a-fallos","title":"4. Resistencia a Fallos \ud83d\udd12","text":"<p>Hadoop est\u00e1 dise\u00f1ado con la seguridad en mente. Al replicar datos en varios nodos dentro del cl\u00faster, garantiza que la informaci\u00f3n est\u00e9 disponible incluso si un nodo falla, asegurando la continuidad operativa sin interrupciones.</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#5-procesamiento-rapido-y-paralelo","title":"5. Procesamiento R\u00e1pido y Paralelo \u26a1","text":"<p>Gracias a MapReduce, Hadoop procesa grandes vol\u00famenes de datos en paralelo, dividiendo tareas complejas en subtareas m\u00e1s peque\u00f1as. Esto ahorra tiempo y mejora la eficiencia al manejar trabajos que, de otro modo, podr\u00edan llevar horas o d\u00edas.</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#procesamiento-distribuido-clusteres-y-tipos-de-nodos","title":"\ud83d\udd27 Procesamiento Distribuido, Cl\u00fasteres y Tipos de Nodos","text":"<p>El procesamiento distribuido es el coraz\u00f3n de Apache Hadoop, permitiendo que grandes vol\u00famenes de datos se dividan en partes m\u00e1s peque\u00f1as y se procesen de manera simult\u00e1nea en varios nodos dentro de un cl\u00faster.</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#procesamiento-distribuido","title":"Procesamiento Distribuido","text":"<p>El procesamiento distribuido implica dividir las tareas de procesamiento de datos en partes m\u00e1s peque\u00f1as, las cuales se ejecutan de forma paralela en diferentes m\u00e1quinas o nodos. En lugar de procesar los datos de forma secuencial en un solo servidor, Hadoop divide los datos en bloques que son procesados simult\u00e1neamente. Esto aumenta la eficiencia y reduce el tiempo necesario para analizar grandes vol\u00famenes de informaci\u00f3n.</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#clusteres-en-hadoop","title":"Cl\u00fasteres en Hadoop","text":"<p>Un cl\u00faster en Hadoop est\u00e1 compuesto por un conjunto de nodos conectados entre s\u00ed, que trabajan en conjunto para almacenar y procesar datos de manera distribuida. Los cl\u00fasteres est\u00e1n dise\u00f1ados para ser escalables, lo que significa que se pueden agregar m\u00e1s nodos seg\u00fan sea necesario, incrementando la capacidad de procesamiento sin afectar el rendimiento general.</p> <ul> <li>Escalabilidad Horizontal: Se logra a\u00f1adiendo m\u00e1s nodos al cl\u00faster.</li> <li>Distribuci\u00f3n de Carga: Los datos y tareas se distribuyen entre todos los nodos para maximizar el uso eficiente de los recursos.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#tipos-de-nodos-en-hadoop","title":"Tipos de Nodos en Hadoop","text":"<p>En un cl\u00faster de Hadoop, existen varios tipos de nodos que desempe\u00f1an funciones cr\u00edticas dentro del sistema:</p> <ol> <li> <p>Nodo Maestro (NameNode):  </p> <ul> <li>Funci\u00f3n: Coordina el almacenamiento de datos y gestiona el sistema de archivos distribuido (HDFS). Supervisa la ubicaci\u00f3n de los bloques de datos y garantiza la replicaci\u00f3n para asegurar la resistencia a fallos.</li> <li>Responsabilidades: Mantiene el registro de d\u00f3nde est\u00e1n almacenados los bloques de datos en los DataNodes. El NameNode es esencial para la administraci\u00f3n general del cl\u00faster y su buen funcionamiento.</li> </ul> </li> <li> <p>Nodos de Datos (DataNode):  </p> <ul> <li>Funci\u00f3n: Son responsables de almacenar los bloques de datos y responder a las solicitudes del NameNode. Estos nodos realizan la mayor parte del trabajo pesado al manejar los datos en bruto.</li> <li>Responsabilidades: Almacenan los bloques de datos y ejecutan las tareas de procesamiento. Cada DataNode puede almacenar varias copias de los datos, asegurando la replicaci\u00f3n y la alta disponibilidad. Esto garantiza la recuperaci\u00f3n de datos en caso de fallos.</li> </ul> </li> <li> <p>Nodos Edge (EdgeNode):  </p> <ul> <li>Funci\u00f3n: Los nodos edge act\u00faan como un puente entre el cl\u00faster Hadoop y la red externa. Son los nodos a trav\u00e9s de los cuales los usuarios pueden interactuar con el cl\u00faster, enviar trabajos y obtener resultados.</li> <li>Responsabilidades: Proporcionan interfaces para que los datos y comandos entren y salgan del cl\u00faster de manera segura. Estos nodos no almacenan ni procesan datos directamente, pero ofrecen una capa de seguridad y control al filtrar el acceso y las solicitudes a los NameNodes y DataNodes.</li> </ul> </li> </ol> <pre><code>graph LR\n    subgraph Cl\u00faster de Hadoop\n        A[NameNode] --&gt; B[DataNode 1]\n        A --&gt; C[DataNode 2]\n        A --&gt; D[DataNode 3]\n    end\n    E[EdgeNode] --&gt; A\n    F[Usuarios] --&gt; E</code></pre>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#tolerancia-a-fallos-en-los-nodos","title":"Tolerancia a Fallos en los Nodos","text":"<p>Hadoop garantiza la tolerancia a fallos mediante la replicaci\u00f3n de los datos en m\u00faltiples DataNodes. Si un nodo falla, los datos a\u00fan est\u00e1n disponibles en otros nodos del cl\u00faster, lo que evita la p\u00e9rdida de informaci\u00f3n y garantiza que el procesamiento contin\u00fae sin interrupciones.</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#casos-de-uso-de-hadoop","title":"\ud83c\udf10 Casos de Uso de Hadoop","text":"<p>Hadoop ha revolucionado m\u00faltiples industrias gracias a su capacidad para manejar grandes vol\u00famenes de datos de manera eficiente. Aqu\u00ed algunos de los sectores donde Hadoop marca la diferencia:</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#1-finanzas-y-bancos","title":"1. Finanzas y Bancos \ud83c\udfe6","text":"<ul> <li>Detecci\u00f3n de Fraudes: Analiza patrones en tiempo real para detectar y prevenir actividades fraudulentas.</li> <li>An\u00e1lisis de Riesgos: Procesa grandes vol\u00famenes de datos financieros para identificar y gestionar riesgos.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#2-salud","title":"2. Salud \ud83c\udfe5","text":"<ul> <li>Gen\u00f3mica: Procesa datos de secuenciaci\u00f3n gen\u00e9tica para avanzar en la medicina personalizada.</li> <li>An\u00e1lisis de Im\u00e1genes M\u00e9dicas: Maneja grandes vol\u00famenes de im\u00e1genes para mejorar diagn\u00f3sticos y tratamientos.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#3-telecomunicaciones","title":"3. Telecomunicaciones \ud83d\udce1","text":"<ul> <li>An\u00e1lisis de Redes: Monitorea y optimiza el rendimiento de las redes en tiempo real.</li> <li>Modelos Predictivos: Utiliza datos hist\u00f3ricos para prever fallos y optimizar el servicio al cliente.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#4-retail-y-e-commerce","title":"4. Retail y E-commerce \ud83d\uded2","text":"<ul> <li>An\u00e1lisis del Comportamiento del Cliente: Utiliza datos de navegaci\u00f3n y compra para personalizar ofertas.</li> <li>Gesti\u00f3n de Inventarios: Optimiza la cadena de suministro basada en patrones de compra y demanda.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#conclusion","title":"\ud83c\udf10 Conclusi\u00f3n","text":"<p>Apache Hadoop no es solo una tecnolog\u00eda; es una revoluci\u00f3n en la forma en que manejamos y procesamos grandes vol\u00famenes de datos. Desde su capacidad para escalar hasta miles de nodos, hasta su capacidad para procesar datos en paralelo, Hadoop ha transformado la forma en que las organizaciones analizan y gestionan sus datos.</p>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/","title":"Ecosistema Hadoop y Distribuciones \ud83c\udf10\ud83d\ude80","text":"<p>El ecosistema Hadoop ha revolucionado la forma en que las organizaciones manejan y procesan datos a gran escala. Hadoop no es solo un software; es un ecosistema completo de herramientas y tecnolog\u00edas que trabajan juntas para resolver los desaf\u00edos del Big Data. A medida que el volumen, la variedad y la velocidad de los datos contin\u00faan creciendo, el ecosistema Hadoop se expande para incluir m\u00faltiples componentes y distribuciones dise\u00f1adas para aprovechar al m\u00e1ximo esta revoluci\u00f3n de datos.</p>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#que-es-el-ecosistema-hadoop","title":"\ud83e\udde0 \u00bfQu\u00e9 es el Ecosistema Hadoop?","text":"<p>El ecosistema Hadoop es una colecci\u00f3n de proyectos y herramientas que interact\u00faan entre s\u00ed para proporcionar una plataforma integral para el almacenamiento, procesamiento y an\u00e1lisis de grandes vol\u00famenes de datos. Este ecosistema incluye componentes para la ingesti\u00f3n de datos, el procesamiento en tiempo real, el an\u00e1lisis avanzado y la gesti\u00f3n de recursos.</p> <pre><code>graph LR\n    A[Hadoop Ecosystem] --&gt; B[HDFS]\n    A --&gt; C[YARN]\n    A --&gt; D[MapReduce]\n    A --&gt; E[Spark]\n    A --&gt; F[Hive]\n    A --&gt; G[HBase]\n    A --&gt; H[Pig]\n    A --&gt; I[Oozie]\n    A --&gt; J[Sqoop]\n    A --&gt; K[Flume]</code></pre>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#componentes-clave-del-ecosistema-hadoop","title":"Componentes Clave del Ecosistema Hadoop \ud83d\udee0\ufe0f","text":"<ol> <li> <p>HDFS (Hadoop Distributed File System) \ud83d\udcc2: El sistema de archivos distribuido que almacena grandes vol\u00famenes de datos de manera eficiente y segura.</p> </li> <li> <p>YARN (Yet Another Resource Negotiator) \ud83c\udfaf: Gestor de recursos que asigna y administra las tareas dentro del cl\u00faster Hadoop.</p> </li> <li> <p>MapReduce \ud83d\udee0\ufe0f: Modelo de programaci\u00f3n que permite el procesamiento paralelo de datos en un entorno distribuido.</p> </li> <li> <p>Apache Spark \u26a1: Motor de procesamiento r\u00e1pido y en memoria que ofrece una alternativa m\u00e1s \u00e1gil a MapReduce para el an\u00e1lisis de datos en tiempo real.</p> </li> <li> <p>Apache Hive \ud83d\udc1d: Herramienta que facilita la consulta y el an\u00e1lisis de datos almacenados en HDFS utilizando un lenguaje similar a SQL, conocido como HiveQL.</p> </li> <li> <p>Apache HBase \ud83d\udcca: Base de datos NoSQL de alto rendimiento que proporciona acceso en tiempo real a grandes vol\u00famenes de datos distribuidos.</p> </li> <li> <p>Apache Pig \ud83d\udc37: Lenguaje de alto nivel para el procesamiento de grandes conjuntos de datos que simplifica la escritura de scripts complejos en comparaci\u00f3n con MapReduce.</p> </li> <li> <p>Apache Oozie \ud83d\udcc5: Coordinador de flujos de trabajo que permite programar y gestionar trabajos en Hadoop.</p> </li> <li> <p>Apache Sqoop \ud83d\udd04: Herramienta que facilita la transferencia de datos entre Hadoop y bases de datos relacionales.</p> </li> <li> <p>Apache Flume \ud83d\udce5: Sistema de ingesti\u00f3n de datos que permite recopilar, agregar y mover grandes cantidades de datos de eventos a Hadoop.</p> </li> </ol>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#distribuciones-populares-de-hadoop","title":"\ud83c\udf10 Distribuciones Populares de Hadoop","text":"<p>Las distribuciones de Hadoop son paquetes que integran el ecosistema de herramientas Hadoop con caracter\u00edsticas adicionales de administraci\u00f3n y soporte. Estas distribuciones est\u00e1n dise\u00f1adas para simplificar la implementaci\u00f3n, configuraci\u00f3n y mantenimiento de cl\u00fasteres Hadoop. Aqu\u00ed te presentamos algunas de las m\u00e1s populares:</p>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#1-cloudera-distribution-for-hadoop-cdh","title":"1. Cloudera Distribution for Hadoop (CDH) \ud83c\udfe2","text":"<p>Cloudera es una de las distribuciones comerciales m\u00e1s reconocidas de Hadoop. Ofrece una versi\u00f3n completa del ecosistema Hadoop con herramientas adicionales para la gesti\u00f3n, seguridad y an\u00e1lisis de datos.</p> <ul> <li>Gesti\u00f3n Simplificada: Cloudera Manager permite gestionar y monitorear el cl\u00faster de forma centralizada.</li> <li>Seguridad Mejorada: Ofrece encriptaci\u00f3n de datos y autenticaci\u00f3n avanzada.</li> <li>Optimizaci\u00f3n de Desempe\u00f1o: Ajustes autom\u00e1ticos que mejoran la eficiencia de las tareas.</li> </ul> <pre><code>// Ejemplo de conexi\u00f3n a un cl\u00faster de Hadoop usando Cloudera\nconst cloudera = require('cloudera-api');\n\n// Conectar al cl\u00faster de Cloudera\nconst client = new cloudera.Cluster({\n  hostname: 'cloudera-cluster.local',\n  username: 'admin',\n  password: 'password'\n});\n\nclient.getStatus((err, status) =&gt; {\n  if (err) {\n    console.error('Error conectando al cl\u00faster:', err);\n  } else {\n    console.log('Estado del cl\u00faster:', status);\n  }\n});\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#2-hortonworks-data-platform-hdp","title":"2. Hortonworks Data Platform (HDP) \ud83d\udc18","text":"<p>Hortonworks, ahora parte de Cloudera, ofrece una distribuci\u00f3n de Hadoop completamente de c\u00f3digo abierto. HDP se enfoca en la integraci\u00f3n de datos y proporciona un s\u00f3lido conjunto de herramientas para el an\u00e1lisis y la gesti\u00f3n de datos.</p> <ul> <li>Soporte 100% Open Source: Fomenta la innovaci\u00f3n y permite la personalizaci\u00f3n completa de la plataforma.</li> <li>Integraci\u00f3n con la Nube: Compatible con implementaciones en la nube y en entornos h\u00edbridos.</li> <li>Simplificaci\u00f3n de Operaciones: Herramientas para la automatizaci\u00f3n de flujos de trabajo y la gesti\u00f3n de datos.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#3-mapr","title":"3. MapR \ud83c\udf32","text":"<p>MapR destaca por su arquitectura \u00fanica que combina Hadoop con un sistema de archivos distribuido patentado y una base de datos NoSQL integrada. Ofrece una alta disponibilidad y rendimiento superior en comparaci\u00f3n con otras distribuciones.</p> <ul> <li>MapR XD y MapR DB: Proporcionan almacenamiento y gesti\u00f3n de datos avanzados con capacidades empresariales.</li> <li>Soporte de Contenedores y Microservicios: Compatible con Kubernetes para la implementaci\u00f3n de aplicaciones modernas.</li> <li>Procesamiento en Tiempo Real: Capacidades para an\u00e1lisis de flujos de datos en tiempo real.</li> </ul> <pre><code>// Ejemplo de integraci\u00f3n con MapR usando JavaScript\nconst mapr = require('mapr-streams');\n\n// Configuraci\u00f3n de una conexi\u00f3n de flujo de datos en tiempo real\nconst stream = mapr.createStream('/path/to/stream');\n\nstream.on('data', (message) =&gt; {\n  console.log('Mensaje recibido:', message.value.toString());\n});\n\nstream.write({ key: 'sensor1', value: 'temperatura: 22\u00b0C' });\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#4-amazon-emr-elastic-mapreduce","title":"4. Amazon EMR (Elastic MapReduce) \u2601\ufe0f","text":"<p>Amazon EMR es la distribuci\u00f3n basada en la nube de Hadoop ofrecida por AWS. Permite escalar f\u00e1cilmente el cl\u00faster y ajustar los recursos seg\u00fan la demanda de procesamiento de datos.</p> <ul> <li>Escalabilidad Autom\u00e1tica: Ajusta la capacidad del cl\u00faster en funci\u00f3n de la carga de trabajo.</li> <li>Integraci\u00f3n con Servicios AWS: F\u00e1cil integraci\u00f3n con S3, Redshift y otras soluciones de AWS.</li> <li>Costos Bajo Demanda: Paga solo por lo que usas, optimizando los costos operativos.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#casos-de-uso-del-ecosistema-hadoop","title":"\ud83d\udea6 Casos de Uso del Ecosistema Hadoop","text":"<p>El ecosistema Hadoop no solo almacena datos; lo transforma en valor accionable. A continuaci\u00f3n, se presentan algunos casos de uso donde las empresas utilizan Hadoop y sus herramientas para generar impacto:</p>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#1-analisis-de-redes-sociales","title":"1. An\u00e1lisis de Redes Sociales \ud83d\udde8\ufe0f","text":"<p>Las empresas analizan millones de interacciones en redes sociales para entender las tendencias del mercado y la opini\u00f3n del cliente. Herramientas como Spark y Hive se utilizan para procesar estos datos r\u00e1pidamente.</p>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#2-recomendacion-de-productos","title":"2. Recomendaci\u00f3n de Productos \ud83d\udecd\ufe0f","text":"<p>Las plataformas de e-commerce utilizan algoritmos de aprendizaje autom\u00e1tico en Hadoop para analizar el comportamiento del usuario y recomendar productos personalizados en tiempo real.</p> <pre><code>// Ejemplo de recomendaci\u00f3n de productos usando Spark y JavaScript\nconst spark = require('apache-spark');\n\n// Crear un modelo de recomendaci\u00f3n basado en el historial de compras del usuario\nconst recommendations = spark.mllib.recommendation.ALS.train(usersPurchases, 10, 0.01);\n\nrecommendations.predict(user, (err, products) =&gt; {\n  if (err) {\n    console.error('Error generando recomendaciones:', err);\n  } else {\n    console.log('Productos recomendados:', products);\n  }\n});\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#3-prevencion-de-fraudes-financieros","title":"3. Prevenci\u00f3n de Fraudes Financieros \ud83c\udfe6","text":"<p>Bancos y aseguradoras usan Hadoop para analizar transacciones en tiempo real y detectar patrones sospechosos. Hadoop permite combinar m\u00faltiples fuentes de datos para una detecci\u00f3n de fraudes m\u00e1s precisa y r\u00e1pida.</p>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#4-monitoreo-de-infraestructuras","title":"4. Monitoreo de Infraestructuras \ud83d\udce1","text":"<p>Las empresas de telecomunicaciones utilizan el ecosistema Hadoop para monitorear sus infraestructuras de red, detectando fallos y optimizando el rendimiento en tiempo real.</p>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#ejemplo-completo-de-integracion-del-ecosistema-hadoop-en-javascript","title":"\ud83d\udee0\ufe0f Ejemplo Completo de Integraci\u00f3n del Ecosistema Hadoop en JavaScript","text":"<p>Para entender c\u00f3mo funciona el ecosistema Hadoop en la pr\u00e1ctica, consideremos un ejemplo completo que integra varios componentes:</p> <pre><code>const hdfs = require('hdfs');\nconst spark = require('apache-spark');\nconst hive = require('hive-client');\n\n// Configuraci\u00f3n de HDFS\nconst hdfsClient = hdfs({\n  protocol: 'http',\n  hostname: 'localhost',\n  port: 9870\n});\n\n// Subir datos a HDFS\nhdfsClient.createFile('/user/data.txt', 'Datos para an\u00e1lisis', (err) =&gt; {\n  if (err) {\n    console.error('Error al subir archivo:', err);\n  } else {\n    console.log('Archivo subido a HDFS correctamente.');\n  }\n});\n\n// Consultar datos en Hive\nconst hiveClient = hive.createClient({ host: 'localhost', port: 10000 });\n\nhiveClient.connect().then(() =&gt; {\n  hiveClient.query('SELECT * FROM logs WHERE event=\"error\";', (err, results) =&gt; {\n    if (err) {\n      console.error('Error en la consulta Hive:', err);\n    } else {\n\n\n console.log('Resultados de la consulta:', results);\n    }\n  });\n});\n\n// Procesar datos con Spark\nspark.session.builder().getOrCreate().then(session =&gt; {\n  const dataFrame = session.read().format('csv').load('/user/data.txt');\n\n  dataFrame.filter(dataFrame.col('event').equalTo('error'))\n    .show()\n    .then(() =&gt; session.stop());\n});\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#conclusion","title":"\ud83c\udf1f Conclusi\u00f3n","text":"<p>El ecosistema Hadoop y sus diversas distribuciones son fundamentales para cualquier estrategia de Big Data moderna. Ofrecen una soluci\u00f3n integral para almacenar, procesar y analizar datos a gran escala, permitiendo a las empresas tomar decisiones informadas basadas en datos. Con la flexibilidad para manejar todo tipo de datos y la capacidad de escalar a cualquier tama\u00f1o de cl\u00faster, Hadoop contin\u00faa liderando la transformaci\u00f3n digital en todo el mundo. \u00a1Explora el poder del ecosistema Hadoop y descubre c\u00f3mo puede revolucionar tu gesti\u00f3n de datos! \ud83d\ude80\ud83d\udcca</p>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/","title":"Arquitectura de Hadoop: Desentra\u00f1ando la Potencia del Big Data \ud83d\ude80\ud83e\udde0","text":"<p>Apache Hadoop es una plataforma robusta de c\u00f3digo abierto dise\u00f1ada para almacenar y procesar grandes vol\u00famenes de datos de manera eficiente y escalable. Pero \u00bfqu\u00e9 hace que Hadoop sea tan poderoso? La respuesta radica en su arquitectura distribuida, que permite procesar datos a trav\u00e9s de m\u00faltiples nodos de manera paralela. En este art\u00edculo, exploraremos en detalle la arquitectura de Hadoop, desglosando sus componentes y c\u00f3mo trabajan juntos para hacer del Big Data una realidad accesible.</p>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#componentes-principales-de-la-arquitectura-de-hadoop","title":"\ud83e\udde9 Componentes Principales de la Arquitectura de Hadoop","text":"<p>La arquitectura de Hadoop se compone de varios m\u00f3dulos que colaboran para ofrecer un entorno completo de almacenamiento y procesamiento de datos. Los componentes clave son:</p> <pre><code>graph TD\n    A[Arquitectura de Hadoop] --&gt; B[HDFS]\n    A --&gt; C[YARN]\n    A --&gt; D[MapReduce]\n    A --&gt; E[Hadoop Common]\n    A --&gt; F[Componentes de Integraci\u00f3n]</code></pre>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#1-hdfs-hadoop-distributed-file-system","title":"1. HDFS (Hadoop Distributed File System) \ud83d\udcc2","text":"<p>HDFS es el sistema de archivos distribuido de Hadoop, dise\u00f1ado para almacenar datos de manera segura y eficiente en grandes cl\u00fasteres. Se encarga de dividir los archivos grandes en bloques y distribuirlos a trav\u00e9s de diferentes nodos en el cl\u00faster.</p> <pre><code>graph TD\n    HDFS[HDFS] --&gt;|Divide Archivos en| Bloques[Bloques]\n    Bloques --&gt;|Distribuye en| Nodos[Nodos]\n    Nodos --&gt;|Copia y Replicaci\u00f3n| Copias[Copias de Seguridad]</code></pre>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#caracteristicas-de-hdfs","title":"Caracter\u00edsticas de HDFS:","text":"<ul> <li>Alta Disponibilidad: Al replicar los bloques de datos en varios nodos, asegura que los datos est\u00e9n siempre disponibles, incluso si uno de los nodos falla.</li> <li>Escalabilidad: A\u00f1adir nuevos nodos al cl\u00faster incrementa autom\u00e1ticamente la capacidad de almacenamiento.</li> <li>Tolerancia a Fallos: Dise\u00f1ado para detectar y recuperarse autom\u00e1ticamente de fallos de hardware y software.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#2-yarn-yet-another-resource-negotiator","title":"2. YARN (Yet Another Resource Negotiator) \ud83c\udfaf","text":"<p>YARN act\u00faa como el gestor de recursos de Hadoop. Asigna recursos de procesamiento a las aplicaciones y coordina la ejecuci\u00f3n de tareas en el cl\u00faster, asegurando que los trabajos se completen de manera eficiente.</p> <pre><code>graph TD\n    YARN[YARN] --&gt;|Gesti\u00f3n de Recursos| Asignaci\u00f3n[Asignaci\u00f3n de Recursos]\n    YARN --&gt;|Ejecuci\u00f3n de Tareas| Tareas[Tareas del Cl\u00faster]\n    Tareas --&gt;|Optimizaci\u00f3n| Eficiencia[Eficiencia del Cl\u00faster]</code></pre>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#caracteristicas-de-yarn","title":"Caracter\u00edsticas de YARN:","text":"<ul> <li>Asignaci\u00f3n Din\u00e1mica de Recursos: Distribuye recursos seg\u00fan las necesidades de las aplicaciones en tiempo real, optimizando el uso del cl\u00faster.</li> <li>Seguridad y Control: Ofrece control granular sobre la ejecuci\u00f3n de tareas, garantizando la seguridad y estabilidad del sistema.</li> <li>Escalabilidad: Permite la expansi\u00f3n del cl\u00faster sin necesidad de reconfiguraciones complejas.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#3-mapreduce","title":"3. MapReduce \ud83d\udee0\ufe0f","text":"<p>MapReduce es el modelo de programaci\u00f3n de Hadoop que permite el procesamiento paralelo de grandes vol\u00famenes de datos. Consiste en dos fases principales: Map y Reduce.</p> <pre><code>graph TD\n    A[MapReduce] --&gt; B[Map]\n    A --&gt; C[Reduce]\n    B --&gt; D[Procesa Datos en Pares Clave-Valor]\n    C --&gt; E[Combina y Reduce Resultados]</code></pre> <ul> <li>Map: Toma los datos de entrada y los procesa en pares clave-valor.</li> <li>Reduce: Combina estos pares para generar un resultado final.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#ejemplo-de-mapreduce-en-javascript","title":"Ejemplo de MapReduce en JavaScript:","text":"<pre><code>// Ejemplo de MapReduce para contar palabras\nconst map = (text) =&gt; {\n  return text.split(' ').map(word =&gt; ({ key: word, value: 1 }));\n};\n\nconst reduce = (mappedData) =&gt; {\n  return mappedData.reduce((acc, curr) =&gt; {\n    acc[curr.key] = (acc[curr.key] || 0) + curr.value;\n    return acc;\n  }, {});\n};\n\nconst data = \"Hadoop es incre\u00edble, Hadoop es poderoso\";\nconst mapped = map(data);\nconst reduced = reduce(mapped);\n\nconsole.log(reduced); // { Hadoop: 2, es: 2, incre\u00edble: 1, poderoso: 1 }\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#4-hadoop-common","title":"4. Hadoop Common \u2699\ufe0f","text":"<p>Hadoop Common proporciona las bibliotecas y utilidades necesarias que soportan los otros m\u00f3dulos de Hadoop, asegurando la integraci\u00f3n y el funcionamiento adecuado de todo el ecosistema.</p> <ul> <li>Funciones B\u00e1sicas: Ofrece soporte para la gesti\u00f3n de configuraci\u00f3n, registro y acceso remoto.</li> <li>Soporte Multiplataforma: Compatible con diferentes sistemas operativos, lo que facilita su implementaci\u00f3n en cualquier entorno.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#5-componentes-de-integracion","title":"5. Componentes de Integraci\u00f3n \ud83d\udd0c","text":"<p>Hadoop no funciona en solitario. Se integra con varias herramientas y tecnolog\u00edas para ampliar sus capacidades y proporcionar un entorno m\u00e1s completo para la gesti\u00f3n de datos:</p> <pre><code>graph TD\n    Hadoop[Hadoop] --&gt; Spark[Spark]\n    Hadoop --&gt; Hive[Hive]\n    Hadoop --&gt; HBase[HBase]\n    Hadoop --&gt; Pig[Pig]\n    Hadoop --&gt; Sqoop[Sqoop]\n    Hadoop --&gt; Flume[Flume]</code></pre> <ul> <li>Apache Spark \u26a1: Ofrece procesamiento en memoria, lo que acelera las tareas de an\u00e1lisis en comparaci\u00f3n con MapReduce.</li> <li>Apache Hive \ud83d\udc1d: Permite consultas SQL sobre datos almacenados en HDFS, facilitando el an\u00e1lisis de datos.</li> <li>Apache HBase \ud83d\udcca: Proporciona acceso en tiempo real a grandes vol\u00famenes de datos distribuidos, ideal para aplicaciones que requieren baja latencia.</li> <li>Apache Pig \ud83d\udc37: Un lenguaje de alto nivel para escribir scripts que procesen grandes conjuntos de datos de manera m\u00e1s simple que MapReduce.</li> <li>Apache Sqoop \ud83d\udd04: Facilita la transferencia de datos entre Hadoop y bases de datos relacionales.</li> <li>Apache Flume \ud83d\udce5: Recoge, agrega y mueve grandes cantidades de datos de eventos a Hadoop.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#como-trabajan-juntos-los-componentes-de-hadoop","title":"\ud83d\udea6 \u00bfC\u00f3mo Trabajan Juntos los Componentes de Hadoop?","text":"<p>La arquitectura de Hadoop se basa en la sinergia de sus componentes. Cada m\u00f3dulo desempe\u00f1a un papel esencial en el procesamiento y almacenamiento de datos a gran escala, trabajando de manera conjunta para ofrecer un entorno completo y robusto.</p> <ol> <li> <p>Ingesti\u00f3n de Datos: Los datos se recopilan mediante herramientas como Flume o Sqoop y se almacenan en HDFS.</p> </li> <li> <p>Gesti\u00f3n de Recursos: YARN administra los recursos del cl\u00faster, asegurando que las tareas se distribuyan de manera eficiente.</p> </li> <li> <p>Procesamiento de Datos: Se realiza mediante MapReduce, Spark, Pig o Hive, dependiendo del tipo de an\u00e1lisis requerido.</p> </li> <li> <p>Acceso y An\u00e1lisis: Hive proporciona un lenguaje similar a SQL para consultar y analizar datos, mientras que HBase permite el acceso en tiempo real.</p> </li> <li> <p>Automatizaci\u00f3n de Flujos de Trabajo: Oozie coordina la ejecuci\u00f3n de trabajos y la automatizaci\u00f3n de tareas repetitivas en el cl\u00faster.</p> </li> </ol>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#ejemplo-completo-de-integracion-de-la-arquitectura-hadoop-en-javascript","title":"\ud83c\udf1f Ejemplo Completo de Integraci\u00f3n de la Arquitectura Hadoop en JavaScript","text":"<p>Para ilustrar c\u00f3mo todos estos componentes trabajan en conjunto, veamos un ejemplo pr\u00e1ctico de integraci\u00f3n utilizando JavaScript:</p> <pre><code>const hdfs = require('hdfs'); // Interacci\u00f3n con HDFS\nconst yarn = require('yarn-client'); // Gesti\u00f3n de tareas en el cl\u00faster\nconst hive = require('hive-client'); // Consultas en Hive\n\n// Conectar a HDFS\nconst hdfsClient = hdfs({\n  protocol: 'http',\n  hostname: 'localhost',\n  port: 9870\n});\n\n// Subir datos a HDFS\nhdfsClient.createFile('/user/data.txt', 'Hadoop es un sistema distribuido', (err) =&gt; {\n  if (err) {\n    console.error('Error al crear archivo en HDFS:', err);\n  } else {\n    console.log('Archivo creado en HDFS.');\n  }\n});\n\n// Ejecutar tarea con YARN\nconst yarnClient = new yarn.Client();\nyarnClient.submitJob('analyze-data', '/user/data.txt', (err) =&gt; {\n  if (err) {\n    console.error('Error ejecutando trabajo en YARN:', err);\n  } else {\n    console.log('Trabajo completado en YARN.');\n  }\n});\n\n// Consultar resultados en Hive\nconst hiveClient = hive.createClient({ host: 'localhost', port: 10000 });\n\nhiveClient.connect().then(() =&gt; {\n  hiveClient.query('SELECT * FROM logs WHERE event=\"Hadoop\";', (err, results) =&gt; {\n    if (err) {\n      console.error('Error en la consulta Hive:', err);\n    } else {\n      console.log('Resultados de la consulta Hive:', results);\n    }\n  });\n});\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#conclusion","title":"\ud83d\ude80 Conclusi\u00f3n","text":"<p>La arquitectura de Hadoop es un ejemplo brillante de c\u00f3mo los sistemas distribuidos pueden transformar la manera en que manejamos y analizamos datos masivos. Con una combinaci\u00f3n de almacenamiento robusto, gesti\u00f3n eficiente de recursos y capacidades avanzadas de procesamiento, Hadoop se ha convertido en la columna vertebral del Big Data moderno. Ya sea que est\u00e9s trabajando en an\u00e1lisis de datos, modelado predictivo o simplemente necesites un sistema escalable y resistente, la arquitectura de Hadoop proporciona las herramientas necesarias para desbloquear el verdadero potencial de tus datos. \ud83c\udf10</p>"},{"location":"ut1-introduccion-a-hadoop/indicedetareas/","title":"\u00cdndice de pr\u00e1cticas y tareas","text":"<p>Este \u00edndice incluye todas las pr\u00e1cticas guiadas y tareas correspondientes al curso. Aseg\u00farate de seguir las instrucciones cuidadosamente para cada pr\u00e1ctica y tarea, y consulta los materiales de apoyo cuando sea necesario.</p>"},{"location":"ut1-introduccion-a-hadoop/indicedetareas/#practicas-guiadas","title":"Pr\u00e1cticas guiadas","text":"<p>Las pr\u00e1cticas guiadas est\u00e1n dise\u00f1adas para que sigas un conjunto de instrucciones paso a paso y te familiarices con el entorno y las herramientas utilizadas en Big Data. Cada pr\u00e1ctica cubre un aspecto clave del entorno de Hadoop y su ecosistema.</p> <ol> <li> <p>Pr\u00e1ctica Instalaci\u00f3n Hadoop Single Node</p> <ul> <li>Descripci\u00f3n: Esta pr\u00e1ctica cubre el proceso de instalaci\u00f3n de Hadoop en un entorno de un solo nodo. El objetivo es configurar un cl\u00faster Hadoop b\u00e1sico en modo pseudo-distribuido en tu m\u00e1quina local.</li> <li>Objetivos: <ul> <li>Configurar Hadoop en un \u00fanico nodo.</li> <li>Ejecutar comandos b\u00e1sicos de HDFS.</li> <li>Verificar la instalaci\u00f3n utilizando los servicios de Hadoop.</li> </ul> </li> </ul> </li> <li> <p>Pr\u00e1ctica Inicial HDFS</p> <ul> <li>Descripci\u00f3n: Esta pr\u00e1ctica introduce el sistema de archivos distribuido de Hadoop (HDFS). Aprender\u00e1s a subir y descargar archivos en el sistema y a utilizar comandos b\u00e1sicos para manipular los datos.</li> <li>Objetivos: <ul> <li>Subir, descargar y listar archivos en HDFS.</li> <li>Utilizar comandos esenciales como <code>hdfs dfs -put</code>, <code>hdfs dfs -get</code>, y <code>hdfs dfs -ls</code>.</li> <li>Verificar la replicaci\u00f3n de los bloques y la ubicaci\u00f3n de los archivos en HDFS.</li> </ul> </li> </ul> </li> </ol>"},{"location":"ut1-introduccion-a-hadoop/indicedetareas/#tareas","title":"Tareas","text":"<p>Las tareas son ejercicios que complementan las pr\u00e1cticas guiadas y tienen como objetivo reforzar los conocimientos adquiridos. Algunas tareas requerir\u00e1n investigaci\u00f3n adicional y la implementaci\u00f3n de soluciones m\u00e1s avanzadas.</p> <ol> <li>Introducci\u00f3n al Big Data<ul> <li>Descripci\u00f3n: Esta tarea introductoria cubre los conceptos clave de Big Data, sus desaf\u00edos, y las tecnolog\u00edas fundamentales en el ecosistema Hadoop.</li> <li>Objetivos:<ul> <li>Definir Big Data y sus caracter\u00edsticas (volumen, velocidad, variedad, etc.).</li> <li>Explicar la importancia de Hadoop en el procesamiento de grandes vol\u00famenes de datos.</li> <li>Realizar una breve investigaci\u00f3n sobre casos de uso de Big Data en la industria.</li> </ul> </li> </ul> </li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/1tareaintroductoria/","title":"Tarea Introducci\u00f3n al Big Data","text":"<ol> <li> <p>Selecciona una Empresa o Tem\u00e1tica:</p> <ul> <li>Elige una empresa real (por ejemplo, Amazon, Facebook, Spotify, etc.) o una tem\u00e1tica espec\u00edfica (por ejemplo, an\u00e1lisis de datos m\u00e9dicos, optimizaci\u00f3n de rutas log\u00edsticas, etc.) que recoja y maneje grandes vol\u00famenes de datos.</li> <li>Explica brevemente c\u00f3mo esa empresa o tem\u00e1tica seleccionada obtiene datos.</li> </ul> </li> <li> <p>Describe Cada Fase del Procesamiento de Datos:</p> <ul> <li> <p>Recolecci\u00f3n de Datos \ud83d\udce5: Explica de d\u00f3nde y c\u00f3mo tu empresa seleccionada recolecta los datos (por ejemplo, redes sociales, sensores, historiales de compra, etc.).</p> </li> <li> <p>Recopilaci\u00f3n: Describe c\u00f3mo los datos recolectados se organizan en un sistema centralizado o base de datos para su posterior an\u00e1lisis.</p> </li> <li> <p>Preprocesamiento o Limpieza de Datos \ud83e\uddf9: Explica c\u00f3mo la empresa limpia los datos (eliminaci\u00f3n de datos duplicados, correcci\u00f3n de errores, conversi\u00f3n de formatos) antes de analizarlos.</p> </li> <li> <p>Procesamiento \ud83d\udda5\ufe0f: Detalla c\u00f3mo se procesan los datos (algoritmos, machine learning, modelos predictivos) para extraer informaci\u00f3n \u00fatil.</p> </li> <li> <p>Interpretaci\u00f3n y Visualizaci\u00f3n \ud83d\udcca: Describe c\u00f3mo se presentan los resultados a los usuarios o tomadores de decisiones (dashboards, gr\u00e1ficos, informes).</p> </li> <li> <p>An\u00e1lisis \ud83e\udde0: Comenta c\u00f3mo la empresa utiliza los resultados del an\u00e1lisis para tomar decisiones estrat\u00e9gicas o mejorar sus operaciones.</p> </li> <li> <p>Almacenamiento: Finalmente, explica c\u00f3mo y d\u00f3nde se almacenan los datos procesados y los resultados para su uso futuro (en la nube, bases de datos, sistemas locales).</p> </li> </ul> </li> <li> <p>Comenta investigando al menos 3Vs m\u00e1s de las mencionadas</p> </li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/","title":"Pr\u00e1ctica Guiada: Implementaci\u00f3n de un Nodo Hadoop con Docker","text":""},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/#1-introduccion","title":"1. Introducci\u00f3n","text":"<p>En esta pr\u00e1ctica, vas a configurar un cl\u00faster Hadoop de un solo nodo utilizando Docker. Hadoop es un framework que permite el procesamiento de grandes cantidades de datos distribuidos en varios nodos. Para fines educativos, configuraremos un cl\u00faster de un solo nodo. Docker facilitar\u00e1 el proceso de instalaci\u00f3n, ya que encapsula todo en un contenedor aislado del sistema operativo principal.</p>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/#2-requisitos-previos","title":"2. Requisitos Previos","text":"<ul> <li>Docker instalado: Aseg\u00farate de tener Docker instalado y funcionando en tu m\u00e1quina. Si no lo tienes instalado, puedes descargarlo desde https://www.docker.com/products/docker-desktop.</li> <li>Git instalado: Se necesita Git para clonar el repositorio de configuraci\u00f3n.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/#3-clonar-el-repositorio-del-proyecto","title":"3. Clonar el repositorio del proyecto","text":"<p>Para empezar, vamos a clonar el repositorio donde ya est\u00e1 configurada la imagen de Hadoop. Este repositorio contiene un archivo <code>Dockerfile</code>, que es un script que define c\u00f3mo se debe crear la imagen de Docker con todos los servicios y configuraciones de Hadoop.</p> <pre><code>git clone https://github.com/rancavil/hadoop-single-node-cluster.git\ncd hadoop-single-node-cluster\n</code></pre> <ul> <li>\u00bfQu\u00e9 es esto?: El comando <code>git clone</code> descarga una copia local del repositorio alojado en GitHub, y <code>cd</code> te lleva al directorio reci\u00e9n clonado.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/#4-construir-la-imagen-docker-de-hadoop","title":"4. Construir la imagen Docker de Hadoop","text":"<p>Ahora que tenemos los archivos de configuraci\u00f3n, es momento de construir una imagen Docker. Esta imagen contendr\u00e1 todo lo necesario para correr Hadoop, incluyendo Java, Hadoop y las configuraciones necesarias para un nodo \u00fanico.</p> <pre><code>docker build -t hadoop .\n</code></pre> <ul> <li>Explicaci\u00f3n: <ul> <li><code>docker build</code> es el comando que construye una imagen a partir de un archivo <code>Dockerfile</code>.</li> <li><code>-t hadoop</code> asigna la etiqueta \"hadoop\" a esta imagen, lo que nos facilitar\u00e1 su referencia m\u00e1s adelante.</li> <li>El <code>.</code> indica que el contexto para la construcci\u00f3n es el directorio actual.</li> </ul> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/#5-crear-y-correr-el-contenedor-hadoop","title":"5. Crear y correr el contenedor Hadoop","text":"<p>Una vez que la imagen est\u00e1 construida, vamos a ejecutar un contenedor basado en ella. Un contenedor es una instancia de la imagen, ejecut\u00e1ndose como si fuera una peque\u00f1a m\u00e1quina virtual, pero m\u00e1s ligera y eficiente.</p> <pre><code>docker run --name hadoop-container -p 9864:9864 -p 9870:9870 -p 8088:8088 -p 9000:9000 --hostname hadoop hadoop\n</code></pre> <ul> <li>Explicaci\u00f3n:<ul> <li><code>docker run</code>: Este comando crea y corre un contenedor.</li> <li><code>--name hadoop-container</code>: Asigna el nombre \"hadoop-container\" al contenedor.</li> <li><code>-p 9864:9864 -p 9870:9870 -p 8088:8088 -p 9000:9000</code>: Mapea los puertos del contenedor a tu m\u00e1quina local. Esto te permite acceder a la interfaz web de Hadoop desde tu navegador y establecer la conexi\u00f3n entre el contenedor y tu m\u00e1quina para el uso de HDFS.<ul> <li><code>9864</code>: Puerto del DataNode.</li> <li><code>9870</code>: Puerto del NameNode (UI para monitoreo de HDFS).</li> <li><code>8088</code>: Puerto del ResourceManager (UI para monitoreo de trabajos en YARN).</li> <li><code>9000</code>: Puerto de HDFS para el NameNode.</li> </ul> </li> <li><code>--hostname hadoop</code>: Asigna el nombre de host \"hadoop\" al contenedor.</li> </ul> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/#6-abrir-una-terminal-dentro-del-contenedor","title":"6. Abrir una terminal dentro del contenedor","text":"<p>Para interactuar con Hadoop dentro del contenedor, necesitas abrir una terminal en su interior. Hay dos formas de hacerlo:</p> <ul> <li> <p>Modo gr\u00e1fico: Si est\u00e1s usando Docker Desktop, puedes acceder al contenedor desde la interfaz gr\u00e1fica seleccionando el contenedor \"hadoop-container\" y abriendo una terminal desde ah\u00ed.</p> </li> <li> <p>Modo terminal: Si prefieres usar la terminal, ejecuta el siguiente comando:</p> </li> </ul> <pre><code>docker exec -i -t hadoop-container /bin/bash\n</code></pre> <ul> <li>Explicaci\u00f3n:<ul> <li><code>docker exec</code>: Permite ejecutar comandos dentro de un contenedor en ejecuci\u00f3n.</li> <li><code>-i -t</code>: Estas opciones te permiten interactuar de manera interactiva con el contenedor (modo terminal).</li> <li><code>hadoop-container</code>: Es el nombre del contenedor que ejecutaste anteriormente.</li> <li><code>/bin/bash</code>: Especifica que quieres abrir una shell Bash dentro del contenedor.</li> </ul> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/#7-probar-hadoop-y-hdfs","title":"7. Probar Hadoop y HDFS","text":"<p>Ahora que est\u00e1s dentro del contenedor, es momento de probar que Hadoop y su sistema de archivos HDFS est\u00e1n funcionando correctamente. Comienza creando un directorio en HDFS.</p> <pre><code>hdfs dfs -mkdir /user\n</code></pre> <p>Luego, lista los archivos del sistema de archivos HDFS recursivamente:</p> <pre><code>hdfs dfs -ls -R /\n</code></pre> <ul> <li>Explicaci\u00f3n:<ul> <li><code>hdfs dfs</code>: Este es el comando que utilizas para interactuar con el sistema de archivos distribuido de Hadoop (HDFS).</li> <li><code>-mkdir /user</code>: Crea un directorio llamado \"user\" en el sistema de archivos HDFS.</li> <li><code>-ls -R /</code>: Lista de manera recursiva todos los archivos en HDFS, comenzando desde la ra\u00edz.</li> </ul> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/#8-acceder-a-la-interfaz-web-de-hadoop","title":"8. Acceder a la interfaz web de Hadoop","text":"<p>Puedes acceder a las interfaces de monitoreo de Hadoop desde tu navegador utilizando las siguientes direcciones:</p> <ul> <li>NameNode (HDFS): http://localhost:9870</li> <li>ResourceManager (YARN): http://localhost:8088</li> </ul> <p>Estas interfaces te permiten ver el estado del cl\u00faster, los nodos disponibles, y los trabajos que se est\u00e1n ejecutando.</p>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/#9-apagar-el-contenedor","title":"9. Apagar el contenedor","text":"<p>Cuando termines de trabajar, puedes detener y eliminar el contenedor. Para detener el contenedor sin eliminarlo, ejecuta:</p> <pre><code>docker stop hadoop-container\n</code></pre> <p>Si deseas eliminar el contenedor completamente:</p> <pre><code>docker rm hadoop-container\n</code></pre> <ul> <li>Explicaci\u00f3n:<ul> <li><code>docker stop</code>: Detiene el contenedor en ejecuci\u00f3n.</li> <li><code>docker rm</code>: Elimina el contenedor de Docker, liberando espacio.</li> </ul> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/#10-conclusion","title":"10. Conclusi\u00f3n","text":"<p>Has completado la pr\u00e1ctica para configurar un cl\u00faster Hadoop de un solo nodo usando Docker. Este entorno te permite experimentar con las funcionalidades principales de Hadoop sin necesidad de una configuraci\u00f3n compleja de varios nodos o sistemas distribuidos. Con este entorno, puedes practicar la gesti\u00f3n de archivos en HDFS, ejecutar trabajos en YARN y familiarizarte con las interfaces de monitoreo de Hadoop.</p>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/1practicainicial/","title":"Pr\u00e1ctica: Gesti\u00f3n de archivos en HDFS en un escenario real","text":""},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/1practicainicial/#contexto-del-caso","title":"Contexto del Caso","text":"<p>Imagina que eres parte del equipo de an\u00e1lisis de datos de una empresa de telecomunicaciones. Tu tarea consiste en gestionar el almacenamiento de grandes vol\u00famenes de registros de llamadas telef\u00f3nicas. Estos registros se almacenar\u00e1n en HDFS para un posterior an\u00e1lisis distribuido. A lo largo de esta pr\u00e1ctica, simular\u00e1s la subida, consulta y gesti\u00f3n de archivos en HDFS utilizando los registros de llamadas de distintos meses.</p>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/1practicainicial/#entregable","title":"Entregable","text":"<ul> <li>Un documento PDF donde se muestren los diferentes pasos a seguir indicados a continuaci\u00f3n, incluyendo capturas y algo de explicaci\u00f3n.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/1practicainicial/#pasos-a-seguir","title":"Pasos a Seguir","text":""},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/1practicainicial/#1-preparacion-de-archivos-locales","title":"1. Preparaci\u00f3n de archivos locales","text":"<ul> <li>Simula dos archivos de texto que contengan los registros de llamadas:<ul> <li><code>llamadas_enero.txt</code>: Este archivo contiene las llamadas realizadas en enero.</li> <li><code>llamadas_febrero.txt</code>: Contiene los registros de las llamadas de febrero.</li> </ul> </li> </ul> <p>Ejemplo de contenido de cada archivo: <pre><code>ID_Llamada, Fecha, Duraci\u00f3n, N\u00famero_Destino\n001, 2024-01-05 14:30:00, 5 minutos, +34987654321\n002, 2024-01-06 16:45:00, 10 minutos, +34123456789\n</code></pre></p>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/1practicainicial/#2-subir-archivos-a-hdfs","title":"2. Subir archivos a HDFS","text":"<ul> <li> <p>Crea un directorio en HDFS donde se almacenar\u00e1n estos archivos:      <pre><code>hdfs dfs -mkdir /user/empresa_telecom/registros_llamadas\n</code></pre></p> </li> <li> <p>Sube los archivos locales <code>llamadas_enero.txt</code> y <code>llamadas_febrero.txt</code> a HDFS usando el comando <code>copyFromLocal</code>:      <pre><code>hdfs dfs -copyFromLocal /ruta/local/llamadas_enero.txt /user/empresa_telecom/registros_llamadas/\nhdfs dfs -copyFromLocal /ruta/local/llamadas_febrero.txt /user/empresa_telecom/registros_llamadas/\n</code></pre></p> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/1practicainicial/#3-verificacion-de-los-archivos-subidos","title":"3. Verificaci\u00f3n de los archivos subidos","text":"<ul> <li> <p>Lista los contenidos del directorio en HDFS para confirmar que los archivos se han subido correctamente:      <pre><code>hdfs dfs -ls /user/empresa_telecom/registros_llamadas\n</code></pre></p> </li> <li> <p>Muestra el contenido de los archivos para verificar su integridad:      <pre><code>hdfs dfs -cat /user/empresa_telecom/registros_llamadas/llamadas_enero.txt\nhdfs dfs -cat /user/empresa_telecom/registros_llamadas/llamadas_febrero.txt\n</code></pre></p> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/1practicainicial/#4-renombrar-los-archivos-para-una-mejor-organizacion","title":"4. Renombrar los archivos para una mejor organizaci\u00f3n","text":"<ul> <li>Sup\u00f3n que la empresa ha decidido que los archivos deben seguir un formato m\u00e1s organizado con el mes en min\u00fasculas. Renombra los archivos:      <pre><code>hdfs dfs -mv /user/empresa_telecom/registros_llamadas/llamadas_enero.txt /user/empresa_telecom/registros_llamadas/llamadas_enero2024.txt\nhdfs dfs -mv /user/empresa_telecom/registros_llamadas/llamadas_febrero.txt /user/empresa_telecom/registros_llamadas/llamadas_febrero2024.txt\n</code></pre></li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/1practicainicial/#5-descargar-archivos-desde-hdfs-para-analisis-local","title":"5. Descargar archivos desde HDFS para an\u00e1lisis local","text":"<ul> <li> <p>Descarga los archivos de registros desde HDFS a tu sistema local para analizarlos utilizando una herramienta de an\u00e1lisis de datos:      <pre><code>hdfs dfs -get /user/empresa_telecom/registros_llamadas/llamadas_enero2024.txt /ruta/local/analisis/\nhdfs dfs -get /user/empresa_telecom/registros_llamadas/llamadas_febrero2024.txt /ruta/local/analisis/\n</code></pre></p> </li> <li> <p>Verifica que los archivos descargados sean los mismos que los originales comparando su contenido.</p> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/1practicainicial/#6-eliminacion-de-archivos-antiguos-de-hdfs","title":"6. Eliminaci\u00f3n de archivos antiguos de HDFS","text":"<ul> <li> <p>Dado que los archivos han sido descargados y procesados, la empresa decide eliminar los registros de enero del sistema HDFS para liberar espacio. Utiliza el comando <code>rm</code> para eliminar el archivo:      <pre><code>hdfs dfs -rm /user/empresa_telecom/registros_llamadas/llamadas_enero2024.txt\n</code></pre></p> </li> <li> <p>Verifica que el archivo ha sido eliminado correctamente listando de nuevo los contenidos del directorio:      <pre><code>hdfs dfs -ls /user/empresa_telecom/registros_llamadas\n</code></pre></p> </li> <li> <p>Si ya no necesitas el directorio donde se almacenaron los registros de llamadas, elim\u00ednalo tambi\u00e9n:      <pre><code>hdfs dfs -rmdir /user/empresa_telecom/registros_llamadas\n</code></pre></p> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/1instalacionhadoopec2/","title":"Instalaci\u00f3n de Hadoop en AWS (EC2)","text":""},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/1instalacionhadoopec2/#paso-1-conectate-a-la-instancia-ec2","title":"Paso 1: Con\u00e9ctate a la instancia EC2","text":"<p>Primero, con\u00e9ctate a tu instancia de EC2 mediante SSH:</p> <pre><code>ssh -i /path/to/your-key.pem ubuntu@&lt;IP_de_la_instancia&gt;\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/1instalacionhadoopec2/#paso-2-actualiza-el-sistema-e-instala-java","title":"Paso 2: Actualiza el sistema e instala Java","text":"<p>Hadoop requiere Java, as\u00ed que primero instala Java antes de continuar.</p> <ol> <li>Actualiza los paquetes de la instancia EC2:    <pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y\n</code></pre></li> <li>Instala Java (Hadoop requiere Java 8):    <pre><code>sudo apt install openjdk-8-jdk -y\n</code></pre></li> <li>Verifica la instalaci\u00f3n de Java:    <pre><code>java -version\n</code></pre></li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/1instalacionhadoopec2/#paso-3-descarga-e-instala-hadoop","title":"Paso 3: Descarga e instala Hadoop","text":"<ol> <li>Descarga la \u00faltima versi\u00f3n de Hadoop desde el sitio oficial de Apache:    <pre><code>wget https://downloads.apache.org/hadoop/common/hadoop-3.3.5/hadoop-3.3.5.tar.gz\n</code></pre></li> <li>Extrae el archivo descargado:    <pre><code>tar -xzvf hadoop-3.3.5.tar.gz\n</code></pre></li> <li>Mueve Hadoop a una ubicaci\u00f3n adecuada, por ejemplo, <code>/usr/local/</code>:    <pre><code>sudo mv hadoop-3.3.5 /usr/local/hadoop\n</code></pre></li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/1instalacionhadoopec2/#paso-4-configura-variables-de-entorno","title":"Paso 4: Configura variables de entorno","text":"<p>Configura las variables de entorno necesarias para Hadoop.</p> <ol> <li>Edita el archivo <code>.bashrc</code> del usuario actual:    <pre><code>nano ~/.bashrc\n</code></pre></li> <li>Agrega las siguientes l\u00edneas al final del archivo:</li> </ol> <pre><code>export HADOOP_HOME=/usr/local/hadoop\nexport HADOOP_INSTALL=$HADOOP_HOME\nexport HADOOP_MAPRED_HOME=$HADOOP_HOME\nexport HADOOP_COMMON_HOME=$HADOOP_HOME\nexport HADOOP_HDFS_HOME=$HADOOP_HOME\nexport YARN_HOME=$HADOOP_HOME\nexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native\nexport PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin\nexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\nexport HADOOP_OPTS=\"$HADOOP_OPTS -Djava.library.path=$HADOOP_HOME/lib/native\"\n</code></pre> <ol> <li> <p>Guarda y cierra el archivo.</p> </li> <li> <p>Aplica los cambios:    <pre><code>source ~/.bashrc\n</code></pre></p> </li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/1instalacionhadoopec2/#paso-5-configura-hadoop","title":"Paso 5: Configura Hadoop","text":"<p>Ahora edita los archivos de configuraci\u00f3n de Hadoop.</p> <ol> <li>Edita el archivo <code>hadoop-env.sh</code> para configurar <code>JAVA_HOME</code>:    <pre><code>sudo nano /usr/local/hadoop/etc/hadoop/hadoop-env.sh\n</code></pre></li> <li>Aseg\u00farate de que la variable <code>JAVA_HOME</code> est\u00e9 configurada correctamente:    <pre><code>export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n</code></pre></li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/1instalacionhadoopec2/#paso-6-configura-los-archivos-core-sitexml-y-hdfs-sitexml","title":"Paso 6: Configura los archivos core-site.xml y hdfs-site.xml","text":"<ol> <li>Edita el archivo <code>core-site.xml</code>:    <pre><code>nano /usr/local/hadoop/etc/hadoop/core-site.xml\n</code></pre> <pre><code>&lt;configuration&gt;\n   &lt;property&gt;\n      &lt;name&gt;fs.defaultFS&lt;/name&gt;\n      &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;\n      &lt;description&gt;NameNode URI&lt;/description&gt;\n   &lt;/property&gt;\n   &lt;property&gt;\n      &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;\n      &lt;value&gt;/usr/local/hadoop/tmp&lt;/value&gt;\n      &lt;description&gt;Directorio temporal de Hadoop&lt;/description&gt;\n   &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre></li> <li>Edita el archivo <code>hdfs-site.xml</code>:</li> </ol> <pre><code>nano /usr/local/hadoop/etc/hadoop/hdfs-site.xml\n</code></pre> <pre><code>&lt;configuration&gt;\n   &lt;property&gt;\n      &lt;name&gt;dfs.replication&lt;/name&gt;\n      &lt;value&gt;1&lt;/value&gt;\n      &lt;description&gt;Factor de replicaci\u00f3n de HDFS (para modo pseudo-distribuido es 1)&lt;/description&gt;\n   &lt;/property&gt;\n\n   &lt;property&gt;\n      &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n      &lt;value&gt;file:///usr/local/hadoop/hdfs/namenode&lt;/value&gt;\n      &lt;description&gt;Directorio para los archivos del NameNode&lt;/description&gt;\n   &lt;/property&gt;\n\n   &lt;property&gt;\n      &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n      &lt;value&gt;file:///usr/local/hadoop/hdfs/datanode&lt;/value&gt;\n      &lt;description&gt;Directorio para los bloques de datos del DataNode&lt;/description&gt;\n   &lt;/property&gt;\n\n   &lt;property&gt;\n      &lt;name&gt;dfs.permissions&lt;/name&gt;\n      &lt;value&gt;false&lt;/value&gt;\n      &lt;description&gt;Desactiva los permisos de HDFS para simplificar la configuraci\u00f3n&lt;/description&gt;\n   &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <ol> <li>Crea las carpetas <code>hdfs/namenode</code> y <code>hdfs/datanode</code>:</li> </ol> <pre><code>mkdir -p /usr/local/hadoop/tmp\nmkdir -p /usr/local/hadoop/hdfs/namenode\nmkdir -p /usr/local/hadoop/hdfs/datanode\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/1instalacionhadoopec2/#paso-6-formatea-el-sistema-de-archivos-de-hadoop","title":"Paso 6: Formatea el sistema de archivos de Hadoop","text":"<p>Antes de iniciar Hadoop, necesitas formatear el sistema de archivos.</p> <ol> <li>Ejecuta el siguiente comando para formatear el sistema de archivos de HDFS:    <pre><code>hdfs namenode -format\n</code></pre></li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/1instalacionhadoopec2/#paso-7-inicia-hadoop","title":"Paso 7: Inicia Hadoop","text":"<p>Para iniciar Hadoop, ejecuta los siguientes comandos:</p> <ol> <li>Inicia el NameNode y el DataNode:    <pre><code>start-dfs.sh\n</code></pre></li> <li>Inicia YARN:    <pre><code>start-yarn.sh\n</code></pre></li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/1instalacionhadoopec2/#paso-9-deten-hadoop","title":"Paso 9: Det\u00e9n Hadoop","text":"<p>Para detener Hadoop, usa los siguientes comandos:</p> <ol> <li>Det\u00e9n HDFS:    <pre><code>stop-dfs.sh\n</code></pre></li> <li>Det\u00e9n YARN:    <pre><code>stop-yarn.sh\n</code></pre></li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/1instalacionhadoopec2/#posibles-errores","title":"Posibles errores","text":""},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/1instalacionhadoopec2/#permission-denied-publickey","title":"Permission denied (publickey).","text":"<p>El error \"Permission denied (publickey)\" al ejecutar <code>start-yarn.sh</code> o similar indica que el sistema est\u00e1 intentando iniciar los nodemanagers y resourcemanager en los nodos locales de Hadoop, pero est\u00e1 fallando la autenticaci\u00f3n SSH.</p> <p>Hadoop, por defecto, intenta comunicarse entre nodos a trav\u00e9s de SSH para iniciar YARN, y parece que no se ha configurado correctamente el acceso SSH entre nodos. Si est\u00e1s ejecutando un cl\u00faster pseudo-distribuido (es decir, en una sola m\u00e1quina), necesitar\u00e1s configurar SSH para que el usuario actual pueda conectarse a localhost sin contrase\u00f1a. Aqu\u00ed est\u00e1n los pasos para solucionar este problema:</p>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/1instalacionhadoopec2/#paso-1-configura-ssh-sin-contrasena","title":"Paso 1: Configura SSH sin contrase\u00f1a","text":"<ol> <li>Genera un par de claves SSH si no tienes uno ya creado:    Ejecuta el siguiente comando y acepta los valores predeterminados:</li> </ol> <pre><code>ssh-keygen -t rsa -P \"\"\n</code></pre> <p>Esto crear\u00e1 una clave SSH p\u00fablica y privada en el directorio <code>~/.ssh/</code> (por defecto).</p> <ol> <li>Agrega la clave p\u00fablica al archivo <code>authorized_keys</code> para permitir la autenticaci\u00f3n SSH sin contrase\u00f1a en localhost:    <pre><code>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys\n</code></pre></li> <li>Aseg\u00farate de que los permisos sean correctos:    <pre><code>chmod 700 ~/.ssh\nchmod 600 ~/.ssh/authorized_keys\n</code></pre></li> <li>Prueba la conexi\u00f3n SSH a localhost para verificar que puedas conectarte sin contrase\u00f1a:</li> </ol> <pre><code>ssh localhost\n</code></pre> <p>Si no te pide una contrase\u00f1a y te deja entrar, la configuraci\u00f3n es correcta. Puedes salir de la sesi\u00f3n con <code>exit</code>.</p>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/1instalacionhadoopec2/#paso-2-inicia-yarn-de-nuevo","title":"Paso 2: Inicia YARN de nuevo","text":"<p>Despu\u00e9s de configurar SSH, intenta iniciar YARN de nuevo:</p> <pre><code>start-yarn.sh\n</code></pre> <p>Esto deber\u00eda iniciar correctamente tanto el ResourceManager como los NodeManagers sin el error de \"Permission denied\".</p>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/2primerospasoshdfs/","title":"Pr\u00e1ctica real con comandos de HDFS en EC2","text":""},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/2primerospasoshdfs/#1-crear-un-directorio-en-hdfs","title":"1. Crear un directorio en HDFS","text":"<p>Crea un directorio llamado <code>/practica_hdfs</code> dentro del sistema de archivos distribuido (HDFS).</p> <pre><code>hdfs dfs -mkdir /practica_hdfs\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/2primerospasoshdfs/#2-subir-un-archivo-desde-el-sistema-local-a-hdfs","title":"2. Subir un archivo desde el sistema local a HDFS","text":"<p>Primero, crea un archivo de texto de prueba en tu sistema local:</p> <pre><code>echo \"Este es un archivo de prueba para HDFS\" &gt; archivo_prueba.txt\n</code></pre> <p>Luego, sube el archivo a HDFS dentro del directorio <code>/practica_hdfs</code>:</p> <pre><code>hdfs dfs -put archivo_prueba.txt /practica_hdfs/\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/2primerospasoshdfs/#3-listar-los-archivos-en-un-directorio-de-hdfs","title":"3. Listar los archivos en un directorio de HDFS","text":"<p>Verifica que el archivo fue subido correctamente listando los contenidos del directorio <code>/practica_hdfs</code>:</p> <pre><code>hdfs dfs -ls /practica_hdfs\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/2primerospasoshdfs/#4-ver-el-contenido-de-un-archivo-en-hdfs","title":"4. Ver el contenido de un archivo en HDFS","text":"<p>Muestra el contenido del archivo directamente en la terminal:</p> <pre><code>hdfs dfs -cat /practica_hdfs/archivo_prueba.txt\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/2primerospasoshdfs/#5-mover-o-renombrar-un-archivo-en-hdfs","title":"5. Mover o renombrar un archivo en HDFS","text":"<p>Renombra o mueve el archivo dentro del directorio de HDFS:</p> <pre><code>hdfs dfs -mv /practica_hdfs/archivo_prueba.txt /practica_hdfs/archivo_renombrado.txt\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/2primerospasoshdfs/#6-descargar-un-archivo-desde-hdfs-al-sistema-local","title":"6. Descargar un archivo desde HDFS al sistema local","text":"<p>Descarga el archivo desde HDFS a tu sistema de archivos local (EC2):</p> <pre><code>hdfs dfs -get /practica_hdfs/archivo_renombrado.txt archivo_descargado.txt\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/2primerospasoshdfs/#7-eliminar-un-archivo-de-hdfs","title":"7. Eliminar un archivo de HDFS","text":"<p>Elimina el archivo de HDFS:</p> <pre><code>hdfs dfs -rm /practica_hdfs/archivo_renombrado.txt\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/2primerospasoshdfs/#8-eliminar-un-directorio-en-hdfs","title":"8. Eliminar un directorio en HDFS","text":"<p>Elimina el directorio <code>/practica_hdfs</code>:</p> <pre><code>hdfs dfs -rm -r /practica_hdfs\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/2primerospasoshdfs/#opcional-comandos-adicionales-para-profundizar","title":"Opcional: Comandos adicionales para profundizar","text":""},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/2primerospasoshdfs/#ver-informacion-sobre-el-espacio-disponible-en-hdfs","title":"Ver informaci\u00f3n sobre el espacio disponible en HDFS","text":"<p>Verifica el uso de espacio en HDFS en formato legible:</p> <pre><code>hdfs dfs -df -h\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/2primerospasoshdfs/#contar-archivos-directorios-y-tamano-total-de-un-directorio-en-hdfs","title":"Contar archivos, directorios y tama\u00f1o total de un directorio en HDFS","text":"<p>Usa este comando para contar el n\u00famero de archivos, directorios y el tama\u00f1o total en un directorio espec\u00edfico de HDFS:</p> <pre><code>hdfs dfs -count /practica_hdfs\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/2primerospasoshdfs/#comprobar-bloques-de-un-archivo-en-hdfs","title":"Comprobar bloques de un archivo en HDFS","text":"<p>Obt\u00e9n detalles sobre los bloques que forman un archivo:</p> <pre><code>hdfs fsck /practica_hdfs/archivo_renombrado.txt -files -blocks\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/3bloques/","title":"3bloques","text":""},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/3bloques/#practica-gestion-de-archivos-en-hdfs-con-datos-de-movielens","title":"Pr\u00e1ctica: Gesti\u00f3n de archivos en HDFS con datos de MovieLens","text":""},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/3bloques/#objetivo","title":"Objetivo:","text":"<p>En esta pr\u00e1ctica, aprender\u00e1s a gestionar archivos en HDFS. Descargar\u00e1s un dataset grande, lo descomprimir\u00e1s, y cargar\u00e1s uno de sus archivos en HDFS. Posteriormente, verificar\u00e1s el estado de los bloques y la replicaci\u00f3n en el sistema de archivos distribuido.</p>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/3bloques/#requisitos-previos","title":"Requisitos previos:","text":"<ul> <li>Conocimientos b\u00e1sicos de HDFS y comandos de terminal.</li> <li>Acceso a un cl\u00faster de Hadoop (puede ser local o en la nube como AWS EMR).</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/3bloques/#pasos-a-seguir","title":"Pasos a seguir:","text":""},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/3bloques/#parte-1-crear-un-directorio-en-hdfs","title":"Parte 1: Crear un directorio en HDFS","text":"<ol> <li>Crear un directorio en HDFS    El primer paso es crear un directorio donde se almacenar\u00e1n los archivos dentro del HDFS. Ejecuta el siguiente comando para crear un directorio llamado <code>prueba</code> en HDFS:    <pre><code>hdfs dfs -mkdir prueba\n</code></pre></li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/3bloques/#parte-2-descargar-el-dataset-de-movielens","title":"Parte 2: Descargar el dataset de MovieLens","text":"<ol> <li>Descargar el dataset    Ahora descargar\u00e1s un archivo de datos p\u00fablicos de MovieLens (un dataset de pel\u00edculas y valoraciones). Usa <code>wget</code> para descargar el archivo:</li> </ol> <pre><code>wget https://files.grouplens.org/datasets/movielens/ml-25m.zip\n</code></pre> <ol> <li>Verificar la descarga    Para asegurarte de que el archivo se descarg\u00f3 correctamente, lista los archivos en el directorio actual:    <pre><code>ls\n</code></pre>    Deber\u00edas ver el archivo <code>ml-25m.zip</code> en la lista.</li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/3bloques/#parte-3-descomprimir-el-archivo","title":"Parte 3: Descomprimir el archivo","text":"<ol> <li>Descomprimir el archivo zip    Descomprime el archivo descargado usando el comando <code>unzip</code>:</li> </ol> <pre><code>unzip ml-25m.zip\n</code></pre> <ol> <li>Verificar los archivos descomprimidos    Lista los archivos descomprimidos para asegurarte de que todo est\u00e1 correcto:    <pre><code>ls ml-25m/\n</code></pre>    Deber\u00edas ver varios archivos CSV, incluyendo <code>ratings.csv</code>, <code>movies.csv</code>, entre otros.</li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/3bloques/#parte-4-subir-un-archivo-a-hdfs","title":"Parte 4: Subir un archivo a HDFS","text":"<ol> <li>Subir el archivo <code>ratings.csv</code> a HDFS    Ahora, sube uno de los archivos CSV (en este caso, <code>ratings.csv</code>) al directorio <code>prueba</code> en HDFS:    <pre><code>hdfs dfs -put ml-25m/ratings.csv prueba\n</code></pre></li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/3bloques/#parte-5-verificar-la-integridad-del-archivo-en-hdfs","title":"Parte 5: Verificar la integridad del archivo en HDFS","text":"<ol> <li>Verificar el archivo con <code>fsck</code>    Utiliza el comando <code>hdfs fsck</code> para verificar la integridad del archivo <code>ratings.csv</code> en HDFS, mostrando los bloques en los que est\u00e1 dividido:</li> </ol> <pre><code>hdfs fsck prueba -files -blocks\n</code></pre> <p>Este comando te proporcionar\u00e1 detalles sobre:</p> <ul> <li>El tama\u00f1o del archivo.</li> <li>El n\u00famero de bloques en que se ha dividido el archivo.</li> <li>El estado de replicaci\u00f3n de cada bloque.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/3bloques/#parte-6-analisis-de-la-salida","title":"Parte 6: An\u00e1lisis de la salida","text":"<ol> <li>Interpretar la salida    La salida del comando <code>fsck</code> mostrar\u00e1 informaci\u00f3n sobre los bloques y la replicaci\u00f3n del archivo <code>ratings.csv</code>. Presta atenci\u00f3n a los siguientes puntos:</li> <li>Tama\u00f1o total del archivo.</li> <li>Cantidad de bloques en los que se divide.</li> <li>El n\u00famero de r\u00e9plicas para cada bloque (en este caso, deber\u00eda ser 1, ya que el factor de replicaci\u00f3n predeterminado es 1).</li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/3bloques/#parte-7-limpieza-opcional","title":"Parte 7: Limpieza (Opcional)","text":"<ol> <li>Eliminar los archivos de HDFS (opcional)    Si deseas limpiar el sistema de archivos y eliminar el archivo subido, puedes usar el siguiente comando:    <pre><code>hdfs dfs -rm -r prueba\n</code></pre></li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/3bloques/#cuestionario-de-evaluacion","title":"Cuestionario de evaluaci\u00f3n:","text":"<ol> <li>\u00bfQu\u00e9 representa un bloque en HDFS y c\u00f3mo se distribuye un archivo grande entre ellos?</li> <li>\u00bfPor qu\u00e9 es importante verificar la replicaci\u00f3n y el estado de los bloques en HDFS?</li> <li>\u00bfQu\u00e9 comando usaste para subir el archivo a HDFS? \u00bfQu\u00e9 hace espec\u00edficamente?</li> <li>Explica el prop\u00f3sito del comando <code>fsck</code> en HDFS.</li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/3bloques/#conclusion","title":"Conclusi\u00f3n:","text":"<p>Esta pr\u00e1ctica te ha permitido descargar, manipular y subir archivos a HDFS, adem\u00e1s de verificar la integridad y el estado de los bloques en un cl\u00faster de Hadoop. La comprensi\u00f3n de c\u00f3mo se gestiona un archivo en HDFS es esencial para trabajar eficientemente con grandes vol\u00famenes de datos.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/1hdfs/","title":"Comprendiendo HDFS: Una Gu\u00eda Completa","text":""},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/1hdfs/#que-es-hdfs","title":"\ud83d\ude80 \u00bfQu\u00e9 es HDFS?","text":"<p>HDFS significa Hadoop Distributed File System (Sistema de Archivos Distribuido de Hadoop), y es la columna vertebral de la capacidad de Hadoop para manejar grandes cantidades de datos. Se origin\u00f3 a partir de GFS (Google File System) y est\u00e1 dise\u00f1ado para almacenar y gestionar grandes vol\u00famenes de datos de manera eficiente, distribuy\u00e9ndolos a trav\u00e9s de varios nodos en un cl\u00faster.</p> <p>HDFS proporciona tolerancia a fallos y acceso de alto rendimiento a grandes conjuntos de datos, lo que lo hace ideal para aplicaciones que manejan procesamiento a gran escala, como miner\u00eda de datos, aprendizaje autom\u00e1tico y an\u00e1lisis.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/1hdfs/#arquitectura-de-hdfs-nodos-y-tipos","title":"\ud83e\udde9 Arquitectura de HDFS: Nodos y Tipos","text":"<p>La arquitectura de HDFS gira en torno a dos componentes principales:</p> <ol> <li>NameNode: El nodo maestro que controla los metadatos de los archivos y la estructura del directorio.</li> <li>DataNodes: Estos son los nodos trabajadores responsables de almacenar los datos reales en bloques. Cada bloque tiene un tama\u00f1o predeterminado de 128 MB, pero este valor es configurable.</li> </ol> <pre><code>graph TD;\n    NameNode --&gt;|Metadatos| DataNode1;\n    NameNode --&gt;|Metadatos| DataNode2;\n    NameNode --&gt;|Metadatos| DataNode3;\n    DataNode1 --&gt; Block1;\n    DataNode2 --&gt; Block2;\n    DataNode3 --&gt; Block3;</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/1hdfs/#tipos-clave-de-nodos","title":"Tipos Clave de Nodos:","text":"<ul> <li>NameNode (Maestro): Almacena metadatos de los archivos (como nombres, permisos, ubicaci\u00f3n de los bloques).</li> <li>DataNode (Trabajador): Almacena bloques de datos y env\u00eda se\u00f1ales de estado (heartbeat) al NameNode para confirmar que est\u00e1 activo.</li> </ul> <p>HDFS sigue una pol\u00edtica de replicaci\u00f3n para mantener la integridad de los datos, donde cada bloque se replica en varios DataNodes para garantizar la redundancia.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/1hdfs/#como-funciona-hdfs","title":"\ud83d\udee0 C\u00f3mo Funciona HDFS:","text":""},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/1hdfs/#proceso-de-escritura","title":"Proceso de Escritura \u270f\ufe0f","text":"<p>Cuando un cliente escribe un archivo en HDFS:</p> <ol> <li>El archivo se divide en bloques (fragmentos de datos) y se distribuye en m\u00faltiples DataNodes.</li> <li>El NameNode almacena los metadatos sobre qu\u00e9 bloques pertenecen al archivo y en qu\u00e9 DataNodes est\u00e1n almacenados.</li> <li>Cada bloque se replica (t\u00edpicamente tres veces) en diferentes DataNodes para asegurar la tolerancia a fallos.</li> </ol> <pre><code>sequenceDiagram\n    participant Cliente\n    participant NameNode\n    participant DataNode1\n    participant DataNode2\n    participant DataNode3\n\n    Cliente-&gt;&gt;NameNode: Solicitud para escribir archivo\n    NameNode-&gt;&gt;Cliente: Informaci\u00f3n de los bloques\n    Cliente-&gt;&gt;DataNode1: Escribir Bloque 1\n    DataNode1-&gt;&gt;DataNode2: Replicar Bloque 1\n    DataNode2-&gt;&gt;DataNode3: Replicar Bloque 1</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/1hdfs/#proceso-de-lectura","title":"Proceso de Lectura \ud83d\udcd6","text":"<p>Cuando un cliente lee un archivo de HDFS:</p> <ol> <li>El cliente solicita al NameNode la ubicaci\u00f3n de los bloques.</li> <li>El cliente recupera los bloques de datos directamente de los DataNodes.</li> <li>Los datos se ensamblan nuevamente en el archivo original.</li> </ol> <pre><code>sequenceDiagram\n    participant Cliente\n    participant NameNode\n    participant DataNode1\n    participant DataNode2\n    participant DataNode3\n\n    Cliente-&gt;&gt;NameNode: Solicitud para leer archivo\n    NameNode-&gt;&gt;Cliente: Ubicaci\u00f3n de los bloques\n    Cliente-&gt;&gt;DataNode1: Leer Bloque 1\n    Cliente-&gt;&gt;DataNode2: Leer Bloque 2</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/1hdfs/#ejemplo-subida-y-descarga-de-un-archivo","title":"\ud83d\udcc2 Ejemplo: Subida y Descarga de un Archivo","text":""},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/1hdfs/#subida-de-un-archivo-500-mb","title":"Subida de un Archivo (500 MB)","text":"<p>El archivo de 500 MB se divide en bloques de 128 MB. Debido a la configuraci\u00f3n predeterminada de HDFS, los bloques se replican en varios DataNodes para garantizar la tolerancia a fallos. En este caso, el archivo se dividir\u00e1 en 4 bloques (3 bloques de 128 MB y 1 de 116 MB).</p> <pre><code>sequenceDiagram\n    participant Cliente\n    participant NameNode\n    participant DataNode1\n    participant DataNode2\n    participant DataNode3\n\n    Cliente-&gt;&gt;NameNode: Solicitud para subir archivo (500 MB)\n    NameNode-&gt;&gt;Cliente: Informaci\u00f3n de bloques y nodos asignados\n\n    Cliente-&gt;&gt;DataNode1: Escribir Bloque 1 (128 MB)\n    Cliente-&gt;&gt;DataNode2: Escribir Bloque 2 (128 MB)\n    Cliente-&gt;&gt;DataNode3: Escribir Bloque 3 (128 MB)\n    Cliente-&gt;&gt;DataNode1: Escribir Bloque 4 (116 MB)\n\n    DataNode1-&gt;&gt;DataNode2: Replicar Bloque 1\n    DataNode2-&gt;&gt;DataNode3: Replicar Bloque 2\n    DataNode3-&gt;&gt;DataNode1: Replicar Bloque 3</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/1hdfs/#descarga-de-un-archivo-500-mb","title":"Descarga de un Archivo (500 MB)","text":"<p>Para leer el archivo, el NameNode le indica al cliente d\u00f3nde se encuentran almacenados los bloques. El cliente recupera los bloques directamente desde los DataNodes y los reensambla para formar el archivo completo.</p> <pre><code>sequenceDiagram\n    participant Cliente\n    participant NameNode\n    participant DataNode1\n    participant DataNode2\n    participant DataNode3\n\n    Cliente-&gt;&gt;NameNode: Solicitud para descargar archivo\n    NameNode-&gt;&gt;Cliente: Ubicaci\u00f3n de los bloques en los DataNodes\n\n    Cliente-&gt;&gt;DataNode1: Descargar Bloque 1 (128 MB)\n    Cliente-&gt;&gt;DataNode2: Descargar Bloque 2 (128 MB)\n    Cliente-&gt;&gt;DataNode3: Descargar Bloque 3 (128 MB)\n    Cliente-&gt;&gt;DataNode1: Descargar Bloque 4 (116 MB)</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/1hdfs/#distribucion-de-los-bloques","title":"Distribuci\u00f3n de los Bloques","text":"<p>En el diagrama, el proceso muestra c\u00f3mo el archivo de 500 MB se divide en 4 bloques, con los bloques replicados en los diferentes DataNodes para garantizar la disponibilidad y redundancia en caso de fallos.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/1hdfs/#ventajas-de-hdfs","title":"Ventajas de HDFS:","text":"<ul> <li>Escalabilidad: Se puede escalar f\u00e1cilmente a\u00f1adiendo m\u00e1s nodos.</li> <li>Tolerancia a fallos: Se recupera autom\u00e1ticamente de fallos de nodos debido a la replicaci\u00f3n de bloques.</li> <li>Alto rendimiento: Optimizado para procesamiento por lotes, lo que permite un alto rendimiento de datos.</li> </ul> <p>HDFS es una parte clave del ecosistema de Hadoop y es esencial para gestionar eficientemente grandes conjuntos de datos.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/2mapreduce/","title":"\ud83d\udda5\ufe0f MapReduce: Procesamiento Distribuido Eficiente en Grandes Vol\u00famenes de Datos","text":""},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/2mapreduce/#que-es-mapreduce","title":"\ud83d\ude80 \u00bfQu\u00e9 es MapReduce?","text":"<p>MapReduce es un modelo de programaci\u00f3n dise\u00f1ado para procesar grandes vol\u00famenes de datos de forma distribuida en cl\u00fasteres de servidores. Fue desarrollado por Google como parte de su infraestructura para manejar y analizar cantidades masivas de informaci\u00f3n en Internet.</p> <p>El modelo MapReduce se compone principalmente de dos fases:</p> <ul> <li>Map: divide una tarea grande en sub-tareas m\u00e1s peque\u00f1as, asign\u00e1ndolas a nodos en el cl\u00faster. Por ejemplo, si se quiere contar el n\u00famero de palabras en un texto, el nodo Map puede dividir el texto en palabras y emitir un par clave-valor <code>(palabra, 1)</code> para cada una de ellas.</li> <li>Reduce: toma los resultados de las sub-tareas y los combina en un resultado final. En el caso de contar palabras, el nodo Reduce puede sumar todos los valores de una palabra determinada y emitir un par clave-valor <code>(palabra, total)</code>.</li> </ul> <p>Es ampliamente utilizado en sistemas distribuidos como Apache Hadoop, donde permite el procesamiento de petabytes de datos de manera eficiente.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/2mapreduce/#como-funciona-mapreduce","title":"\ud83e\udde9 \u00bfC\u00f3mo Funciona MapReduce?","text":"<ol> <li> <p>Map: Los datos de entrada son divididos en fragmentos y procesados en paralelo por diferentes nodos. Cada nodo aplica una funci\u00f3n de mapeo para generar pares clave-valor intermedios. Por ejemplo, si se quiere contar el n\u00famero de palabras en un texto, el nodo Map puede dividir el texto en palabras y emitir un par clave-valor <code>(palabra, 1)</code> para cada una de ellas.</p> </li> <li> <p>Shuffle &amp; Sort: Los resultados de la fase Map son organizados y agrupados por clave. Este proceso permite que todos los valores relacionados con una clave se env\u00eden al mismo nodo en la fase de Reduce.</p> </li> <li> <p>Reduce: El nodo ejecuta una funci\u00f3n que combina los valores asociados con la misma clave, generando un conjunto de resultados finales. En el caso de contar palabras, el nodo Reduce puede sumar todos los valores de una palabra determinada y emitir un par clave-valor <code>(palabra, total)</code>.</p> </li> </ol>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/2mapreduce/#estructura-de-un-programa-mapreduce","title":"\ud83d\udee0\ufe0f Estructura de un Programa MapReduce","text":"<p>Un programa MapReduce t\u00edpico se compone de dos funciones clave: map y reduce.</p> <pre><code>// Funci\u00f3n MAP\nfunction map(key, value) {\n  // Divide el valor de entrada y emite pares clave-valor\n  const words = value.split(' ');\n  for (const word of words) {\n    emit(word, 1);\n  }\n}\n\n// Funci\u00f3n REDUCE\nfunction reduce(key, values) {\n  // Suma todos los valores para cada clave\n  const sum = values.reduce((a, b) =&gt; a + b, 0);\n  emit(key, sum);\n}\n</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/2mapreduce/#diagrama-de-flujo-de-un-programa-mapreduce","title":"\ud83d\udcca Diagrama de Flujo de un Programa MapReduce","text":"<pre><code>graph TD;\n    A[Datos de Entrada] --&gt;|Fragmentar| B[Fase Map]\n    B --&gt; C[Shuffle &amp; Sort]\n    C --&gt; D[Fase Reduce]\n    D --&gt; E[Resultado Final]\n\n    B --&gt;|Claves y Valores Intermedios| C\n    C --&gt;|Agrupaci\u00f3n por Clave| D</code></pre> <p>Este diagrama ilustra c\u00f3mo los datos de entrada son primero procesados por la fase Map, luego se organizan por clave en Shuffle &amp; Sort, y finalmente se combinan en la fase Reduce para generar el resultado final.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/2mapreduce/#ejemplos-populares-de-mapreduce","title":"\ud83c\udf93 Ejemplos Populares de MapReduce","text":""},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/2mapreduce/#1-word-count-conteo-de-palabras","title":"1. Word Count (Conteo de Palabras)","text":"<p>Este programa MapReduce cuenta cu\u00e1ntas veces aparece cada palabra en un conjunto de documentos.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/2mapreduce/#fase-map","title":"Fase Map","text":"<pre><code>function map(key, value) {\n  const words = value.split(' ');\n  for (const word of words) {\n    emit(word, 1);  // (clave, valor) =&gt; (palabra, 1)\n  }\n}\n</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/2mapreduce/#fase-reduce","title":"Fase Reduce","text":"<pre><code>function reduce(key, values) {\n  const total = values.reduce((a, b) =&gt; a + b, 0);\n  emit(key, total);  // (clave, valor) =&gt; (palabra, total)\n}\n</code></pre> <pre><code>graph TD;\n    Texto[Documento de Entrada] --&gt;|Fragmentar en Palabras| Map[Funci\u00f3n Map]\n    Map --&gt; Intermedio[Claves y Valores Intermedios]\n    Intermedio --&gt; Shuffle[Shuffle &amp; Sort]\n    Shuffle --&gt; Reduce[Funci\u00f3n Reduce]\n    Reduce --&gt; Resultado[Frecuencia de Palabras]</code></pre> <p>Explicaci\u00f3n:</p> <ul> <li>La fase Map emite pares clave-valor <code>(palabra, 1)</code> para cada palabra en el texto.</li> <li>La fase Shuffle agrupa todas las instancias de la misma palabra.</li> <li>La fase Reduce suma todas las ocurrencias de una palabra y emite el resultado final.</li> </ul>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/2mapreduce/#2-sumar-ventas-por-producto","title":"2. Sumar Ventas por Producto","text":"<p>Este ejemplo muestra c\u00f3mo sumar el total de ventas por producto a partir de un conjunto de registros de ventas.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/2mapreduce/#datos-de-entrada","title":"Datos de Entrada","text":"<pre><code>ProductoA, 20\nProductoB, 15\nProductoA, 10\nProductoC, 30\nProductoB, 5\n</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/2mapreduce/#fase-map_1","title":"Fase Map","text":"<p>La funci\u00f3n <code>map</code> toma cada l\u00ednea de ventas y emite un par clave-valor donde la clave es el nombre del producto y el valor es la venta.</p> <pre><code>function map(key, value) {\n  const [product, sales] = value.split(', ');\n  emit(product, parseInt(sales));  // (clave, valor) =&gt; (producto, ventas)\n}\n</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/2mapreduce/#fase-reduce_1","title":"Fase Reduce","text":"<p>La funci\u00f3n <code>reduce</code> toma todas las ventas de un producto en particular y las suma.</p> <pre><code>function reduce(key, values) {\n  const totalSales = values.reduce((a, b) =&gt; a + b, 0);\n  emit(key, totalSales);  // (clave, valor) =&gt; (producto, ventas_totales)\n}\n</code></pre> <pre><code>graph TD;\n    Ventas[Datos de Ventas] --&gt;|Leer Producto y Venta| Map[Funci\u00f3n Map]\n    Map --&gt; Intermedio[Claves: Producto, Valores: Ventas]\n    Intermedio --&gt; Shuffle[Shuffle &amp; Sort]\n    Shuffle --&gt; Reduce[Funci\u00f3n Reduce]\n    Reduce --&gt; Resultado[Ventas Totales por Producto]</code></pre> <p>Explicaci\u00f3n:</p> <ul> <li>La fase Map toma cada l\u00ednea de ventas y emite pares clave-valor donde la clave es el producto y el valor es la venta.</li> <li>La fase Shuffle agrupa todas las ventas de cada producto.</li> <li>La fase Reduce suma las ventas totales para cada producto y emite el resultado final.</li> </ul>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/2mapreduce/#por-que-es-importante-mapreduce","title":"\ud83d\udca1 \u00bfPor qu\u00e9 es Importante MapReduce?","text":"<p>MapReduce es fundamental para el procesamiento de grandes cantidades de datos debido a su capacidad para:</p> <ul> <li>Escalabilidad: Procesar datos en paralelo a trav\u00e9s de m\u00faltiples servidores.</li> <li>Tolerancia a fallos: Maneja autom\u00e1ticamente los errores en los nodos, replicando datos para asegurar su disponibilidad.</li> <li>Simplicidad: Permite a los desarrolladores escribir programas para grandes vol\u00famenes de datos sin preocuparse por la infraestructura subyacente.</li> </ul> <p>\u00a1Con MapReduce, el procesamiento distribuido de grandes datos se vuelve accesible y eficiente! \ud83c\udf89</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/","title":"\u00cdndice de pr\u00e1cticas y tareas","text":"<p>Este \u00edndice incluye todas las pr\u00e1cticas guiadas y tareas correspondientes al curso. Aseg\u00farate de seguir las instrucciones cuidadosamente para cada pr\u00e1ctica y tarea, y consulta los materiales de apoyo cuando sea necesario.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/#practicas-guiadas","title":"Pr\u00e1cticas guiadas","text":"<p>Las pr\u00e1cticas guiadas est\u00e1n dise\u00f1adas para que sigas un conjunto de instrucciones paso a paso y te familiarices con el entorno y las herramientas utilizadas en Big Data. Cada pr\u00e1ctica cubre un aspecto clave del entorno de Hadoop y su ecosistema.</p> <ul> <li> <p>WORDCOUNT en Hadoop con MapReduce</p> <ul> <li>Descripci\u00f3n: En esta pr\u00e1ctica aprender\u00e1s a ejecutar el ejemplo cl\u00e1sico de WordCount utilizando Hadoop MapReduce. El objetivo es procesar un archivo de texto para contar la cantidad de veces que aparece cada palabra.</li> <li>Objetivos:<ul> <li>Subir un archivo de texto a HDFS.</li> <li>Ejecutar un trabajo de MapReduce para contar palabras.</li> <li>Verificar la salida del proceso en HDFS.</li> </ul> </li> </ul> </li> <li> <p>An\u00e1lisis de costos por categor\u00eda con MapReduce en Python</p> <ul> <li>Descripci\u00f3n: En esta pr\u00e1ctica aprender\u00e1s a implementar y ejecutar un ejemplo de MapReduce utilizando Python para analizar datos de compras. El objetivo es procesar un archivo de transacciones, agrupar las ventas por tipo de producto y calcular el total de ventas para cada categor\u00eda.</li> <li>Objetivos:<ul> <li>Descargar el proyecto de ejemplo desde Google Drive.</li> <li>Ejecutar un script de shell para descargar los datos de prueba.</li> <li>Explicar el funcionamiento de los archivos <code>mapper.py</code> y <code>reducer.py</code>.</li> <li>Ejecutar el <code>mapper.py</code> para observar la salida intermedia.</li> <li>Ejecutar <code>mapper.py</code> y <code>reducer.py</code> juntos para obtener el total de ventas por categor\u00eda.</li> </ul> </li> </ul> </li> <li> <p>An\u00e1lisis de Visualizaci\u00f3n en Netflix con MapReduce en Python</p> <ul> <li>Descripci\u00f3n: En esta tarea, desarrollar\u00e1s un programa en Python utilizando el paradigma de MapReduce para analizar los datos de visualizaci\u00f3n en Netflix. El objetivo es identificar los d\u00edas de la semana y las franjas horarias con mayor n\u00famero de visualizaciones.</li> <li>Objetivos:<ul> <li>Implementar una funci\u00f3n <code>map</code> que procese las columnas <code>day_of_week</code> y <code>time_of_day</code>.</li> <li>Implementar una funci\u00f3n <code>reduce</code> que acumule el n\u00famero total de visualizaciones para cada combinaci\u00f3n de d\u00eda y franja horaria.</li> <li>Ejecutar el programa para obtener un resumen del patr\u00f3n de visualizaci\u00f3n.</li> <li>Interpretar los resultados y explicar qu\u00e9 d\u00edas y horarios tienen m\u00e1s visualizaciones.</li> </ul> </li> </ul> </li> </ul>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/category-cost-python-sample/","title":"Category cost python sample","text":""},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/category-cost-python-sample/#practica-de-procesamiento-de-datos-con-mapreduce-en-python","title":"Pr\u00e1ctica de Procesamiento de Datos con MapReduce en Python","text":""},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/category-cost-python-sample/#objetivo-de-la-practica","title":"Objetivo de la Pr\u00e1ctica","text":"<p>En esta pr\u00e1ctica, los alumnos aprender\u00e1n a utilizar un proyecto b\u00e1sico de MapReduce en Python, ejecutar scripts de shell, y entender c\u00f3mo funcionan los archivos <code>mapper.py</code> y <code>reducer.py</code> para realizar un procesamiento de datos sencillo. Al finalizar, los estudiantes ser\u00e1n capaces de analizar datos utilizando comandos de Linux y comprender el flujo de datos en un sistema MapReduce.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/category-cost-python-sample/#1-descargar-el-proyecto","title":"1. Descargar el Proyecto","text":"<ul> <li>Acci\u00f3n: Descarga el proyecto desde el siguiente enlace de Google Drive.</li> <li>Enlace del proyecto: Proyecto en Google Drive</li> <li>Objetivo: Obtener los archivos necesarios para realizar la pr\u00e1ctica, que incluyen el c\u00f3digo del Mapper y Reducer, un archivo <code>download_data.sh</code> y un archivo de datos <code>purchases.txt</code> en el que se basar\u00e1 el procesamiento.</li> </ul>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/category-cost-python-sample/#2-ejecutar-el-script-para-descargar-el-archivo-de-prueba","title":"2. Ejecutar el Script para Descargar el Archivo de Prueba","text":"<ul> <li>Acci\u00f3n: Desde la terminal, navega a la carpeta donde descargaste el proyecto y ejecuta el siguiente comando:   <pre><code>bash download_data.sh\n</code></pre></li> <li>Objetivo: Descargar el archivo <code>purchases.txt</code> que contiene datos de prueba de transacciones de compras. Este archivo se encuentra en la carpeta <code>data/</code>.</li> </ul>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/category-cost-python-sample/#3-explicar-el-funcionamiento-del-archivo-mapperpy","title":"3. Explicar el Funcionamiento del Archivo <code>mapper.py</code>","text":"<ul> <li>Tarea: Abre el archivo <code>mapper.py</code> y analiza su contenido.</li> <li>Instrucciones:</li> <li>Lee el c\u00f3digo para entender c\u00f3mo el Mapper procesa cada l\u00ednea de entrada.</li> <li>Describe el prop\u00f3sito de cada parte del c\u00f3digo.</li> <li>Explica c\u00f3mo el Mapper convierte la l\u00ednea de entrada en un par <code>clave-valor</code>, generalmente extrayendo informaci\u00f3n como el nombre del producto y su precio.</li> </ul>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/category-cost-python-sample/#4-explicar-el-funcionamiento-del-archivo-reducerpy","title":"4. Explicar el Funcionamiento del Archivo <code>reducer.py</code>","text":"<ul> <li>Tarea: Abre el archivo <code>reducer.py</code> y analiza su contenido.</li> <li>Instrucciones:</li> <li>Lee el c\u00f3digo para comprender c\u00f3mo el Reducer agrupa y procesa los datos de salida generados por el Mapper.</li> <li>Describe c\u00f3mo el Reducer suma el total de ventas para cada tipo de producto.</li> <li>Explica el uso de variables y estructuras en el c\u00f3digo.</li> </ul>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/category-cost-python-sample/#5-ejecutar-el-mapper-con-el-archivo-de-datos","title":"5. Ejecutar el Mapper con el Archivo de Datos","text":"<ul> <li>Acci\u00f3n: Ejecuta el siguiente comando desde la terminal para procesar los datos usando el Mapper:   <pre><code>cat data/purchases.txt | python2 mapper.py\n</code></pre></li> <li>Objetivo: Verificar que el Mapper est\u00e1 funcionando correctamente y analizar su salida.</li> <li>Tarea:</li> <li>Observa la salida en la terminal y anota qu\u00e9 representa.</li> <li>Explica por qu\u00e9 cada l\u00ednea de salida del Mapper contiene una clave (tipo de producto) y un valor (costo).</li> </ul>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/category-cost-python-sample/#6-ejecutar-mapper-y-reducer-juntos-y-explicar-la-salida","title":"6. Ejecutar Mapper y Reducer Juntos y Explicar la Salida","text":"<ul> <li>Acci\u00f3n: Ejecuta el siguiente comando para procesar los datos con el Mapper y el Reducer combinados:   <pre><code>cat data/purchases.txt | python2 mapper.py | python2 reducer.py\n</code></pre></li> <li>Objetivo: Procesar los datos completamente y obtener el total de ventas por cada tipo de producto.</li> <li>Tarea:</li> <li>Observa la salida en la terminal, que debe mostrar el total de ventas para cada categor\u00eda de producto.</li> <li>Explica c\u00f3mo el Reducer acumula las ventas y por qu\u00e9 cada l\u00ednea de salida representa un total de ventas por categor\u00eda de producto.</li> </ul>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/category-cost-python-sample/#resultados-esperados","title":"Resultados Esperados","text":"<ul> <li>Comprensi\u00f3n de los Conceptos: Los estudiantes deben ser capaces de explicar c\u00f3mo funciona MapReduce y c\u00f3mo el Mapper y el Reducer procesan los datos en etapas.</li> <li>Aplicaci\u00f3n Pr\u00e1ctica: Ejecutar los comandos para ver c\u00f3mo el flujo de datos pasa de un Mapper a un Reducer, simulando un sistema distribuido simple de procesamiento de datos.</li> </ul>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/category-cost-python-sample/#preguntas","title":"Preguntas","text":"<ol> <li>\u00bfQu\u00e9 sucede si una l\u00ednea en <code>purchases.txt</code> tiene un formato incorrecto? \u00bfC\u00f3mo lo maneja el Mapper?</li> <li>\u00bfPor qu\u00e9 es importante verificar el formato de los datos en el Mapper?</li> <li>\u00bfQu\u00e9 modificaciones podr\u00edas hacer al Reducer para agregar m\u00e1s funcionalidad, como calcular el promedio de ventas por categor\u00eda?</li> </ol>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/hashtags-tweets/","title":"Hashtags tweets","text":""},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/hashtags-tweets/#enunciado-para-ejercicio-de-mapreduce","title":"Enunciado para Ejercicio de MapReduce","text":"<p>En este ejercicio, los estudiantes deber\u00e1n implementar una serie de operaciones de MapReduce sobre un archivo CSV de tweets, analizando y extrayendo informaci\u00f3n de inter\u00e9s a partir de grandes vol\u00famenes de datos.</p> <p>Cada estudiante deber\u00e1 responder a las siguientes preguntas de an\u00e1lisis, ejecutando tareas de MapReduce para cada una.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/hashtags-tweets/#formato-del-archivo-csv","title":"Formato del archivo CSV","text":"<p>El archivo <code>tweets_data.csv</code> tiene el siguiente formato de columnas y datos de ejemplo:</p> <pre><code>username;tweet_content;date_written;likes;retweets;replies;location;source\n@carlos_fernandez;La ciencia de datos est\u00e1 cambiando el mundo. #CienciaDeDatos;2024-10-18 16:23:16;182;49;90;Barcelona;Twitter para iPhone\n@carlos_fernandez;Aprendiendo m\u00e1s sobre Python cada d\u00eda.;2024-10-17 16:23:16;382;238;24;M\u00e1laga;Twitter para iPhone\n@carlos_fernandez;Aprendiendo m\u00e1s sobre Python cada d\u00eda.;2024-10-19 16:23:16;285;91;0;Sevilla;Twitter para iPhone\n@maria_garcia;La ciencia de datos est\u00e1 cambiando el mundo. #Python;2024-10-22 16:23:16;76;92;29;M\u00e1laga;Twitter Web App\n@carlos_fernandez;Explorando nuevos modelos de IA. #IA;2024-10-12 16:23:16;56;54;89;Bilbao;Twitter para iPhone\n@maria_garcia;La ciencia de datos est\u00e1 cambiando el mundo. #IA;2024-10-26 16:23:16;36;220;60;Granada;Twitter para iPhone\n@lucia_martinez;Los desaf\u00edos de programaci\u00f3n me mantienen afilado.;2024-10-20 16:23:16;303;155;78;Sevilla;Twitter para Android\n@laura_rodriguez;La ciencia de datos est\u00e1 cambiando el mundo.;2024-10-02 16:23:16;387;242;60;Barcelona;Twitter para iPhone\n@laura_rodriguez;Los desaf\u00edos de programaci\u00f3n me mantienen afilado.;2024-10-19 16:23:16;191;231;35;Bilbao;Twitter para iPhone\n@laura_rodriguez;\u00a1Acabo de terminar una gran sesi\u00f3n de codificaci\u00f3n!;2024-10-25 16:23:16;44;48;20;Granada;Twitter Web App\n</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/hashtags-tweets/#tareas-de-mapreduce","title":"Tareas de MapReduce","text":"<ol> <li>Contar cu\u00e1ntas veces aparece cada hashtag</li> <li> <p>Determinar la frecuencia de cada hashtag en el conjunto de tweets para identificar cu\u00e1les son los temas m\u00e1s populares.</p> </li> <li> <p>Contar cu\u00e1ntos tweets ha escrito cada usuario</p> </li> <li> <p>Obtener el n\u00famero de tweets por cada usuario registrado en el archivo, analizando su nivel de actividad en la plataforma.</p> </li> <li> <p>An\u00e1lisis de popularidad de tweets (likes + retweets)</p> </li> <li> <p>Calcular la popularidad de cada tweet considerando la suma de \"likes\" y \"retweets\".</p> </li> <li> <p>Tweets por ubicaci\u00f3n geogr\u00e1fica</p> </li> <li> <p>Determinar el n\u00famero de tweets enviados desde cada ubicaci\u00f3n para observar las \u00e1reas de mayor actividad.</p> </li> <li> <p>Promedio de interacciones (likes, retweets y replies) por usuario</p> </li> <li> <p>Calcular el promedio de interacciones (considerando \"likes\", \"retweets\" y \"replies\") que recibe cada usuario en sus tweets.</p> </li> <li> <p>Tweets diarios</p> </li> <li>Contabilizar la cantidad de tweets publicados por d\u00eda, analizando la actividad en diferentes fechas.</li> </ol> <p>Cada pregunta representa una tarea de MapReduce. Para cada tarea, los estudiantes deben: 1. Implementar el c\u00f3digo MapReduce. 2. Documentar el proceso. 3. Analizar los resultados obtenidos para identificar patrones o insights.</p> <p>Este ejercicio permitir\u00e1 a los estudiantes comprender y practicar las t\u00e9cnicas de MapReduce aplicadas a un dataset real de redes sociales.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/","title":"Wordcount sample","text":"<p>Aqu\u00ed tienes la pr\u00e1ctica actualizada, incluyendo el comando para visualizar todas las partes del archivo de salida juntas:</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/#practica-ejecucion-del-ejemplo-wordcount-en-hadoop-con-mapreduce","title":"Pr\u00e1ctica: Ejecuci\u00f3n del ejemplo WordCount en Hadoop con MapReduce","text":"<p>En esta pr\u00e1ctica, vas a ejecutar el ejemplo cl\u00e1sico de WordCount utilizando Hadoop MapReduce para contar el n\u00famero de veces que aparece cada palabra en un archivo de texto. Seguir\u00e1s los siguientes pasos para crear un archivo, cargarlo en HDFS, y ejecutar un trabajo de MapReduce que contar\u00e1 las palabras.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/#objetivo","title":"Objetivo:","text":"<p>Familiarizarse con los comandos b\u00e1sicos de HDFS y MapReduce en Hadoop, ejecutar un trabajo de ejemplo y obtener los resultados.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/#requisitos","title":"Requisitos:","text":"<ul> <li>Un cl\u00faster de Hadoop corriendo (local o en la nube, como EMR de AWS).</li> <li>Acceso a la terminal de la m\u00e1quina maestra del cl\u00faster.</li> </ul>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/#implementacion-del-ejemplo","title":"Implementaci\u00f3n del ejemplo:","text":"<p>El c\u00f3digo Java que da lugar al ejemplo que se utiliza en esta pr\u00e1ctica est\u00e1 disponible en WordCount.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/#pasos-a-seguir","title":"Pasos a seguir:","text":""},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/#1-crear-el-archivo-de-texto-de-entrada","title":"1. Crear el archivo de texto de entrada","text":"<p>Primero, crea un archivo de texto en la m\u00e1quina local que contenga el texto que quieres procesar:</p> <pre><code>echo -e \"Hadoop is a distributed system\\nHadoop is scalable\\nHadoop can handle big data\" &gt; word_count.txt\n</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/#2-verificar-el-contenido-del-archivo","title":"2. Verificar el contenido del archivo","text":"<p>Aseg\u00farate de que el archivo contiene el texto deseado ejecutando:</p> <pre><code>cat word_count.txt\n</code></pre> <p>La salida deber\u00eda mostrar el siguiente contenido:</p> <pre><code>Hadoop is a distributed system\nHadoop is scalable\nHadoop can handle big data\n</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/#3-crear-un-directorio-en-hdfs-para-almacenar-el-archivo-de-entrada","title":"3. Crear un directorio en HDFS para almacenar el archivo de entrada","text":"<p>Crea un directorio en HDFS donde almacenar\u00e1s el archivo de entrada:</p> <pre><code>hdfs dfs -mkdir /user/hadoop/input\n</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/#4-subir-el-archivo-de-entrada-a-hdfs","title":"4. Subir el archivo de entrada a HDFS","text":"<p>Sube el archivo <code>word_count.txt</code> a HDFS para que est\u00e9 disponible para el procesamiento:</p> <pre><code>hdfs dfs -put word_count.txt /user/hadoop/input/\n</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/#5-ejecutar-el-trabajo-de-mapreduce-para-wordcount","title":"5. Ejecutar el trabajo de MapReduce para WordCount","text":"<p>Ejecuta el trabajo de MapReduce utilizando el ejemplo de WordCount. Este comando toma el archivo de entrada desde HDFS, lo procesa, y genera un archivo de salida con el conteo de palabras.</p> <pre><code>hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount /user/hadoop/input/word_count.txt /user/hadoop/output\n</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/#6-verificar-la-salida-en-hdfs","title":"6. Verificar la salida en HDFS","text":"<p>El trabajo de MapReduce puede generar varios archivos de salida (<code>part-r-00000</code>, <code>part-r-00001</code>, etc.). Para visualizar todas las partes juntas y concatenarlas en un archivo local, ejecuta el siguiente comando:</p> <pre><code>hdfs dfs -cat /user/hadoop/output/part-r-* &gt; combined_output.txt\n</code></pre> <p>Este comando toma todas las partes del archivo de salida (por ejemplo, <code>part-r-00000</code>, <code>part-r-00001</code>, etc.) y las guarda en un archivo llamado <code>combined_output.txt</code> en el sistema local.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/#7-verificar-el-archivo-de-salida-combinado","title":"7. Verificar el archivo de salida combinado","text":"<p>Una vez concatenado, puedes verificar el contenido del archivo <code>combined_output.txt</code>:</p> <pre><code>cat combined_output.txt\n</code></pre> <p>Deber\u00edas ver la salida con el conteo de palabras, por ejemplo:</p> <pre><code>Hadoop    3\nis        2\na         1\ndistributed 1\nsystem    1\nscalable  1\ncan       1\nhandle    1\nbig       1\ndata      1\n</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/#8-copiar-el-archivo-de-salida-a-la-maquina-local-opcional","title":"8. Copiar el archivo de salida a la m\u00e1quina local (opcional)","text":"<p>Si prefieres obtener las partes del archivo de salida por separado y copiarlas desde HDFS a tu sistema local:</p> <pre><code>hdfs dfs -get /user/hadoop/output/part-r-00000 .\n</code></pre> <p>Repite el proceso para todas las partes generadas (por ejemplo, <code>part-r-00001</code>, <code>part-r-00002</code>, etc.), o concat\u00e9nalas usando el paso anterior.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/#conclusion","title":"Conclusi\u00f3n:","text":"<p>En esta pr\u00e1ctica has aprendido a:</p> <ul> <li>Crear un archivo de texto en la m\u00e1quina local.</li> <li>Subir un archivo a HDFS.</li> <li>Ejecutar un trabajo de MapReduce en Hadoop.</li> <li>Concatenar los resultados de salida generados en m\u00faltiples partes utilizando <code>hdfs dfs -cat</code> para visualizar todos los resultados juntos.</li> </ul> <p>Con estos pasos, te has familiarizado con el flujo b\u00e1sico de trabajo de MapReduce en Hadoop y la gesti\u00f3n de archivos en HDFS.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/#preguntas-para-los-estudiantes","title":"Preguntas para los estudiantes:","text":"<p>Despu\u00e9s de realizar la pr\u00e1ctica, responde a las siguientes preguntas para evaluar tu comprensi\u00f3n del proceso:</p> <ol> <li> <p>\u00bfCu\u00e1ntas partes resultaron del proceso de Reduce?</p> <ul> <li>Pista: Revisa cu\u00e1ntos archivos <code>part-r-0000x</code> se generaron en el directorio de salida de HDFS.</li> </ul> </li> <li> <p>\u00bfQu\u00e9 comando utilizaste para visualizar todas las partes juntas en un solo archivo?</p> <ul> <li>Describe brevemente el uso del comando <code>hdfs dfs -cat</code> y c\u00f3mo se concatenaron las partes de salida.</li> </ul> </li> <li> <p>\u00bfCu\u00e1l es la funci\u00f3n principal de la fase \"Map\" en el proceso de MapReduce?</p> <ul> <li>Explica brevemente qu\u00e9 hace la fase Map y c\u00f3mo genera los pares clave-valor.</li> </ul> </li> <li> <p>\u00bfCu\u00e1l es la funci\u00f3n principal de la fase \"Reduce\" en el proceso de MapReduce?</p> <ul> <li>Describe el objetivo de la fase Reduce y qu\u00e9 sucede con los pares clave-valor generados por el mapper.</li> </ul> </li> <li> <p>\u00bfPor qu\u00e9 el trabajo de MapReduce genera m\u00faltiples archivos de salida (part-r-00000, part-r-00001, etc.)?</p> <ul> <li>Explica por qu\u00e9 los resultados pueden dividirse en varias partes en lugar de generarse en un solo archivo.</li> </ul> </li> <li> <p>\u00bfQu\u00e9 informaci\u00f3n proporciona el archivo <code>_SUCCESS</code> que aparece en el directorio de salida?</p> <ul> <li>Investiga qu\u00e9 indica la presencia de este archivo despu\u00e9s de la ejecuci\u00f3n de un trabajo de MapReduce.</li> </ul> </li> <li> <p>\u00bfQu\u00e9 pasos adicionales ser\u00edan necesarios si quisieras procesar un archivo mucho m\u00e1s grande en lugar de <code>word_count.txt</code>?</p> <ul> <li>Explica si tendr\u00edas que hacer algo diferente en t\u00e9rminos de configuraci\u00f3n de MapReduce o HDFS para procesar un archivo de mayor tama\u00f1o.</li> </ul> </li> </ol>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/netflix/enunciado/","title":"Ejercicio de MapReduce en Python: An\u00e1lisis de D\u00edas y Horarios de Visualizaci\u00f3n en Netflix","text":"<p>Objetivo: El objetivo de este ejercicio es desarrollar un programa en Python que use el paradigma MapReduce para analizar los datos de visualizaci\u00f3n de contenido en Netflix. Queremos identificar los d\u00edas de la semana y las franjas horarias en las que los usuarios realizan m\u00e1s visualizaciones. El programa debe procesar los datos de visualizaci\u00f3n y devolver una tabla con el n\u00famero de visualizaciones agrupadas por d\u00eda de la semana y franja horaria.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/netflix/enunciado/#estructura-de-los-datos","title":"Estructura de los Datos","text":"<p>Para este ejercicio, trabajaremos con un archivo de datos CSV que contiene las siguientes columnas relevantes para el an\u00e1lisis:</p> <ul> <li>day_of_week: D\u00eda de la semana en el que se realiz\u00f3 la visualizaci\u00f3n (por ejemplo, \"Monday\", \"Tuesday\", etc.).</li> <li>time_of_day: Franja horaria en la que se realiz\u00f3 la visualizaci\u00f3n (\"Morning\", \"Afternoon\", \"Evening\", \"Night\").</li> </ul> <p>Un ejemplo de fila de datos podr\u00eda verse as\u00ed: <pre><code>day_of_week,time_of_day,other_columns...\nMonday,Evening,...\nTuesday,Morning,...\n...\n</code></pre></p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/netflix/enunciado/#pasos-para-resolver-el-ejercicio","title":"Pasos para Resolver el Ejercicio","text":"<ol> <li> <p>Funci\u00f3n Map:    La funci\u00f3n <code>map</code> debe leer cada l\u00ednea del archivo de datos y extraer las columnas <code>day_of_week</code> y <code>time_of_day</code>. Por cada entrada, la funci\u00f3n emitir\u00e1 una clave-valor en el formato <code>(day_of_week, time_of_day) -&gt; 1</code>, donde <code>1</code> representa una visualizaci\u00f3n.</p> </li> <li> <p>Funci\u00f3n Reduce:    La funci\u00f3n <code>reduce</code> debe tomar todas las claves <code>(day_of_week, time_of_day)</code> emitidas por la fase de <code>map</code> y sumar los valores asociados para obtener el total de visualizaciones para cada combinaci\u00f3n de d\u00eda y franja horaria.</p> </li> <li> <p>Formato de Salida:    El programa debe generar un informe que muestre las combinaciones de <code>day_of_week</code> y <code>time_of_day</code>, junto con la cantidad total de visualizaciones. El formato de salida puede ser un archivo CSV o una tabla impresa en pantalla con las columnas <code>day_of_week</code>, <code>time_of_day</code> y <code>count</code>.</p> </li> </ol>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/netflix/enunciado/#ejemplo-de-salida-esperada","title":"Ejemplo de Salida Esperada","text":"<pre><code>day_of_week, time_of_day, count\nMonday, Evening, 450\nMonday, Morning, 300\nTuesday, Afternoon, 350\n...\n</code></pre>"}]}