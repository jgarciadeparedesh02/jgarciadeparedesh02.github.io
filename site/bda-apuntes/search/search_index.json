{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83c\udf89 \u00a1Bienvenidos a Big Data Aplicado! \ud83c\udf1f","text":"<p>\ud83d\ude80 Especializaci\u00f3n en Inteligencia Artificial y Big Data</p> <p>\u00a1Hola a todos! \ud83d\udc4b Me complace darles la bienvenida al m\u00f3dulo de Big Data Aplicado dentro del curso de especializaci\u00f3n en Inteligencia Artificial y Big Data. En este m\u00f3dulo, nos adentraremos en el fascinante mundo del Big Data y aprenderemos a usar herramientas clave como Apache Hadoop para manejar y analizar grandes vol\u00famenes de datos. \ud83c\udf10\ud83d\udcca</p> <p></p>"},{"location":"#que-aprenderemos","title":"\ud83c\udfaf \u00bfQu\u00e9 aprenderemos?","text":"<p>En este curso, exploraremos c\u00f3mo almacenar, procesar y analizar grandes cantidades de informaci\u00f3n utilizando Hadoop y su extenso ecosistema. Estas tecnolog\u00edas est\u00e1n revolucionando industrias, permitiendo que las empresas aprovechen al m\u00e1ximo sus datos. \u00a1Prep\u00e1rate para convertirte en un experto en Big Data! \ud83d\ude80</p> <p>\ud83d\udcda \u00cdndice de Contenidos A lo largo de este m\u00f3dulo, profundizaremos en varias \u00e1reas clave del Big Data, desde conceptos introductorios hasta aplicaciones pr\u00e1cticas. Aqu\u00ed te dejamos una visi\u00f3n general de los contenidos:</p>"},{"location":"#unidad-1-introduccion-a-apache-hadoop","title":"Unidad 1: Introducci\u00f3n a Apache Hadoop","text":"<p>Comenzamos por lo b\u00e1sico: \u00bfQu\u00e9 es Hadoop y por qu\u00e9 es tan importante para el procesamiento de grandes datos?</p> <ul> <li>Motivaci\u00f3n y origen: Exploramos los problemas que dieron origen a Hadoop y c\u00f3mo se convirti\u00f3 en una soluci\u00f3n para el manejo de grandes vol\u00famenes de datos.</li> <li>Apache Hadoop a alto nivel: Nos familiarizamos con los conceptos clave de Hadoop y sus componentes principales.</li> <li>\u00bfQu\u00e9 es Apache Hadoop? Conoce qu\u00e9 es y por qu\u00e9 es crucial para el almacenamiento y procesamiento de grandes cantidades de datos.</li> <li>Ecosistema Hadoop y distribuciones: Descubre c\u00f3mo Hadoop es solo el n\u00facleo de un ecosistema m\u00e1s amplio, que incluye herramientas como Hive, Pig, y HBase.</li> <li>Arquitectura de Hadoop: Examinamos c\u00f3mo Hadoop organiza el procesamiento distribuido a trav\u00e9s de HDFS (Hadoop Distributed File System) y MapReduce.</li> <li>Beneficios, desventajas y dificultades: Analizamos las ventajas que ofrece Hadoop, pero tambi\u00e9n sus limitaciones y retos.</li> </ul>"},{"location":"#unidad-2-almacenamiento-y-procesamiento-en-hadoop","title":"Unidad 2: Almacenamiento y Procesamiento en Hadoop","text":"<p>Profundizamos en c\u00f3mo Hadoop almacena y procesa datos utilizando HDFS, YARN y MapReduce.</p> <ul> <li>HDFS (Hadoop Distributed File System): Es el sistema de archivos distribuido que permite almacenar grandes vol\u00famenes de datos de manera eficiente. Explicaremos c\u00f3mo funciona la lectura y escritura de datos en HDFS.</li> <li>Arquitectura de HDFS: Conoceremos la estructura interna de HDFS y c\u00f3mo los NameNodes y DataNodes se comunican para mantener la integridad de los datos.</li> <li>YARN (Yet Another Resource Negotiator): Esta herramienta gestiona los recursos dentro del cl\u00faster de Hadoop, permitiendo ejecutar m\u00faltiples tareas en paralelo.</li> <li>MapReduce: Modelo de programaci\u00f3n distribuida que divide las tareas en \"Map\" y \"Reduce\". Veremos c\u00f3mo se utiliza para procesar grandes cantidades de datos de manera eficiente y escalable.</li> </ul>"},{"location":"#unidad-3-ecosistema-hadoop","title":"Unidad 3: Ecosistema Hadoop","text":"<p>El ecosistema Hadoop es vasto y est\u00e1 lleno de herramientas dise\u00f1adas para facilitar el procesamiento y an\u00e1lisis de datos.</p> <ul> <li>Apache Pig: Un lenguaje de scripting dise\u00f1ado para analizar grandes conjuntos de datos de manera simple y eficiente.</li> <li>Apache Hive: Un sistema de data warehousing que permite realizar consultas sobre datos almacenados en Hadoop mediante un lenguaje similar a SQL, llamado HQL.</li> <li>Apache Impala: Herramienta de an\u00e1lisis que permite realizar consultas en tiempo real sobre grandes vol\u00famenes de datos.</li> <li>Apache HBase: Una base de datos NoSQL que permite el almacenamiento y acceso a datos estructurados de manera distribuida.</li> <li>Apache Spark: Un motor de procesamiento r\u00e1pido y general para grandes vol\u00famenes de datos, ideal para aplicaciones en tiempo real.</li> <li>Componentes de Ingesta de Datos: Exploraremos herramientas como Apache Sqoop y Apache Flume, dise\u00f1adas para la ingesta eficiente de datos desde fuentes externas a Hadoop.</li> <li>Apache Oozie: Una herramienta de flujo de trabajo que permite coordinar y programar trabajos dentro del ecosistema Hadoop.</li> <li>Procesamiento en Streaming: Veremos c\u00f3mo tecnolog\u00edas como Apache Spark (Structured Streaming), Apache Flink, y Apache Storm manejan datos en tiempo real, permitiendo an\u00e1lisis continuos.</li> </ul>"},{"location":"#unidad-4-administracion-y-monitorizacion-de-sistemas-hadoop","title":"Unidad 4: Administraci\u00f3n y Monitorizaci\u00f3n de Sistemas Hadoop","text":"<p>Gestionar un cl\u00faster Hadoop es crucial para asegurar que todo funcione correctamente, y en esta unidad aprenderemos las mejores pr\u00e1cticas y herramientas.</p> <ul> <li>Interfaz de HDFS y YARN: Exploraremos las interfaces de usuario que permiten monitorizar el funcionamiento del sistema de archivos y la gesti\u00f3n de recursos.</li> <li>Apache Ambari y Cloudera Manager: Dos herramientas poderosas que facilitan la administraci\u00f3n de cl\u00fasteres Hadoop.</li> <li>Ganglia: Un sistema de monitorizaci\u00f3n dise\u00f1ado para analizar el rendimiento y la eficiencia de un cl\u00faster Hadoop en tiempo real.</li> </ul>"},{"location":"#unidad-5-aplicacion-practica-de-tecnologias-big-data","title":"Unidad 5: Aplicaci\u00f3n Pr\u00e1ctica de Tecnolog\u00edas Big Data","text":"<p>Finalmente, aplicaremos los conocimientos adquiridos para resolver problemas reales utilizando tecnolog\u00edas Big Data.</p> <ul> <li>Arquitecturas y Modelos de Despliegue: Exploraremos diferentes arquitecturas de sistemas Big Data y sus modelos de despliegue, incluyendo soluciones on-premise y en la nube.</li> <li>Hadoop en la Pr\u00e1ctica: Veremos ejemplos de implementaci\u00f3n pr\u00e1ctica de Hadoop en diversos sectores y casos de uso.</li> <li>Soluciones Hadoop-as-a-Service: Exploraremos plataformas como Amazon EMR y Microsoft Azure HDInsight, que permiten implementar y gestionar soluciones Hadoop en la nube.</li> </ul>"},{"location":"#a-por-ello","title":"\u00a1A por ello!","text":"<p>Este curso te proporcionar\u00e1 las habilidades y conocimientos necesarios para dise\u00f1ar, implementar y administrar sistemas de Big Data. Al finalizar, ser\u00e1s capaz de gestionar grandes vol\u00famenes de datos de manera eficiente, desde el almacenamiento hasta el procesamiento avanzado. \ud83c\udfc6</p> <p>\ud83d\udca1 Consejo del d\u00eda: El mundo del Big Data est\u00e1 lleno de oportunidades. No dudes en preguntar, explorar y experimentar a lo largo del curso. \u00a1Estamos aqu\u00ed para ayudarte a cada paso del camino!</p> <p>\ud83d\udd25 \u00a1Vamos a dominar el Big Data, un cl\u00faster a la vez! \ud83d\udcaa Espero que est\u00e9s tan emocionado como yo para comenzar este viaje en el mundo del Big Data. \u00a1Es hora de hacer que los datos trabajen para nosotros! \ud83c\udf0d\ud83c\udf93</p>"},{"location":"docker/","title":"Introducci\u00f3n a Docker y Docker-compose \ud83d\udea2","text":""},{"location":"docker/#que-es-docker","title":"\ud83d\ude80 \u00bfQu\u00e9 es Docker?","text":"<p>Docker es una plataforma dise\u00f1ada para desarrollar, desplegar y ejecutar aplicaciones en contenedores. Un contenedor es un paquete liviano, port\u00e1til y aut\u00f3nomo que incluye todo lo necesario para ejecutar una aplicaci\u00f3n: c\u00f3digo, runtime, bibliotecas y configuraciones. La principal ventaja de Docker es que permite garantizar que la aplicaci\u00f3n se ejecutar\u00e1 de la misma manera en cualquier entorno, ya sea tu m\u00e1quina local o un servidor en la nube.</p> <pre><code># Ejemplo de comando para ejecutar un contenedor Docker:\ndocker run -d -p 8080:80 --name mi_contenedor nginx\n</code></pre> <p>En el ejemplo anterior:</p> <ul> <li><code>-d</code> indica que el contenedor se ejecutar\u00e1 en segundo plano (modo detached).</li> <li><code>-p 8080:80</code> enlaza el puerto 8080 de tu m\u00e1quina local al puerto 80 del contenedor.</li> <li><code>--name mi_contenedor</code> le asigna un nombre al contenedor.</li> <li><code>nginx</code> es la imagen que estamos usando para crear el contenedor.</li> </ul>"},{"location":"docker/#componentes-clave-de-docker","title":"\ud83d\udee0 Componentes clave de Docker","text":"<ol> <li>Imagen: Es una plantilla de solo lectura que se utiliza para crear contenedores. Piensa en una imagen como una instant\u00e1nea de un sistema operativo m\u00e1s una aplicaci\u00f3n.</li> </ol> <p>Ejemplo: La imagen <code>nginx</code> es una plantilla que contiene el servidor web NGINX.</p> <ol> <li> <p>Contenedor: Es una instancia ejecutable de una imagen. Cada contenedor es aut\u00f3nomo y aislado.</p> </li> <li> <p>Dockerfile: Es un archivo de texto que contiene todas las instrucciones necesarias para construir una imagen. </p> </li> </ol> <p><pre><code># Ejemplo de un Dockerfile simple:\nFROM node:14\nWORKDIR /app\nCOPY . .\nRUN npm install\nCMD [\"npm\", \"start\"]\n</code></pre> El Dockerfile anterior:</p> <ul> <li>Usa la imagen base <code>node:14</code>.</li> <li>Copia el contenido del directorio actual en el contenedor.</li> <li>Instala las dependencias de Node.js.</li> <li> <p>Ejecuta el comando <code>npm start</code> para iniciar la aplicaci\u00f3n.</p> </li> <li> <p>Docker Hub: Es un registro p\u00fablico donde puedes encontrar im\u00e1genes creadas por otros usuarios o subir las tuyas propias.</p> </li> </ul>"},{"location":"docker/#que-es-docker-compose","title":"\ud83d\udce6 \u00bfQu\u00e9 es Docker-compose?","text":"<p>Docker-compose es una herramienta que te permite definir y ejecutar aplicaciones multi-contenedor. En lugar de ejecutar m\u00faltiples contenedores de forma manual, puedes utilizar un archivo <code>docker-compose.yml</code> para definir c\u00f3mo se deben configurar y ejecutar.</p> <pre><code># Ejemplo b\u00e1sico de un archivo docker-compose.yml:\nversion: '3'\nservices:\n  web:\n    image: nginx\n    ports:\n      - \"8080:80\"\n  db:\n    image: mysql\n    environment:\n      MYSQL_ROOT_PASSWORD: example\n</code></pre> <p>Este archivo YAML define dos servicios:</p> <ul> <li>web: que usa la imagen <code>nginx</code> y mapea el puerto 80 del contenedor al 8080 de la m\u00e1quina local.</li> <li>db: que usa la imagen <code>mysql</code> y define una contrase\u00f1a para el usuario root.</li> </ul> <p>Para iniciar los contenedores definidos en el archivo <code>docker-compose.yml</code>, simplemente ejecutas:</p> <pre><code>docker-compose up\n</code></pre>"},{"location":"docker/#ventajas-de-docker-y-docker-compose","title":"\ud83c\udf00 Ventajas de Docker y Docker-compose","text":"<ol> <li>Portabilidad: Los contenedores se ejecutan de la misma manera en cualquier entorno.</li> <li>Aislamiento: Cada contenedor es independiente, evitando conflictos entre aplicaciones.</li> <li>Escalabilidad: Docker-compose facilita la creaci\u00f3n de aplicaciones complejas al permitir la orquestaci\u00f3n de m\u00faltiples contenedores.</li> <li>Reproducibilidad: Con un <code>Dockerfile</code> y <code>docker-compose</code>, puedes recrear un entorno exacto f\u00e1cilmente.</li> </ol>"},{"location":"docker/#diferencia-entre-imagenes-y-contenedores-en-docker","title":"\ud83d\uddbc\ufe0f Diferencia entre Im\u00e1genes y Contenedores en Docker \ud83d\udea2","text":"<p>Una de las confusiones m\u00e1s comunes al empezar con Docker es la diferencia entre im\u00e1genes y contenedores. Vamos a aclarar esto:</p>"},{"location":"docker/#imagenes","title":"Im\u00e1genes \ud83d\udcf8","text":"<p>Una imagen en Docker es como una plantilla de solo lectura que contiene todo lo necesario para ejecutar una aplicaci\u00f3n, incluyendo:</p> <ul> <li>Sistema operativo (por ejemplo, Ubuntu, Alpine).</li> <li>C\u00f3digo de la aplicaci\u00f3n.</li> <li>Dependencias o librer\u00edas necesarias para que la aplicaci\u00f3n funcione.</li> <li>Configuraciones o variables de entorno.</li> </ul> <p>Las im\u00e1genes son est\u00e1ticas, lo que significa que no cambian una vez que se crean. Son utilizadas para generar contenedores, pero por s\u00ed solas no hacen nada. </p> <p>Piensa en una imagen como una receta o blueprint que no se ejecuta, pero que contiene todas las instrucciones necesarias para crear algo que s\u00ed lo har\u00e1.</p> <pre><code># Ejemplo: Listar las im\u00e1genes disponibles en tu sistema\ndocker images\n</code></pre> <p>Salida t\u00edpica:</p> <pre><code>REPOSITORY       TAG       IMAGE ID       CREATED        SIZE\nnginx            latest    d1a364dc548d   2 weeks ago    133MB\nmysql            5.7       2a3174d2e2f7   1 month ago    450MB\n</code></pre> <p>En el ejemplo anterior:</p> <ul> <li>La imagen <code>nginx</code> est\u00e1 lista para ser utilizada para crear un servidor web.</li> <li>La imagen <code>mysql</code> contiene la base de datos MySQL en su versi\u00f3n 5.7.</li> </ul>"},{"location":"docker/#contenedores","title":"Contenedores \ud83d\udef3\ufe0f","text":"<p>Un contenedor es una instancia en ejecuci\u00f3n de una imagen. Es como si tomaras la plantilla (imagen) y la ejecutaras para crear un entorno real donde tu aplicaci\u00f3n corre. Un contenedor:</p> <ul> <li>Est\u00e1 basado en una imagen.</li> <li>Es din\u00e1mico: puede ejecutar procesos, guardar datos, recibir tr\u00e1fico de red, etc.</li> <li>Puede ser creado, iniciado, detenido y destruido.</li> </ul> <p>Cada contenedor tiene su propio sistema de archivos y entorno de ejecuci\u00f3n aislado del resto. Esto significa que puedes tener m\u00faltiples contenedores ejecutando la misma imagen sin que interfieran entre s\u00ed.</p> <pre><code># Ejemplo: Listar los contenedores en ejecuci\u00f3n\ndocker ps\n</code></pre> <p>Salida t\u00edpica:</p> <pre><code>CONTAINER ID   IMAGE     COMMAND                  CREATED         STATUS         PORTS                  NAMES\na1b2c3d4e5f6   nginx     \"nginx -g 'daemon off\u2026   5 minutes ago   Up 5 minutes   0.0.0.0:8080-&gt;80/tcp   webserver\n</code></pre> <p>En este caso, el contenedor <code>webserver</code> fue creado a partir de la imagen <code>nginx</code> y est\u00e1 en ejecuci\u00f3n mapeando el puerto 80 al 8080 de la m\u00e1quina local.</p>"},{"location":"docker/#diferencia-esencial","title":"\ud83d\udcca Diferencia Esencial","text":"Im\u00e1genes Contenedores Son plantillas est\u00e1ticas. Son instancias en ejecuci\u00f3n basadas en una imagen. No pueden modificarse una vez creadas. Pueden cambiar mientras est\u00e1n en ejecuci\u00f3n (p.ej. archivos creados dentro del contenedor). Se almacenan localmente o en repositorios como Docker Hub. Pueden ser creados, iniciados, detenidos y destruidos. Son inmutables (no cambian). Son din\u00e1micos y pueden ejecutar procesos."},{"location":"docker/#metafora-restaurante-y-recetas","title":"\ud83e\uddd1\u200d\ud83c\udf73 Met\u00e1fora: Restaurante y Recetas","text":"<ul> <li>Imagen: Es como una receta. Tienes todos los ingredientes y pasos necesarios para preparar un plato, pero no puedes comer la receta.</li> <li>Contenedor: Es el plato servido en la mesa. Ya se ha preparado a partir de la receta (imagen) y est\u00e1 listo para que lo disfrutes (o en nuestro caso, para que la aplicaci\u00f3n se ejecute).</li> </ul> <p>\ud83d\udca1 Conclusi\u00f3n:</p> <ul> <li>Imagen = Plantilla (Receta).</li> <li>Contenedor = Ejecuci\u00f3n de la Imagen (Plato Listo).</li> </ul> <p>Las im\u00e1genes son el punto de partida, pero son los contenedores los que realmente ejecutan y manejan tu aplicaci\u00f3n. Cada vez que usas Docker para ejecutar algo, est\u00e1s creando uno o m\u00e1s contenedores basados en im\u00e1genes preexistentes.</p>"},{"location":"docker/#conclusion","title":"\ud83c\udfaf Conclusi\u00f3n","text":"<p>Docker y Docker-compose son herramientas poderosas que simplifican el desarrollo y despliegue de aplicaciones en diferentes entornos. No importa si eres un desarrollador que quiere probar su aplicaci\u00f3n localmente o si administras un servidor en producci\u00f3n, estas herramientas te ayudar\u00e1n a crear entornos confiables y escalables.</p> <p>\ud83d\udca1 Consejo: Comienza por crear un peque\u00f1o contenedor con Docker y luego explora c\u00f3mo usar Docker-compose para manejar aplicaciones m\u00e1s complejas con m\u00faltiples servicios.</p>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/","title":"Big Data: Motivaci\u00f3n, Almacenamiento y Procesamiento \ud83d\ude80\ud83d\udcca","text":"<p>El t\u00e9rmino Big Data ha ganado una gran relevancia en la era digital moderna. No se trata solo de manejar grandes vol\u00famenes de datos, sino de extraer valor y conocimiento de ellos para tomar decisiones informadas y estrat\u00e9gicas. Desde sus or\u00edgenes hasta su integraci\u00f3n con tecnolog\u00edas avanzadas como el Cloud Computing y la Inteligencia Artificial, el Big Data ha revolucionado la forma en que las organizaciones operan en m\u00faltiples sectores.</p>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#motivacion-del-big-data-y-su-origen","title":"\ud83c\udf1f Motivaci\u00f3n del Big Data y su Origen","text":"<p>El Big Data surgi\u00f3 como una respuesta natural a la creciente cantidad de datos generados por dispositivos digitales, redes sociales, sensores IoT, smartphones, y una multitud de otros dispositivos conectados a la red. Este crecimiento exponencial de datos oblig\u00f3 a las organizaciones a buscar soluciones para almacenar, procesar y analizar eficientemente esta informaci\u00f3n masiva.</p> <p>Empresas pioneras como Google, Facebook y Amazon fueron las primeras en desarrollar e implementar infraestructuras capaces de manejar cantidades inmensas de datos. Al hacerlo, comenzaron a descubrir patrones, comportamientos y tendencias que ofrec\u00edan insights valiosos para la toma de decisiones.</p>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#por-que-es-importante-el-big-data","title":"\u00bfPor qu\u00e9 es Importante el Big Data?","text":"<p>El Big Data ha sido un motor clave para la transformaci\u00f3n digital de las empresas. A trav\u00e9s del an\u00e1lisis masivo de datos, las organizaciones pueden descubrir patrones y relaciones que no eran visibles antes, optimizando su rendimiento y abriendo nuevas oportunidades de crecimiento. Las razones clave de su importancia incluyen:</p> <ol> <li> <p>Tomar Decisiones Basadas en Datos:     Gracias al an\u00e1lisis de grandes vol\u00famenes de datos, las empresas pueden identificar patrones y tendencias que antes eran dif\u00edciles de detectar. Esto permite tomar decisiones m\u00e1s r\u00e1pidas y acertadas. Por ejemplo, una empresa minorista puede analizar millones de transacciones de clientes para identificar qu\u00e9 productos tienen una mayor probabilidad de compra en ciertas temporadas.</p> </li> <li> <p>Optimizaci\u00f3n de Procesos:     El Big Data permite optimizar procesos en todos los niveles. Desde la cadena de suministro hasta el marketing, el an\u00e1lisis de datos permite identificar ineficiencias y \u00e1reas de mejora. Un ejemplo claro es el uso de datos en la log\u00edstica, donde el an\u00e1lisis de rutas en tiempo real puede reducir los tiempos de entrega y los costos de transporte.</p> </li> <li> <p>Innovaci\u00f3n y Desarrollo de Nuevos Productos:     El an\u00e1lisis de datos masivos permite identificar nuevas oportunidades de mercado y desarrollar productos personalizados para satisfacer mejor las necesidades del cliente. Por ejemplo, empresas del sector de la salud pueden analizar grandes cantidades de datos m\u00e9dicos para identificar tendencias de consumo en productos saludables, lo que puede llevar al desarrollo de productos m\u00e1s alineados con los intereses del consumidor.</p> </li> </ol>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#almacenamiento-masivo-de-datos-las-vs-del-big-data","title":"\ud83c\udfe2 Almacenamiento Masivo de Datos: Las Vs del Big Data","text":"<p>El almacenamiento de grandes vol\u00famenes de datos es un pilar fundamental en el ecosistema de Big Data. Tradicionalmente, se mencionan las \"4Vs\" para describir las caracter\u00edsticas de Big Data, pero en realidad, existen muchas m\u00e1s Vs que a\u00f1aden complejidad y potencial a su an\u00e1lisis:</p> <ul> <li> <p>Volumen \ud83d\udce6: Se refiere a la enorme cantidad de datos generados y almacenados en sistemas digitales cada segundo. Desde redes sociales hasta dispositivos IoT, cada actividad genera datos que se deben almacenar y analizar. Empresas como Facebook y YouTube generan petabytes de datos diariamente. El reto es manejar este volumen de datos de manera eficiente sin perder velocidad de procesamiento.</p> </li> <li> <p>Velocidad \u26a1: Es la rapidez con la que se generan, recopilan y procesan los datos. Los sistemas de Big Data deben ser capaces de procesar la informaci\u00f3n casi en tiempo real para generar valor. Por ejemplo, empresas financieras utilizan datos en tiempo real para tomar decisiones sobre inversiones.</p> </li> <li> <p>Variedad \ud83c\udf08: Los datos provienen de diferentes fuentes y formatos, como texto, im\u00e1genes, videos, audio, transacciones, sensores IoT, etc. Esto hace necesario el uso de tecnolog\u00edas avanzadas capaces de analizar tanto datos estructurados (como bases de datos) como no estructurados (como publicaciones en redes sociales).</p> </li> <li> <p>Veracidad \ud83d\udee1\ufe0f: La calidad de los datos es crucial para tomar decisiones acertadas. Los datos incorrectos, duplicados o incompletos pueden llevar a conclusiones err\u00f3neas. Garantizar la veracidad de los datos implica establecer controles de calidad y limpieza antes de procesarlos para asegurar su fiabilidad.</p> </li> <li> <p>Variabilidad: No solo los vol\u00famenes de datos crecen, sino que tambi\u00e9n var\u00edan constantemente. Las tendencias y patrones de comportamiento cambian a lo largo del tiempo, y el an\u00e1lisis debe adaptarse a estos cambios.</p> </li> <li> <p>Valor \ud83d\udcb0: Es la capacidad de extraer insights \u00fatiles de grandes vol\u00famenes de datos. De nada sirve almacenar cantidades masivas de datos si no se puede extraer valor de ellos. Las empresas que logran transformar los datos en informaci\u00f3n valiosa pueden mejorar su posici\u00f3n competitiva y tomar mejores decisiones.</p> </li> </ul> <p>Para m\u00e1s informaci\u00f3n sobre las m\u00faltiples Vs del Big Data, puedes consultar esta infograf\u00eda: Infograf\u00eda sobre Big Data.</p>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#sistemas-de-almacenamiento-de-datos","title":"\ud83d\udcc2 Sistemas de Almacenamiento de Datos","text":"<p>El almacenamiento de grandes vol\u00famenes de datos debe cumplir con una serie de requisitos clave para manejar el crecimiento de la informaci\u00f3n de manera eficiente. Estos requisitos son:</p> <ol> <li>Capacidad: Los sistemas de almacenamiento deben ser escalables para soportar el crecimiento continuo de datos sin comprometer el rendimiento del sistema.</li> <li>Rendimiento: El acceso r\u00e1pido y eficiente a los datos es esencial para asegurar un procesamiento eficaz. Los sistemas de almacenamiento deben estar optimizados para acceder a los datos de manera \u00e1gil, sin cuellos de botella.</li> <li>Fiabilidad: Es crucial asegurar que los datos est\u00e9n protegidos contra p\u00e9rdidas y fallos del sistema. La replicaci\u00f3n y redundancia de datos ayudan a garantizar la disponibilidad continua de la informaci\u00f3n.</li> <li>Recuperabilidad: En caso de un fallo o p\u00e9rdida accidental de datos, los sistemas deben facilitar su recuperaci\u00f3n de manera r\u00e1pida y eficiente, minimizando el tiempo de inactividad.</li> </ol>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#dispositivos-mas-usados","title":"\ud83d\ude80 Dispositivos M\u00e1s Usados","text":"<ol> <li> <p>Discos (HDD, SSD, RAID): </p> <ul> <li>Los discos duros (HDD) ofrecen gran capacidad de almacenamiento a bajo costo, pero son m\u00e1s lentos en comparaci\u00f3n con los SSD.</li> <li>Los discos de estado s\u00f3lido (SSD) son mucho m\u00e1s r\u00e1pidos y eficientes, lo que los hace ideales para aplicaciones que requieren procesamiento a alta velocidad.</li> <li>Los arreglos RAID mejoran tanto la fiabilidad como el rendimiento al combinar varios discos en configuraciones redundantes. Esto permite la continuidad del servicio en caso de fallos de un disco individual.</li> </ul> <p></p> </li> <li> <p>Cintas Magn\u00e9ticas \ud83e\uddf2: Aunque puede parecer una tecnolog\u00eda antigua, las cintas magn\u00e9ticas siguen siendo una opci\u00f3n popular para el archivado a largo plazo debido a su bajo costo. Se utilizan com\u00fanmente para almacenar grandes vol\u00famenes de datos que no necesitan ser accedidos con frecuencia, como copias de seguridad.</p> </li> <li> <p>Almacenamiento en Red (NAS, SAN) \ud83c\udf10: </p> <ul> <li>NAS (Network Attached Storage) y SAN (Storage Area Network) permiten compartir almacenamiento a trav\u00e9s de una red, facilitando el acceso a los datos desde m\u00faltiples dispositivos. Estas soluciones son comunes en empresas que manejan grandes cantidades de datos de forma colaborativa.</li> </ul> </li> <li> <p>Almacenamiento en la Nube \u2601\ufe0f: El almacenamiento en la nube se ha convertido en una de las soluciones m\u00e1s populares debido a su escalabilidad, flexibilidad y capacidad para facilitar la recuperaci\u00f3n de datos ante desastres. Adem\u00e1s, permite a las empresas reducir costos al no tener que invertir en infraestructura propia.</p> </li> </ol>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#metodos-avanzados-de-almacenamiento-clusters","title":"\ud83d\udee0\ufe0f M\u00e9todos Avanzados de Almacenamiento: Clusters","text":"<p>Los sistemas distribuidos y clusters permiten gestionar grandes vol\u00famenes de datos de manera m\u00e1s eficiente, segura y escalable. Estos sistemas distribuyen los datos en varios nodos o servidores, asegurando redundancia y mejorando el rendimiento.</p> <ul> <li> <p>Tipos de RAID: Los diferentes niveles de RAID (como RAID 0, RAID 1, RAID 5, RAID 10) ofrecen diversas combinaciones de redundancia y rendimiento. RAID 5, por ejemplo, ofrece un equilibrio entre protecci\u00f3n de datos y rendimiento, siendo ideal para entornos que necesitan redundancia sin sacrificar velocidad.</p> </li> <li> <p>GlusterFS y MooseFS: Son sistemas de archivos distribuidos dise\u00f1ados para manejar grandes vol\u00famenes de datos. Estos sistemas permiten a las organizaciones administrar sus datos a trav\u00e9s de m\u00faltiples servidores, garantizando la disponibilidad y la redundancia de la informaci\u00f3n.</p> </li> <li> <p>CephFileSystem: Es un sistema de almacenamiento distribuido y altamente escalable que ofrece capacidades avanzadas de auto-reparaci\u00f3n y recuperaci\u00f3n. Es utilizado por grandes empresas que necesitan manejar petabytes de datos.</p> </li> <li> <p>DRBD (Distributed Replicated Block Device): Proporciona replicaci\u00f3n de datos en tiempo real entre servidores, asegurando que los datos est\u00e9n siempre disponibles y sincronizados en m\u00faltiples ubicaciones. Esto es vital para sistemas de alta disponibilidad.</p> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#procesamiento-de-datos-de-la-recoleccion-a-la-visualizacion","title":"\ud83d\udd0d Procesamiento de Datos: De la Recolecci\u00f3n a la Visualizaci\u00f3n","text":"<p>El procesamiento de datos en Big Data abarca una serie de etapas clave que transforman los datos brutos en informaci\u00f3n \u00fatil y aplicable. Cada etapa es esencial para obtener insights valiosos. A continuaci\u00f3n, ilustramos cada fase utilizando un ejemplo en el sector de la salud, donde se analizan datos de millones de pacientes para detectar patrones relacionados con enfermedades cr\u00f3nicas como la diabetes o la hipertensi\u00f3n.</p>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#etapas-de-procesamiento","title":"\ud83d\udcdd Etapas de Procesamiento","text":"<pre><code>graph TB\n    A[Recolecci\u00f3n de Datos] --&gt; B[Recopilaci\u00f3n]\n    B --&gt; C[Preprocesamiento o Limpieza de Datos]\n    C --&gt; D[Procesamiento]\n    D --&gt; E[Interpretaci\u00f3n y Visualizaci\u00f3n]\n    E --&gt; F[An\u00e1lisis]\n    F --&gt; G[Almacenamiento]</code></pre> <ol> <li> <p>Recolecci\u00f3n de Datos \ud83d\udce5:     En el caso de la salud, los datos provienen de diversas fuentes como:</p> <ul> <li>Historiales m\u00e9dicos electr\u00f3nicos (EMR).</li> <li>Dispositivos port\u00e1tiles como pulseras de actividad o relojes inteligentes que monitorean constantes vitales.</li> <li>Encuestas y cuestionarios de salud.</li> <li>Bases de datos gen\u00e9ticas.</li> </ul> <p>Ejemplo: Recolectamos datos de los niveles de glucosa, actividad f\u00edsica y dieta de millones de pacientes que utilizan dispositivos m\u00e9dicos y de bienestar.</p> </li> <li> <p>Recopilaci\u00f3n: Una vez recolectados, los datos de diferentes fuentes se consolidan en un almac\u00e9n de datos distribuido (como un sistema Hadoop o un almac\u00e9n en la nube como Amazon S3 o Azure Blob Storage) para su an\u00e1lisis posterior.</p> <p>Ejemplo: Los datos de pacientes de varios hospitales y dispositivos m\u00e9dicos son centralizados en una plataforma de almacenamiento en la nube para ser procesados de manera unificada.</p> </li> <li> <p>Preprocesamiento o Limpieza de Datos \ud83e\uddf9: En esta fase, se eliminan los datos duplicados, inconsistentes o incompletos para asegurar la calidad del an\u00e1lisis. Se estandarizan los formatos de datos para que todas las fuentes utilicen las mismas unidades de medida y estructura.</p> <p>Ejemplo: Se eliminan las entradas duplicadas y se estandarizan las unidades de medida (por ejemplo, convertir los niveles de glucosa de mg/dL a mmol/L) para que los datos sean coherentes en todo el conjunto.</p> </li> <li> <p>Procesamiento \ud83d\udda5\ufe0f: Se aplican algoritmos avanzados como machine learning y t\u00e9cnicas de miner\u00eda de datos para analizar patrones dentro de los datos de los pacientes y predecir la probabilidad de desarrollar enfermedades cr\u00f3nicas.</p> <p>Ejemplo: Un algoritmo de regresi\u00f3n log\u00edstica analiza los datos y predice la probabilidad de que un paciente desarrolle diabetes en los pr\u00f3ximos cinco a\u00f1os en funci\u00f3n de sus niveles de glucosa, actividad f\u00edsica y gen\u00e9tica.</p> </li> <li> <p>Interpretaci\u00f3n y Visualizaci\u00f3n \ud83d\udcca: Los resultados se presentan mediante gr\u00e1ficos interactivos, dashboards y reportes comprensibles que ayudan a los m\u00e9dicos a entender los patrones y tendencias.</p> <p>Ejemplo: Un dashboard interactivo muestra gr\u00e1ficos sobre c\u00f3mo diferentes factores como la obesidad, la falta de ejercicio y los antecedentes familiares influyen en el riesgo de desarrollar diabetes. Los m\u00e9dicos pueden ver f\u00e1cilmente c\u00f3mo var\u00edan estos factores seg\u00fan la regi\u00f3n geogr\u00e1fica o la edad del paciente.</p> </li> <li> <p>An\u00e1lisis \ud83e\udde0: En esta fase se profundiza en los resultados obtenidos para descubrir insights valiosos. Por ejemplo, el an\u00e1lisis puede revelar correlaciones inesperadas entre los h\u00e1bitos alimenticios y la aparici\u00f3n de enfermedades.</p> <p>Ejemplo: El an\u00e1lisis revela que el 80% de los pacientes con obesidad y antecedentes familiares tienen una alta probabilidad de desarrollar diabetes tipo 2 dentro de los pr\u00f3ximos cinco a\u00f1os.</p> </li> <li> <p>Almacenamiento: Finalmente, los datos analizados y sus resultados se almacenan para usos futuros, auditor\u00edas o para ser comparados con nuevos datos en investigaciones posteriores.</p> <p>Ejemplo: Los resultados se almacenan en una base de datos distribuida para su posterior an\u00e1lisis y comparaci\u00f3n con nuevos pacientes a lo largo del tiempo, lo que permite un monitoreo continuo de las tendencias de salud p\u00fablica.</p> </li> </ol>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#analitica-en-tiempo-real","title":"\ud83d\udcc8 Anal\u00edtica en Tiempo Real","text":"<p>Uno de los mayores beneficios del Big Data es la capacidad de realizar an\u00e1lisis en tiempo real. La anal\u00edtica en tiempo real permite a las empresas reaccionar inmediatamente ante eventos que ocurren en el momento. Algunos ejemplos incluyen:</p> <ul> <li> <p>Servicios Financieros: Las instituciones financieras utilizan an\u00e1lisis en tiempo real para monitorear transacciones y detectar posibles fraudes en el momento en que ocurren. Esto permite bloquear transacciones sospechosas antes de que se completen.</p> </li> <li> <p>Plataformas de Streaming: Empresas como Netflix y Spotify analizan en tiempo real el comportamiento de sus usuarios para ofrecer recomendaciones personalizadas sobre qu\u00e9 series, pel\u00edculas o canciones ver o escuchar a continuaci\u00f3n.</p> </li> <li> <p>Smart Cities: Las ciudades inteligentes utilizan sensores distribuidos en toda la infraestructura urbana para monitorear el tr\u00e1fico, los niveles de contaminaci\u00f3n y el consumo de energ\u00eda en tiempo real. Esto permite ajustes autom\u00e1ticos para optimizar el uso de recursos y mejorar la calidad de vida de los ciudadanos.</p> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#big-data-y-cloud-computing","title":"\u2601\ufe0f Big Data y Cloud Computing","text":"<p>El Cloud Computing ha abierto un nuevo mundo de posibilidades para el Big Data. Al combinar ambas tecnolog\u00edas, las empresas pueden escalar sus operaciones sin necesidad de costosas inversiones en infraestructura f\u00edsica.</p>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#ventajas-del-cloud-computing-para-big-data","title":"Ventajas del Cloud Computing para Big Data","text":"<ul> <li> <p>Escalabilidad Ilimitada: El cloud computing permite a las empresas ajustar su capacidad de procesamiento y almacenamiento seg\u00fan sea necesario. Esto significa que pueden escalar vertical y horizontalmente sin comprometer el rendimiento.</p> </li> <li> <p>Costos Bajo Demanda: Las empresas solo pagan por los recursos que utilizan, lo que optimiza los costos operativos y evita gastos innecesarios en infraestructura f\u00edsica.</p> </li> <li> <p>Accesibilidad Global: El almacenamiento en la nube permite que los datos y las aplicaciones sean accesibles desde cualquier parte del mundo, facilitando la colaboraci\u00f3n entre equipos distribuidos geogr\u00e1ficamente.</p> </li> <li> <p>Seguridad y Recuperaci\u00f3n: Las soluciones en la nube ofrecen avanzadas medidas de seguridad y recuperaci\u00f3n ante desastres, asegurando que los datos est\u00e9n protegidos y disponibles incluso en situaciones de emergencia.</p> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#conclusion","title":"\ud83d\ude80 Conclusi\u00f3n","text":"<p>El Big Data ha transformado la forma en que las organizaciones capturan, almacenan, procesan y analizan datos. Desde sus or\u00edgenes hasta las avanzadas soluciones de Cloud Computing y Inteligencia Artificial, el Big Data ha proporcionado una plataforma poderosa para la innovaci\u00f3n y la toma de decisiones estrat\u00e9gicas. La combinaci\u00f3n de almacenamiento masivo, procesamiento distribuido y an\u00e1lisis en tiempo real est\u00e1 remodelando industrias enteras, creando nuevas oportunidades y optimizando las operaciones empresariales. \u00a1Es el momento de aprovechar el poder del Big Data para llevar tu organizaci\u00f3n al siguiente nivel!</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/","title":"2.1. \u00bfQu\u00e9 es Apache Hadoop?","text":""},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#que-es-apache-hadoop","title":"\u00bfQu\u00e9 es Apache Hadoop? \ud83d\ude80","text":"<p>Apache Hadoop es un marco de software de c\u00f3digo abierto dise\u00f1ado para el almacenamiento y procesamiento masivo de datos en cl\u00fasteres de computadoras. Gracias a su arquitectura distribuida, Hadoop es capaz de manejar grandes cantidades de informaci\u00f3n de manera eficiente y rentable, convirti\u00e9ndose en un pilar esencial en el mundo del Big Data.</p> <p>Hadoop no solo almacena datos, sino que tambi\u00e9n facilita su procesamiento en paralelo, lo que permite analizar grandes vol\u00famenes de informaci\u00f3n de manera r\u00e1pida. Su capacidad para escalar desde unos pocos servidores hasta miles lo convierte en una herramienta flexible y poderosa para empresas de todos los tama\u00f1os.</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#como-funciona-hadoop","title":"\ud83e\udde0 \u00bfC\u00f3mo Funciona Hadoop?","text":"<p>Hadoop se compone principalmente de cuatro m\u00f3dulos que trabajan en conjunto para proporcionar un ecosistema completo de Big Data:</p> <ol> <li> <p>HDFS (Hadoop Distributed File System) \ud83d\udcc2: Almacena grandes vol\u00famenes de datos distribuidos a trav\u00e9s de m\u00faltiples nodos, garantizando alta disponibilidad y resistencia a fallos.</p> </li> <li> <p>YARN (Yet Another Resource Negotiator) \ud83c\udfaf: Act\u00faa como un administrador de recursos, asignando tareas y gestionando recursos de manera eficiente dentro del cl\u00faster.</p> </li> <li> <p>MapReduce \ud83d\udee0\ufe0f: Es el motor de procesamiento de datos que divide las tareas en subtareas m\u00e1s peque\u00f1as, permitiendo el procesamiento en paralelo de grandes conjuntos de datos.</p> </li> <li> <p>Hadoop Common \u2699\ufe0f: Proporciona las herramientas y utilidades b\u00e1sicas que soportan los dem\u00e1s m\u00f3dulos, facilitando la integraci\u00f3n y el funcionamiento del ecosistema.</p> </li> </ol>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#por-que-elegir-hadoop","title":"\ud83d\udea6 \u00bfPor Qu\u00e9 Elegir Hadoop?","text":""},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#1-escalabilidad-infinita","title":"1. Escalabilidad Infinita \ud83c\udfd7\ufe0f","text":"<p>Hadoop est\u00e1 dise\u00f1ado para crecer junto con tus necesidades. Desde unos pocos nodos hasta miles de m\u00e1quinas, puede manejar crecimientos exponenciales de datos sin perder rendimiento. Su arquitectura permite la adici\u00f3n de nodos sin necesidad de reconfigurar el sistema, lo que facilita la expansi\u00f3n continua.</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#2-rentabilidad","title":"2. Rentabilidad \ud83d\udcb0","text":"<p>El uso de hardware b\u00e1sico y de bajo costo hace que Hadoop sea una soluci\u00f3n asequible para las empresas que necesitan manejar grandes vol\u00famenes de datos. A diferencia de otros sistemas de datos que requieren hardware especializado, Hadoop se ejecuta en servidores comunes, reduciendo significativamente los costos de infraestructura.</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#3-flexibilidad-y-adaptabilidad","title":"3. Flexibilidad y Adaptabilidad \ud83d\udd04","text":"<p>No importa si tus datos son estructurados, no estructurados o semiestructurados; Hadoop puede almacenar y procesar cualquier tipo de informaci\u00f3n. Esto lo hace ideal para un amplio rango de aplicaciones, desde an\u00e1lisis de redes sociales hasta procesamiento de registros de sensores.</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#4-resistencia-a-fallos","title":"4. Resistencia a Fallos \ud83d\udd12","text":"<p>Hadoop est\u00e1 dise\u00f1ado con la seguridad en mente. Al replicar datos en varios nodos dentro del cl\u00faster, garantiza que la informaci\u00f3n est\u00e9 disponible incluso si un nodo falla, asegurando la continuidad operativa sin interrupciones.</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#5-procesamiento-rapido-y-paralelo","title":"5. Procesamiento R\u00e1pido y Paralelo \u26a1","text":"<p>Gracias a MapReduce, Hadoop procesa grandes vol\u00famenes de datos en paralelo, dividiendo tareas complejas en subtareas m\u00e1s peque\u00f1as. Esto ahorra tiempo y mejora la eficiencia al manejar trabajos que, de otro modo, podr\u00edan llevar horas o d\u00edas.</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#procesamiento-distribuido-clusteres-y-tipos-de-nodos","title":"\ud83d\udd27 Procesamiento Distribuido, Cl\u00fasteres y Tipos de Nodos","text":"<p>El procesamiento distribuido es el coraz\u00f3n de Apache Hadoop, permitiendo que grandes vol\u00famenes de datos se dividan en partes m\u00e1s peque\u00f1as y se procesen de manera simult\u00e1nea en varios nodos dentro de un cl\u00faster.</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#procesamiento-distribuido","title":"Procesamiento Distribuido","text":"<p>El procesamiento distribuido implica dividir las tareas de procesamiento de datos en partes m\u00e1s peque\u00f1as, las cuales se ejecutan de forma paralela en diferentes m\u00e1quinas o nodos. En lugar de procesar los datos de forma secuencial en un solo servidor, Hadoop divide los datos en bloques que son procesados simult\u00e1neamente. Esto aumenta la eficiencia y reduce el tiempo necesario para analizar grandes vol\u00famenes de informaci\u00f3n.</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#clusteres-en-hadoop","title":"Cl\u00fasteres en Hadoop","text":"<p>Un cl\u00faster en Hadoop est\u00e1 compuesto por un conjunto de nodos conectados entre s\u00ed, que trabajan en conjunto para almacenar y procesar datos de manera distribuida. Los cl\u00fasteres est\u00e1n dise\u00f1ados para ser escalables, lo que significa que se pueden agregar m\u00e1s nodos seg\u00fan sea necesario, incrementando la capacidad de procesamiento sin afectar el rendimiento general.</p> <ul> <li>Escalabilidad Horizontal: Se logra a\u00f1adiendo m\u00e1s nodos al cl\u00faster.</li> <li>Distribuci\u00f3n de Carga: Los datos y tareas se distribuyen entre todos los nodos para maximizar el uso eficiente de los recursos.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#tipos-de-nodos-en-hadoop","title":"Tipos de Nodos en Hadoop","text":"<p>En un cl\u00faster de Hadoop, existen varios tipos de nodos que desempe\u00f1an funciones cr\u00edticas dentro del sistema:</p> <ol> <li> <p>Nodo Maestro (NameNode):  </p> <ul> <li>Funci\u00f3n: Coordina el almacenamiento de datos y gestiona el sistema de archivos distribuido (HDFS). Supervisa la ubicaci\u00f3n de los bloques de datos y garantiza la replicaci\u00f3n para asegurar la resistencia a fallos.</li> <li>Responsabilidades: Mantiene el registro de d\u00f3nde est\u00e1n almacenados los bloques de datos en los DataNodes. El NameNode es esencial para la administraci\u00f3n general del cl\u00faster y su buen funcionamiento.</li> </ul> </li> <li> <p>Nodos de Datos (DataNode):  </p> <ul> <li>Funci\u00f3n: Son responsables de almacenar los bloques de datos y responder a las solicitudes del NameNode. Estos nodos realizan la mayor parte del trabajo pesado al manejar los datos en bruto.</li> <li>Responsabilidades: Almacenan los bloques de datos y ejecutan las tareas de procesamiento. Cada DataNode puede almacenar varias copias de los datos, asegurando la replicaci\u00f3n y la alta disponibilidad. Esto garantiza la recuperaci\u00f3n de datos en caso de fallos.</li> </ul> </li> <li> <p>Nodos Edge (EdgeNode):  </p> <ul> <li>Funci\u00f3n: Los nodos edge act\u00faan como un puente entre el cl\u00faster Hadoop y la red externa. Son los nodos a trav\u00e9s de los cuales los usuarios pueden interactuar con el cl\u00faster, enviar trabajos y obtener resultados.</li> <li>Responsabilidades: Proporcionan interfaces para que los datos y comandos entren y salgan del cl\u00faster de manera segura. Estos nodos no almacenan ni procesan datos directamente, pero ofrecen una capa de seguridad y control al filtrar el acceso y las solicitudes a los NameNodes y DataNodes.</li> </ul> </li> </ol> <pre><code>graph LR\n    subgraph Cl\u00faster de Hadoop\n        A[NameNode] --&gt; B[DataNode 1]\n        A --&gt; C[DataNode 2]\n        A --&gt; D[DataNode 3]\n    end\n    E[EdgeNode] --&gt; A\n    F[Usuarios] --&gt; E</code></pre>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#tolerancia-a-fallos-en-los-nodos","title":"Tolerancia a Fallos en los Nodos","text":"<p>Hadoop garantiza la tolerancia a fallos mediante la replicaci\u00f3n de los datos en m\u00faltiples DataNodes. Si un nodo falla, los datos a\u00fan est\u00e1n disponibles en otros nodos del cl\u00faster, lo que evita la p\u00e9rdida de informaci\u00f3n y garantiza que el procesamiento contin\u00fae sin interrupciones.</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#casos-de-uso-de-hadoop","title":"\ud83c\udf10 Casos de Uso de Hadoop","text":"<p>Hadoop ha revolucionado m\u00faltiples industrias gracias a su capacidad para manejar grandes vol\u00famenes de datos de manera eficiente. Aqu\u00ed algunos de los sectores donde Hadoop marca la diferencia:</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#1-finanzas-y-bancos","title":"1. Finanzas y Bancos \ud83c\udfe6","text":"<ul> <li>Detecci\u00f3n de Fraudes: Analiza patrones en tiempo real para detectar y prevenir actividades fraudulentas.</li> <li>An\u00e1lisis de Riesgos: Procesa grandes vol\u00famenes de datos financieros para identificar y gestionar riesgos.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#2-salud","title":"2. Salud \ud83c\udfe5","text":"<ul> <li>Gen\u00f3mica: Procesa datos de secuenciaci\u00f3n gen\u00e9tica para avanzar en la medicina personalizada.</li> <li>An\u00e1lisis de Im\u00e1genes M\u00e9dicas: Maneja grandes vol\u00famenes de im\u00e1genes para mejorar diagn\u00f3sticos y tratamientos.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#3-telecomunicaciones","title":"3. Telecomunicaciones \ud83d\udce1","text":"<ul> <li>An\u00e1lisis de Redes: Monitorea y optimiza el rendimiento de las redes en tiempo real.</li> <li>Modelos Predictivos: Utiliza datos hist\u00f3ricos para prever fallos y optimizar el servicio al cliente.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#4-retail-y-e-commerce","title":"4. Retail y E-commerce \ud83d\uded2","text":"<ul> <li>An\u00e1lisis del Comportamiento del Cliente: Utiliza datos de navegaci\u00f3n y compra para personalizar ofertas.</li> <li>Gesti\u00f3n de Inventarios: Optimiza la cadena de suministro basada en patrones de compra y demanda.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#conclusion","title":"\ud83c\udf10 Conclusi\u00f3n","text":"<p>Apache Hadoop no es solo una tecnolog\u00eda; es una revoluci\u00f3n en la forma en que manejamos y procesamos grandes vol\u00famenes de datos. Desde su capacidad para escalar hasta miles de nodos, hasta su capacidad para procesar datos en paralelo, Hadoop ha transformado la forma en que las organizaciones analizan y gestionan sus datos.</p>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/","title":"Ecosistema Hadoop y Distribuciones \ud83c\udf10\ud83d\ude80","text":"<p>El ecosistema Hadoop ha revolucionado la forma en que las organizaciones manejan y procesan datos a gran escala. Hadoop no es solo un software; es un ecosistema completo de herramientas y tecnolog\u00edas que trabajan juntas para resolver los desaf\u00edos del Big Data. A medida que el volumen, la variedad y la velocidad de los datos contin\u00faan creciendo, el ecosistema Hadoop se expande para incluir m\u00faltiples componentes y distribuciones dise\u00f1adas para aprovechar al m\u00e1ximo esta revoluci\u00f3n de datos.</p>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#que-es-el-ecosistema-hadoop","title":"\ud83e\udde0 \u00bfQu\u00e9 es el Ecosistema Hadoop?","text":"<p>El ecosistema Hadoop es una colecci\u00f3n de proyectos y herramientas que interact\u00faan entre s\u00ed para proporcionar una plataforma integral para el almacenamiento, procesamiento y an\u00e1lisis de grandes vol\u00famenes de datos. Este ecosistema incluye componentes para la ingesti\u00f3n de datos, el procesamiento en tiempo real, el an\u00e1lisis avanzado y la gesti\u00f3n de recursos.</p> <pre><code>graph LR\n    A[Hadoop Ecosystem] --&gt; B[HDFS]\n    A --&gt; C[YARN]\n    A --&gt; D[MapReduce]\n    A --&gt; E[Spark]\n    A --&gt; F[Hive]\n    A --&gt; G[HBase]\n    A --&gt; H[Pig]\n    A --&gt; I[Oozie]\n    A --&gt; J[Sqoop]\n    A --&gt; K[Flume]</code></pre>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#componentes-clave-del-ecosistema-hadoop","title":"Componentes Clave del Ecosistema Hadoop \ud83d\udee0\ufe0f","text":"<ol> <li> <p>HDFS (Hadoop Distributed File System) \ud83d\udcc2: El sistema de archivos distribuido que almacena grandes vol\u00famenes de datos de manera eficiente y segura.</p> </li> <li> <p>YARN (Yet Another Resource Negotiator) \ud83c\udfaf: Gestor de recursos que asigna y administra las tareas dentro del cl\u00faster Hadoop.</p> </li> <li> <p>MapReduce \ud83d\udee0\ufe0f: Modelo de programaci\u00f3n que permite el procesamiento paralelo de datos en un entorno distribuido.</p> </li> <li> <p>Apache Spark \u26a1: Motor de procesamiento r\u00e1pido y en memoria que ofrece una alternativa m\u00e1s \u00e1gil a MapReduce para el an\u00e1lisis de datos en tiempo real.</p> </li> <li> <p>Apache Hive \ud83d\udc1d: Herramienta que facilita la consulta y el an\u00e1lisis de datos almacenados en HDFS utilizando un lenguaje similar a SQL, conocido como HiveQL.</p> </li> <li> <p>Apache HBase \ud83d\udcca: Base de datos NoSQL de alto rendimiento que proporciona acceso en tiempo real a grandes vol\u00famenes de datos distribuidos.</p> </li> <li> <p>Apache Pig \ud83d\udc37: Lenguaje de alto nivel para el procesamiento de grandes conjuntos de datos que simplifica la escritura de scripts complejos en comparaci\u00f3n con MapReduce.</p> </li> <li> <p>Apache Oozie \ud83d\udcc5: Coordinador de flujos de trabajo que permite programar y gestionar trabajos en Hadoop.</p> </li> <li> <p>Apache Sqoop \ud83d\udd04: Herramienta que facilita la transferencia de datos entre Hadoop y bases de datos relacionales.</p> </li> <li> <p>Apache Flume \ud83d\udce5: Sistema de ingesti\u00f3n de datos que permite recopilar, agregar y mover grandes cantidades de datos de eventos a Hadoop.</p> </li> </ol>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#distribuciones-populares-de-hadoop","title":"\ud83c\udf10 Distribuciones Populares de Hadoop","text":"<p>Las distribuciones de Hadoop son paquetes que integran el ecosistema de herramientas Hadoop con caracter\u00edsticas adicionales de administraci\u00f3n y soporte. Estas distribuciones est\u00e1n dise\u00f1adas para simplificar la implementaci\u00f3n, configuraci\u00f3n y mantenimiento de cl\u00fasteres Hadoop. Aqu\u00ed te presentamos algunas de las m\u00e1s populares:</p>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#1-cloudera-distribution-for-hadoop-cdh","title":"1. Cloudera Distribution for Hadoop (CDH) \ud83c\udfe2","text":"<p>Cloudera es una de las distribuciones comerciales m\u00e1s reconocidas de Hadoop. Ofrece una versi\u00f3n completa del ecosistema Hadoop con herramientas adicionales para la gesti\u00f3n, seguridad y an\u00e1lisis de datos.</p> <ul> <li>Gesti\u00f3n Simplificada: Cloudera Manager permite gestionar y monitorear el cl\u00faster de forma centralizada.</li> <li>Seguridad Mejorada: Ofrece encriptaci\u00f3n de datos y autenticaci\u00f3n avanzada.</li> <li>Optimizaci\u00f3n de Desempe\u00f1o: Ajustes autom\u00e1ticos que mejoran la eficiencia de las tareas.</li> </ul> <pre><code>// Ejemplo de conexi\u00f3n a un cl\u00faster de Hadoop usando Cloudera\nconst cloudera = require('cloudera-api');\n\n// Conectar al cl\u00faster de Cloudera\nconst client = new cloudera.Cluster({\n  hostname: 'cloudera-cluster.local',\n  username: 'admin',\n  password: 'password'\n});\n\nclient.getStatus((err, status) =&gt; {\n  if (err) {\n    console.error('Error conectando al cl\u00faster:', err);\n  } else {\n    console.log('Estado del cl\u00faster:', status);\n  }\n});\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#2-hortonworks-data-platform-hdp","title":"2. Hortonworks Data Platform (HDP) \ud83d\udc18","text":"<p>Hortonworks, ahora parte de Cloudera, ofrece una distribuci\u00f3n de Hadoop completamente de c\u00f3digo abierto. HDP se enfoca en la integraci\u00f3n de datos y proporciona un s\u00f3lido conjunto de herramientas para el an\u00e1lisis y la gesti\u00f3n de datos.</p> <ul> <li>Soporte 100% Open Source: Fomenta la innovaci\u00f3n y permite la personalizaci\u00f3n completa de la plataforma.</li> <li>Integraci\u00f3n con la Nube: Compatible con implementaciones en la nube y en entornos h\u00edbridos.</li> <li>Simplificaci\u00f3n de Operaciones: Herramientas para la automatizaci\u00f3n de flujos de trabajo y la gesti\u00f3n de datos.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#3-mapr","title":"3. MapR \ud83c\udf32","text":"<p>MapR destaca por su arquitectura \u00fanica que combina Hadoop con un sistema de archivos distribuido patentado y una base de datos NoSQL integrada. Ofrece una alta disponibilidad y rendimiento superior en comparaci\u00f3n con otras distribuciones.</p> <ul> <li>MapR XD y MapR DB: Proporcionan almacenamiento y gesti\u00f3n de datos avanzados con capacidades empresariales.</li> <li>Soporte de Contenedores y Microservicios: Compatible con Kubernetes para la implementaci\u00f3n de aplicaciones modernas.</li> <li>Procesamiento en Tiempo Real: Capacidades para an\u00e1lisis de flujos de datos en tiempo real.</li> </ul> <pre><code>// Ejemplo de integraci\u00f3n con MapR usando JavaScript\nconst mapr = require('mapr-streams');\n\n// Configuraci\u00f3n de una conexi\u00f3n de flujo de datos en tiempo real\nconst stream = mapr.createStream('/path/to/stream');\n\nstream.on('data', (message) =&gt; {\n  console.log('Mensaje recibido:', message.value.toString());\n});\n\nstream.write({ key: 'sensor1', value: 'temperatura: 22\u00b0C' });\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#4-amazon-emr-elastic-mapreduce","title":"4. Amazon EMR (Elastic MapReduce) \u2601\ufe0f","text":"<p>Amazon EMR es la distribuci\u00f3n basada en la nube de Hadoop ofrecida por AWS. Permite escalar f\u00e1cilmente el cl\u00faster y ajustar los recursos seg\u00fan la demanda de procesamiento de datos.</p> <ul> <li>Escalabilidad Autom\u00e1tica: Ajusta la capacidad del cl\u00faster en funci\u00f3n de la carga de trabajo.</li> <li>Integraci\u00f3n con Servicios AWS: F\u00e1cil integraci\u00f3n con S3, Redshift y otras soluciones de AWS.</li> <li>Costos Bajo Demanda: Paga solo por lo que usas, optimizando los costos operativos.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#casos-de-uso-del-ecosistema-hadoop","title":"\ud83d\udea6 Casos de Uso del Ecosistema Hadoop","text":"<p>El ecosistema Hadoop no solo almacena datos; lo transforma en valor accionable. A continuaci\u00f3n, se presentan algunos casos de uso donde las empresas utilizan Hadoop y sus herramientas para generar impacto:</p>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#1-analisis-de-redes-sociales","title":"1. An\u00e1lisis de Redes Sociales \ud83d\udde8\ufe0f","text":"<p>Las empresas analizan millones de interacciones en redes sociales para entender las tendencias del mercado y la opini\u00f3n del cliente. Herramientas como Spark y Hive se utilizan para procesar estos datos r\u00e1pidamente.</p>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#2-recomendacion-de-productos","title":"2. Recomendaci\u00f3n de Productos \ud83d\udecd\ufe0f","text":"<p>Las plataformas de e-commerce utilizan algoritmos de aprendizaje autom\u00e1tico en Hadoop para analizar el comportamiento del usuario y recomendar productos personalizados en tiempo real.</p> <pre><code>// Ejemplo de recomendaci\u00f3n de productos usando Spark y JavaScript\nconst spark = require('apache-spark');\n\n// Crear un modelo de recomendaci\u00f3n basado en el historial de compras del usuario\nconst recommendations = spark.mllib.recommendation.ALS.train(usersPurchases, 10, 0.01);\n\nrecommendations.predict(user, (err, products) =&gt; {\n  if (err) {\n    console.error('Error generando recomendaciones:', err);\n  } else {\n    console.log('Productos recomendados:', products);\n  }\n});\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#3-prevencion-de-fraudes-financieros","title":"3. Prevenci\u00f3n de Fraudes Financieros \ud83c\udfe6","text":"<p>Bancos y aseguradoras usan Hadoop para analizar transacciones en tiempo real y detectar patrones sospechosos. Hadoop permite combinar m\u00faltiples fuentes de datos para una detecci\u00f3n de fraudes m\u00e1s precisa y r\u00e1pida.</p>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#4-monitoreo-de-infraestructuras","title":"4. Monitoreo de Infraestructuras \ud83d\udce1","text":"<p>Las empresas de telecomunicaciones utilizan el ecosistema Hadoop para monitorear sus infraestructuras de red, detectando fallos y optimizando el rendimiento en tiempo real.</p>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#ejemplo-completo-de-integracion-del-ecosistema-hadoop-en-javascript","title":"\ud83d\udee0\ufe0f Ejemplo Completo de Integraci\u00f3n del Ecosistema Hadoop en JavaScript","text":"<p>Para entender c\u00f3mo funciona el ecosistema Hadoop en la pr\u00e1ctica, consideremos un ejemplo completo que integra varios componentes:</p> <pre><code>const hdfs = require('hdfs');\nconst spark = require('apache-spark');\nconst hive = require('hive-client');\n\n// Configuraci\u00f3n de HDFS\nconst hdfsClient = hdfs({\n  protocol: 'http',\n  hostname: 'localhost',\n  port: 9870\n});\n\n// Subir datos a HDFS\nhdfsClient.createFile('/user/data.txt', 'Datos para an\u00e1lisis', (err) =&gt; {\n  if (err) {\n    console.error('Error al subir archivo:', err);\n  } else {\n    console.log('Archivo subido a HDFS correctamente.');\n  }\n});\n\n// Consultar datos en Hive\nconst hiveClient = hive.createClient({ host: 'localhost', port: 10000 });\n\nhiveClient.connect().then(() =&gt; {\n  hiveClient.query('SELECT * FROM logs WHERE event=\"error\";', (err, results) =&gt; {\n    if (err) {\n      console.error('Error en la consulta Hive:', err);\n    } else {\n\n\n console.log('Resultados de la consulta:', results);\n    }\n  });\n});\n\n// Procesar datos con Spark\nspark.session.builder().getOrCreate().then(session =&gt; {\n  const dataFrame = session.read().format('csv').load('/user/data.txt');\n\n  dataFrame.filter(dataFrame.col('event').equalTo('error'))\n    .show()\n    .then(() =&gt; session.stop());\n});\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#conclusion","title":"\ud83c\udf1f Conclusi\u00f3n","text":"<p>El ecosistema Hadoop y sus diversas distribuciones son fundamentales para cualquier estrategia de Big Data moderna. Ofrecen una soluci\u00f3n integral para almacenar, procesar y analizar datos a gran escala, permitiendo a las empresas tomar decisiones informadas basadas en datos. Con la flexibilidad para manejar todo tipo de datos y la capacidad de escalar a cualquier tama\u00f1o de cl\u00faster, Hadoop contin\u00faa liderando la transformaci\u00f3n digital en todo el mundo. \u00a1Explora el poder del ecosistema Hadoop y descubre c\u00f3mo puede revolucionar tu gesti\u00f3n de datos! \ud83d\ude80\ud83d\udcca</p>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/","title":"Arquitectura de Hadoop: Desentra\u00f1ando la Potencia del Big Data \ud83d\ude80\ud83e\udde0","text":"<p>Apache Hadoop es una plataforma robusta de c\u00f3digo abierto dise\u00f1ada para almacenar y procesar grandes vol\u00famenes de datos de manera eficiente y escalable. Pero \u00bfqu\u00e9 hace que Hadoop sea tan poderoso? La respuesta radica en su arquitectura distribuida, que permite procesar datos a trav\u00e9s de m\u00faltiples nodos de manera paralela. En este art\u00edculo, exploraremos en detalle la arquitectura de Hadoop, desglosando sus componentes y c\u00f3mo trabajan juntos para hacer del Big Data una realidad accesible.</p>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#componentes-principales-de-la-arquitectura-de-hadoop","title":"\ud83e\udde9 Componentes Principales de la Arquitectura de Hadoop","text":"<p>La arquitectura de Hadoop se compone de varios m\u00f3dulos que colaboran para ofrecer un entorno completo de almacenamiento y procesamiento de datos. Los componentes clave son:</p> <pre><code>graph TD\n    A[Arquitectura de Hadoop] --&gt; B[HDFS]\n    A --&gt; C[YARN]\n    A --&gt; D[MapReduce]\n    A --&gt; E[Hadoop Common]\n    A --&gt; F[Componentes de Integraci\u00f3n]</code></pre>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#1-hdfs-hadoop-distributed-file-system","title":"1. HDFS (Hadoop Distributed File System) \ud83d\udcc2","text":"<p>HDFS es el sistema de archivos distribuido de Hadoop, dise\u00f1ado para almacenar datos de manera segura y eficiente en grandes cl\u00fasteres. Se encarga de dividir los archivos grandes en bloques y distribuirlos a trav\u00e9s de diferentes nodos en el cl\u00faster.</p> <pre><code>graph TD\n    HDFS[HDFS] --&gt;|Divide Archivos en| Bloques[Bloques]\n    Bloques --&gt;|Distribuye en| Nodos[Nodos]\n    Nodos --&gt;|Copia y Replicaci\u00f3n| Copias[Copias de Seguridad]</code></pre>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#caracteristicas-de-hdfs","title":"Caracter\u00edsticas de HDFS:","text":"<ul> <li>Alta Disponibilidad: Al replicar los bloques de datos en varios nodos, asegura que los datos est\u00e9n siempre disponibles, incluso si uno de los nodos falla.</li> <li>Escalabilidad: A\u00f1adir nuevos nodos al cl\u00faster incrementa autom\u00e1ticamente la capacidad de almacenamiento.</li> <li>Tolerancia a Fallos: Dise\u00f1ado para detectar y recuperarse autom\u00e1ticamente de fallos de hardware y software.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#2-yarn-yet-another-resource-negotiator","title":"2. YARN (Yet Another Resource Negotiator) \ud83c\udfaf","text":"<p>YARN act\u00faa como el gestor de recursos de Hadoop. Asigna recursos de procesamiento a las aplicaciones y coordina la ejecuci\u00f3n de tareas en el cl\u00faster, asegurando que los trabajos se completen de manera eficiente.</p> <pre><code>graph TD\n    YARN[YARN] --&gt;|Gesti\u00f3n de Recursos| Asignaci\u00f3n[Asignaci\u00f3n de Recursos]\n    YARN --&gt;|Ejecuci\u00f3n de Tareas| Tareas[Tareas del Cl\u00faster]\n    Tareas --&gt;|Optimizaci\u00f3n| Eficiencia[Eficiencia del Cl\u00faster]</code></pre>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#caracteristicas-de-yarn","title":"Caracter\u00edsticas de YARN:","text":"<ul> <li>Asignaci\u00f3n Din\u00e1mica de Recursos: Distribuye recursos seg\u00fan las necesidades de las aplicaciones en tiempo real, optimizando el uso del cl\u00faster.</li> <li>Seguridad y Control: Ofrece control granular sobre la ejecuci\u00f3n de tareas, garantizando la seguridad y estabilidad del sistema.</li> <li>Escalabilidad: Permite la expansi\u00f3n del cl\u00faster sin necesidad de reconfiguraciones complejas.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#3-mapreduce","title":"3. MapReduce \ud83d\udee0\ufe0f","text":"<p>MapReduce es el modelo de programaci\u00f3n de Hadoop que permite el procesamiento paralelo de grandes vol\u00famenes de datos. Consiste en dos fases principales: Map y Reduce.</p> <pre><code>graph TD\n    A[MapReduce] --&gt; B[Map]\n    A --&gt; C[Reduce]\n    B --&gt; D[Procesa Datos en Pares Clave-Valor]\n    C --&gt; E[Combina y Reduce Resultados]</code></pre> <ul> <li>Map: Toma los datos de entrada y los procesa en pares clave-valor.</li> <li>Reduce: Combina estos pares para generar un resultado final.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#ejemplo-de-mapreduce-en-javascript","title":"Ejemplo de MapReduce en JavaScript:","text":"<pre><code>// Ejemplo de MapReduce para contar palabras\nconst map = (text) =&gt; {\n  return text.split(' ').map(word =&gt; ({ key: word, value: 1 }));\n};\n\nconst reduce = (mappedData) =&gt; {\n  return mappedData.reduce((acc, curr) =&gt; {\n    acc[curr.key] = (acc[curr.key] || 0) + curr.value;\n    return acc;\n  }, {});\n};\n\nconst data = \"Hadoop es incre\u00edble, Hadoop es poderoso\";\nconst mapped = map(data);\nconst reduced = reduce(mapped);\n\nconsole.log(reduced); // { Hadoop: 2, es: 2, incre\u00edble: 1, poderoso: 1 }\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#4-hadoop-common","title":"4. Hadoop Common \u2699\ufe0f","text":"<p>Hadoop Common proporciona las bibliotecas y utilidades necesarias que soportan los otros m\u00f3dulos de Hadoop, asegurando la integraci\u00f3n y el funcionamiento adecuado de todo el ecosistema.</p> <ul> <li>Funciones B\u00e1sicas: Ofrece soporte para la gesti\u00f3n de configuraci\u00f3n, registro y acceso remoto.</li> <li>Soporte Multiplataforma: Compatible con diferentes sistemas operativos, lo que facilita su implementaci\u00f3n en cualquier entorno.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#5-componentes-de-integracion","title":"5. Componentes de Integraci\u00f3n \ud83d\udd0c","text":"<p>Hadoop no funciona en solitario. Se integra con varias herramientas y tecnolog\u00edas para ampliar sus capacidades y proporcionar un entorno m\u00e1s completo para la gesti\u00f3n de datos:</p> <pre><code>graph TD\n    Hadoop[Hadoop] --&gt; Spark[Spark]\n    Hadoop --&gt; Hive[Hive]\n    Hadoop --&gt; HBase[HBase]\n    Hadoop --&gt; Pig[Pig]\n    Hadoop --&gt; Sqoop[Sqoop]\n    Hadoop --&gt; Flume[Flume]</code></pre> <ul> <li>Apache Spark \u26a1: Ofrece procesamiento en memoria, lo que acelera las tareas de an\u00e1lisis en comparaci\u00f3n con MapReduce.</li> <li>Apache Hive \ud83d\udc1d: Permite consultas SQL sobre datos almacenados en HDFS, facilitando el an\u00e1lisis de datos.</li> <li>Apache HBase \ud83d\udcca: Proporciona acceso en tiempo real a grandes vol\u00famenes de datos distribuidos, ideal para aplicaciones que requieren baja latencia.</li> <li>Apache Pig \ud83d\udc37: Un lenguaje de alto nivel para escribir scripts que procesen grandes conjuntos de datos de manera m\u00e1s simple que MapReduce.</li> <li>Apache Sqoop \ud83d\udd04: Facilita la transferencia de datos entre Hadoop y bases de datos relacionales.</li> <li>Apache Flume \ud83d\udce5: Recoge, agrega y mueve grandes cantidades de datos de eventos a Hadoop.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#como-trabajan-juntos-los-componentes-de-hadoop","title":"\ud83d\udea6 \u00bfC\u00f3mo Trabajan Juntos los Componentes de Hadoop?","text":"<p>La arquitectura de Hadoop se basa en la sinergia de sus componentes. Cada m\u00f3dulo desempe\u00f1a un papel esencial en el procesamiento y almacenamiento de datos a gran escala, trabajando de manera conjunta para ofrecer un entorno completo y robusto.</p> <ol> <li> <p>Ingesti\u00f3n de Datos: Los datos se recopilan mediante herramientas como Flume o Sqoop y se almacenan en HDFS.</p> </li> <li> <p>Gesti\u00f3n de Recursos: YARN administra los recursos del cl\u00faster, asegurando que las tareas se distribuyan de manera eficiente.</p> </li> <li> <p>Procesamiento de Datos: Se realiza mediante MapReduce, Spark, Pig o Hive, dependiendo del tipo de an\u00e1lisis requerido.</p> </li> <li> <p>Acceso y An\u00e1lisis: Hive proporciona un lenguaje similar a SQL para consultar y analizar datos, mientras que HBase permite el acceso en tiempo real.</p> </li> <li> <p>Automatizaci\u00f3n de Flujos de Trabajo: Oozie coordina la ejecuci\u00f3n de trabajos y la automatizaci\u00f3n de tareas repetitivas en el cl\u00faster.</p> </li> </ol>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#ejemplo-completo-de-integracion-de-la-arquitectura-hadoop-en-javascript","title":"\ud83c\udf1f Ejemplo Completo de Integraci\u00f3n de la Arquitectura Hadoop en JavaScript","text":"<p>Para ilustrar c\u00f3mo todos estos componentes trabajan en conjunto, veamos un ejemplo pr\u00e1ctico de integraci\u00f3n utilizando JavaScript:</p> <pre><code>const hdfs = require('hdfs'); // Interacci\u00f3n con HDFS\nconst yarn = require('yarn-client'); // Gesti\u00f3n de tareas en el cl\u00faster\nconst hive = require('hive-client'); // Consultas en Hive\n\n// Conectar a HDFS\nconst hdfsClient = hdfs({\n  protocol: 'http',\n  hostname: 'localhost',\n  port: 9870\n});\n\n// Subir datos a HDFS\nhdfsClient.createFile('/user/data.txt', 'Hadoop es un sistema distribuido', (err) =&gt; {\n  if (err) {\n    console.error('Error al crear archivo en HDFS:', err);\n  } else {\n    console.log('Archivo creado en HDFS.');\n  }\n});\n\n// Ejecutar tarea con YARN\nconst yarnClient = new yarn.Client();\nyarnClient.submitJob('analyze-data', '/user/data.txt', (err) =&gt; {\n  if (err) {\n    console.error('Error ejecutando trabajo en YARN:', err);\n  } else {\n    console.log('Trabajo completado en YARN.');\n  }\n});\n\n// Consultar resultados en Hive\nconst hiveClient = hive.createClient({ host: 'localhost', port: 10000 });\n\nhiveClient.connect().then(() =&gt; {\n  hiveClient.query('SELECT * FROM logs WHERE event=\"Hadoop\";', (err, results) =&gt; {\n    if (err) {\n      console.error('Error en la consulta Hive:', err);\n    } else {\n      console.log('Resultados de la consulta Hive:', results);\n    }\n  });\n});\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#conclusion","title":"\ud83d\ude80 Conclusi\u00f3n","text":"<p>La arquitectura de Hadoop es un ejemplo brillante de c\u00f3mo los sistemas distribuidos pueden transformar la manera en que manejamos y analizamos datos masivos. Con una combinaci\u00f3n de almacenamiento robusto, gesti\u00f3n eficiente de recursos y capacidades avanzadas de procesamiento, Hadoop se ha convertido en la columna vertebral del Big Data moderno. Ya sea que est\u00e9s trabajando en an\u00e1lisis de datos, modelado predictivo o simplemente necesites un sistema escalable y resistente, la arquitectura de Hadoop proporciona las herramientas necesarias para desbloquear el verdadero potencial de tus datos. \ud83c\udf10</p>"},{"location":"ut1-introduccion-a-hadoop/indicedetareas/","title":"\u00cdndice de pr\u00e1cticas y tareas","text":"<p>Este \u00edndice incluye todas las pr\u00e1cticas guiadas y tareas correspondientes al curso. Aseg\u00farate de seguir las instrucciones cuidadosamente para cada pr\u00e1ctica y tarea, y consulta los materiales de apoyo cuando sea necesario.</p>"},{"location":"ut1-introduccion-a-hadoop/indicedetareas/#practicas-guiadas","title":"Pr\u00e1cticas guiadas","text":"<p>Las pr\u00e1cticas guiadas est\u00e1n dise\u00f1adas para que sigas un conjunto de instrucciones paso a paso y te familiarices con el entorno y las herramientas utilizadas en Big Data. Cada pr\u00e1ctica cubre un aspecto clave del entorno de Hadoop y su ecosistema.</p> <ol> <li> <p>Pr\u00e1ctica Instalaci\u00f3n Hadoop Single Node</p> <ul> <li>Descripci\u00f3n: Esta pr\u00e1ctica cubre el proceso de instalaci\u00f3n de Hadoop en un entorno de un solo nodo. El objetivo es configurar un cl\u00faster Hadoop b\u00e1sico en modo pseudo-distribuido en tu m\u00e1quina local.</li> <li>Objetivos: <ul> <li>Configurar Hadoop en un \u00fanico nodo.</li> <li>Ejecutar comandos b\u00e1sicos de HDFS.</li> <li>Verificar la instalaci\u00f3n utilizando los servicios de Hadoop.</li> </ul> </li> </ul> </li> <li> <p>Pr\u00e1ctica Inicial HDFS</p> <ul> <li>Descripci\u00f3n: Esta pr\u00e1ctica introduce el sistema de archivos distribuido de Hadoop (HDFS). Aprender\u00e1s a subir y descargar archivos en el sistema y a utilizar comandos b\u00e1sicos para manipular los datos.</li> <li>Objetivos: <ul> <li>Subir, descargar y listar archivos en HDFS.</li> <li>Utilizar comandos esenciales como <code>hdfs dfs -put</code>, <code>hdfs dfs -get</code>, y <code>hdfs dfs -ls</code>.</li> <li>Verificar la replicaci\u00f3n de los bloques y la ubicaci\u00f3n de los archivos en HDFS.</li> </ul> </li> </ul> </li> </ol>"},{"location":"ut1-introduccion-a-hadoop/indicedetareas/#tareas","title":"Tareas","text":"<p>Las tareas son ejercicios que complementan las pr\u00e1cticas guiadas y tienen como objetivo reforzar los conocimientos adquiridos. Algunas tareas requerir\u00e1n investigaci\u00f3n adicional y la implementaci\u00f3n de soluciones m\u00e1s avanzadas.</p> <ol> <li>Introducci\u00f3n al Big Data<ul> <li>Descripci\u00f3n: Esta tarea introductoria cubre los conceptos clave de Big Data, sus desaf\u00edos, y las tecnolog\u00edas fundamentales en el ecosistema Hadoop.</li> <li>Objetivos:<ul> <li>Definir Big Data y sus caracter\u00edsticas (volumen, velocidad, variedad, etc.).</li> <li>Explicar la importancia de Hadoop en el procesamiento de grandes vol\u00famenes de datos.</li> <li>Realizar una breve investigaci\u00f3n sobre casos de uso de Big Data en la industria.</li> </ul> </li> </ul> </li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/1tareaintroductoria/","title":"Tarea Introducci\u00f3n al Big Data","text":"<ol> <li> <p>Selecciona una Empresa o Tem\u00e1tica:</p> <ul> <li>Elige una empresa real (por ejemplo, Amazon, Facebook, Spotify, etc.) o una tem\u00e1tica espec\u00edfica (por ejemplo, an\u00e1lisis de datos m\u00e9dicos, optimizaci\u00f3n de rutas log\u00edsticas, etc.) que recoja y maneje grandes vol\u00famenes de datos.</li> <li>Explica brevemente c\u00f3mo esa empresa o tem\u00e1tica seleccionada obtiene datos.</li> </ul> </li> <li> <p>Describe Cada Fase del Procesamiento de Datos:</p> <ul> <li> <p>Recolecci\u00f3n de Datos \ud83d\udce5: Explica de d\u00f3nde y c\u00f3mo tu empresa seleccionada recolecta los datos (por ejemplo, redes sociales, sensores, historiales de compra, etc.).</p> </li> <li> <p>Recopilaci\u00f3n: Describe c\u00f3mo los datos recolectados se organizan en un sistema centralizado o base de datos para su posterior an\u00e1lisis.</p> </li> <li> <p>Preprocesamiento o Limpieza de Datos \ud83e\uddf9: Explica c\u00f3mo la empresa limpia los datos (eliminaci\u00f3n de datos duplicados, correcci\u00f3n de errores, conversi\u00f3n de formatos) antes de analizarlos.</p> </li> <li> <p>Procesamiento \ud83d\udda5\ufe0f: Detalla c\u00f3mo se procesan los datos (algoritmos, machine learning, modelos predictivos) para extraer informaci\u00f3n \u00fatil.</p> </li> <li> <p>Interpretaci\u00f3n y Visualizaci\u00f3n \ud83d\udcca: Describe c\u00f3mo se presentan los resultados a los usuarios o tomadores de decisiones (dashboards, gr\u00e1ficos, informes).</p> </li> <li> <p>An\u00e1lisis \ud83e\udde0: Comenta c\u00f3mo la empresa utiliza los resultados del an\u00e1lisis para tomar decisiones estrat\u00e9gicas o mejorar sus operaciones.</p> </li> <li> <p>Almacenamiento: Finalmente, explica c\u00f3mo y d\u00f3nde se almacenan los datos procesados y los resultados para su uso futuro (en la nube, bases de datos, sistemas locales).</p> </li> </ul> </li> <li> <p>Comenta investigando al menos 3Vs m\u00e1s de las mencionadas</p> </li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/","title":"Pr\u00e1ctica Guiada: Implementaci\u00f3n de un Nodo Hadoop con Docker","text":""},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/#1-introduccion","title":"1. Introducci\u00f3n","text":"<p>En esta pr\u00e1ctica, vas a configurar un cl\u00faster Hadoop de un solo nodo utilizando Docker. Hadoop es un framework que permite el procesamiento de grandes cantidades de datos distribuidos en varios nodos. Para fines educativos, configuraremos un cl\u00faster de un solo nodo. Docker facilitar\u00e1 el proceso de instalaci\u00f3n, ya que encapsula todo en un contenedor aislado del sistema operativo principal.</p>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/#2-requisitos-previos","title":"2. Requisitos Previos","text":"<ul> <li>Docker instalado: Aseg\u00farate de tener Docker instalado y funcionando en tu m\u00e1quina. Si no lo tienes instalado, puedes descargarlo desde https://www.docker.com/products/docker-desktop.</li> <li>Git instalado: Se necesita Git para clonar el repositorio de configuraci\u00f3n.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/#3-clonar-el-repositorio-del-proyecto","title":"3. Clonar el repositorio del proyecto","text":"<p>Para empezar, vamos a clonar el repositorio donde ya est\u00e1 configurada la imagen de Hadoop. Este repositorio contiene un archivo <code>Dockerfile</code>, que es un script que define c\u00f3mo se debe crear la imagen de Docker con todos los servicios y configuraciones de Hadoop.</p> <pre><code>git clone https://github.com/rancavil/hadoop-single-node-cluster.git\ncd hadoop-single-node-cluster\n</code></pre> <ul> <li>\u00bfQu\u00e9 es esto?: El comando <code>git clone</code> descarga una copia local del repositorio alojado en GitHub, y <code>cd</code> te lleva al directorio reci\u00e9n clonado.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/#4-construir-la-imagen-docker-de-hadoop","title":"4. Construir la imagen Docker de Hadoop","text":"<p>Ahora que tenemos los archivos de configuraci\u00f3n, es momento de construir una imagen Docker. Esta imagen contendr\u00e1 todo lo necesario para correr Hadoop, incluyendo Java, Hadoop y las configuraciones necesarias para un nodo \u00fanico.</p> <pre><code>docker build -t hadoop .\n</code></pre> <ul> <li>Explicaci\u00f3n: <ul> <li><code>docker build</code> es el comando que construye una imagen a partir de un archivo <code>Dockerfile</code>.</li> <li><code>-t hadoop</code> asigna la etiqueta \"hadoop\" a esta imagen, lo que nos facilitar\u00e1 su referencia m\u00e1s adelante.</li> <li>El <code>.</code> indica que el contexto para la construcci\u00f3n es el directorio actual.</li> </ul> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/#5-crear-y-correr-el-contenedor-hadoop","title":"5. Crear y correr el contenedor Hadoop","text":"<p>Una vez que la imagen est\u00e1 construida, vamos a ejecutar un contenedor basado en ella. Un contenedor es una instancia de la imagen, ejecut\u00e1ndose como si fuera una peque\u00f1a m\u00e1quina virtual, pero m\u00e1s ligera y eficiente.</p> <pre><code>docker run --name hadoop-container -p 9864:9864 -p 9870:9870 -p 8088:8088 -p 9000:9000 --hostname hadoop hadoop\n</code></pre> <ul> <li>Explicaci\u00f3n:<ul> <li><code>docker run</code>: Este comando crea y corre un contenedor.</li> <li><code>--name hadoop-container</code>: Asigna el nombre \"hadoop-container\" al contenedor.</li> <li><code>-p 9864:9864 -p 9870:9870 -p 8088:8088 -p 9000:9000</code>: Mapea los puertos del contenedor a tu m\u00e1quina local. Esto te permite acceder a la interfaz web de Hadoop desde tu navegador y establecer la conexi\u00f3n entre el contenedor y tu m\u00e1quina para el uso de HDFS.<ul> <li><code>9864</code>: Puerto del DataNode.</li> <li><code>9870</code>: Puerto del NameNode (UI para monitoreo de HDFS).</li> <li><code>8088</code>: Puerto del ResourceManager (UI para monitoreo de trabajos en YARN).</li> <li><code>9000</code>: Puerto de HDFS para el NameNode.</li> </ul> </li> <li><code>--hostname hadoop</code>: Asigna el nombre de host \"hadoop\" al contenedor.</li> </ul> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/#6-abrir-una-terminal-dentro-del-contenedor","title":"6. Abrir una terminal dentro del contenedor","text":"<p>Para interactuar con Hadoop dentro del contenedor, necesitas abrir una terminal en su interior. Hay dos formas de hacerlo:</p> <ul> <li> <p>Modo gr\u00e1fico: Si est\u00e1s usando Docker Desktop, puedes acceder al contenedor desde la interfaz gr\u00e1fica seleccionando el contenedor \"hadoop-container\" y abriendo una terminal desde ah\u00ed.</p> </li> <li> <p>Modo terminal: Si prefieres usar la terminal, ejecuta el siguiente comando:</p> </li> </ul> <pre><code>docker exec -i -t hadoop-container /bin/bash\n</code></pre> <ul> <li>Explicaci\u00f3n:<ul> <li><code>docker exec</code>: Permite ejecutar comandos dentro de un contenedor en ejecuci\u00f3n.</li> <li><code>-i -t</code>: Estas opciones te permiten interactuar de manera interactiva con el contenedor (modo terminal).</li> <li><code>hadoop-container</code>: Es el nombre del contenedor que ejecutaste anteriormente.</li> <li><code>/bin/bash</code>: Especifica que quieres abrir una shell Bash dentro del contenedor.</li> </ul> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/#7-probar-hadoop-y-hdfs","title":"7. Probar Hadoop y HDFS","text":"<p>Ahora que est\u00e1s dentro del contenedor, es momento de probar que Hadoop y su sistema de archivos HDFS est\u00e1n funcionando correctamente. Comienza creando un directorio en HDFS.</p> <pre><code>hdfs dfs -mkdir /user\n</code></pre> <p>Luego, lista los archivos del sistema de archivos HDFS recursivamente:</p> <pre><code>hdfs dfs -ls -R /\n</code></pre> <ul> <li>Explicaci\u00f3n:<ul> <li><code>hdfs dfs</code>: Este es el comando que utilizas para interactuar con el sistema de archivos distribuido de Hadoop (HDFS).</li> <li><code>-mkdir /user</code>: Crea un directorio llamado \"user\" en el sistema de archivos HDFS.</li> <li><code>-ls -R /</code>: Lista de manera recursiva todos los archivos en HDFS, comenzando desde la ra\u00edz.</li> </ul> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/#8-acceder-a-la-interfaz-web-de-hadoop","title":"8. Acceder a la interfaz web de Hadoop","text":"<p>Puedes acceder a las interfaces de monitoreo de Hadoop desde tu navegador utilizando las siguientes direcciones:</p> <ul> <li>NameNode (HDFS): http://localhost:9870</li> <li>ResourceManager (YARN): http://localhost:8088</li> </ul> <p>Estas interfaces te permiten ver el estado del cl\u00faster, los nodos disponibles, y los trabajos que se est\u00e1n ejecutando.</p>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/#9-apagar-el-contenedor","title":"9. Apagar el contenedor","text":"<p>Cuando termines de trabajar, puedes detener y eliminar el contenedor. Para detener el contenedor sin eliminarlo, ejecuta:</p> <pre><code>docker stop hadoop-container\n</code></pre> <p>Si deseas eliminar el contenedor completamente:</p> <pre><code>docker rm hadoop-container\n</code></pre> <ul> <li>Explicaci\u00f3n:<ul> <li><code>docker stop</code>: Detiene el contenedor en ejecuci\u00f3n.</li> <li><code>docker rm</code>: Elimina el contenedor de Docker, liberando espacio.</li> </ul> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/#10-conclusion","title":"10. Conclusi\u00f3n","text":"<p>Has completado la pr\u00e1ctica para configurar un cl\u00faster Hadoop de un solo nodo usando Docker. Este entorno te permite experimentar con las funcionalidades principales de Hadoop sin necesidad de una configuraci\u00f3n compleja de varios nodos o sistemas distribuidos. Con este entorno, puedes practicar la gesti\u00f3n de archivos en HDFS, ejecutar trabajos en YARN y familiarizarte con las interfaces de monitoreo de Hadoop.</p>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/1practicainicial/","title":"Pr\u00e1ctica: Gesti\u00f3n de archivos en HDFS en un escenario real","text":""},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/1practicainicial/#contexto-del-caso","title":"Contexto del Caso","text":"<p>Imagina que eres parte del equipo de an\u00e1lisis de datos de una empresa de telecomunicaciones. Tu tarea consiste en gestionar el almacenamiento de grandes vol\u00famenes de registros de llamadas telef\u00f3nicas. Estos registros se almacenar\u00e1n en HDFS para un posterior an\u00e1lisis distribuido. A lo largo de esta pr\u00e1ctica, simular\u00e1s la subida, consulta y gesti\u00f3n de archivos en HDFS utilizando los registros de llamadas de distintos meses.</p>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/1practicainicial/#entregable","title":"Entregable","text":"<ul> <li>Un documento PDF donde se muestren los diferentes pasos a seguir indicados a continuaci\u00f3n, incluyendo capturas y algo de explicaci\u00f3n.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/1practicainicial/#pasos-a-seguir","title":"Pasos a Seguir","text":""},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/1practicainicial/#1-preparacion-de-archivos-locales","title":"1. Preparaci\u00f3n de archivos locales","text":"<ul> <li>Simula dos archivos de texto que contengan los registros de llamadas:<ul> <li><code>llamadas_enero.txt</code>: Este archivo contiene las llamadas realizadas en enero.</li> <li><code>llamadas_febrero.txt</code>: Contiene los registros de las llamadas de febrero.</li> </ul> </li> </ul> <p>Ejemplo de contenido de cada archivo: <pre><code>ID_Llamada, Fecha, Duraci\u00f3n, N\u00famero_Destino\n001, 2024-01-05 14:30:00, 5 minutos, +34987654321\n002, 2024-01-06 16:45:00, 10 minutos, +34123456789\n</code></pre></p>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/1practicainicial/#2-subir-archivos-a-hdfs","title":"2. Subir archivos a HDFS","text":"<ul> <li> <p>Crea un directorio en HDFS donde se almacenar\u00e1n estos archivos:      <pre><code>hdfs dfs -mkdir /user/empresa_telecom/registros_llamadas\n</code></pre></p> </li> <li> <p>Sube los archivos locales <code>llamadas_enero.txt</code> y <code>llamadas_febrero.txt</code> a HDFS usando el comando <code>copyFromLocal</code>:      <pre><code>hdfs dfs -copyFromLocal /ruta/local/llamadas_enero.txt /user/empresa_telecom/registros_llamadas/\nhdfs dfs -copyFromLocal /ruta/local/llamadas_febrero.txt /user/empresa_telecom/registros_llamadas/\n</code></pre></p> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/1practicainicial/#3-verificacion-de-los-archivos-subidos","title":"3. Verificaci\u00f3n de los archivos subidos","text":"<ul> <li> <p>Lista los contenidos del directorio en HDFS para confirmar que los archivos se han subido correctamente:      <pre><code>hdfs dfs -ls /user/empresa_telecom/registros_llamadas\n</code></pre></p> </li> <li> <p>Muestra el contenido de los archivos para verificar su integridad:      <pre><code>hdfs dfs -cat /user/empresa_telecom/registros_llamadas/llamadas_enero.txt\nhdfs dfs -cat /user/empresa_telecom/registros_llamadas/llamadas_febrero.txt\n</code></pre></p> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/1practicainicial/#4-renombrar-los-archivos-para-una-mejor-organizacion","title":"4. Renombrar los archivos para una mejor organizaci\u00f3n","text":"<ul> <li>Sup\u00f3n que la empresa ha decidido que los archivos deben seguir un formato m\u00e1s organizado con el mes en min\u00fasculas. Renombra los archivos:      <pre><code>hdfs dfs -mv /user/empresa_telecom/registros_llamadas/llamadas_enero.txt /user/empresa_telecom/registros_llamadas/llamadas_enero2024.txt\nhdfs dfs -mv /user/empresa_telecom/registros_llamadas/llamadas_febrero.txt /user/empresa_telecom/registros_llamadas/llamadas_febrero2024.txt\n</code></pre></li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/1practicainicial/#5-descargar-archivos-desde-hdfs-para-analisis-local","title":"5. Descargar archivos desde HDFS para an\u00e1lisis local","text":"<ul> <li> <p>Descarga los archivos de registros desde HDFS a tu sistema local para analizarlos utilizando una herramienta de an\u00e1lisis de datos:      <pre><code>hdfs dfs -get /user/empresa_telecom/registros_llamadas/llamadas_enero2024.txt /ruta/local/analisis/\nhdfs dfs -get /user/empresa_telecom/registros_llamadas/llamadas_febrero2024.txt /ruta/local/analisis/\n</code></pre></p> </li> <li> <p>Verifica que los archivos descargados sean los mismos que los originales comparando su contenido.</p> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/1practicainicial/#6-eliminacion-de-archivos-antiguos-de-hdfs","title":"6. Eliminaci\u00f3n de archivos antiguos de HDFS","text":"<ul> <li> <p>Dado que los archivos han sido descargados y procesados, la empresa decide eliminar los registros de enero del sistema HDFS para liberar espacio. Utiliza el comando <code>rm</code> para eliminar el archivo:      <pre><code>hdfs dfs -rm /user/empresa_telecom/registros_llamadas/llamadas_enero2024.txt\n</code></pre></p> </li> <li> <p>Verifica que el archivo ha sido eliminado correctamente listando de nuevo los contenidos del directorio:      <pre><code>hdfs dfs -ls /user/empresa_telecom/registros_llamadas\n</code></pre></p> </li> <li> <p>Si ya no necesitas el directorio donde se almacenaron los registros de llamadas, elim\u00ednalo tambi\u00e9n:      <pre><code>hdfs dfs -rmdir /user/empresa_telecom/registros_llamadas\n</code></pre></p> </li> </ul>"}]}