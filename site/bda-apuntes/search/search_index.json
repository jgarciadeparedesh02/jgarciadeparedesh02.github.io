{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83c\udf89 \u00a1Bienvenidos a Big Data Aplicado! \ud83c\udf1f","text":"<p>\ud83d\ude80 Especializaci\u00f3n en Inteligencia Artificial y Big Data</p> <p>\u00a1Hola a todos! \ud83d\udc4b Me complace darles la bienvenida al m\u00f3dulo de Big Data Aplicado dentro del curso de especializaci\u00f3n en Inteligencia Artificial y Big Data. En este m\u00f3dulo, nos adentraremos en el fascinante mundo del Big Data y aprenderemos a usar herramientas clave como Apache Hadoop para manejar y analizar grandes vol\u00famenes de datos. \ud83c\udf10\ud83d\udcca</p> <p></p>"},{"location":"#que-aprenderemos","title":"\ud83c\udfaf \u00bfQu\u00e9 aprenderemos?","text":"<p>En este curso, exploraremos c\u00f3mo almacenar, procesar y analizar grandes cantidades de informaci\u00f3n utilizando Hadoop y su extenso ecosistema. Estas tecnolog\u00edas est\u00e1n revolucionando industrias, permitiendo que las empresas aprovechen al m\u00e1ximo sus datos. \u00a1Prep\u00e1rate para convertirte en un experto en Big Data! \ud83d\ude80</p> <p>\ud83d\udcda \u00cdndice de Contenidos A lo largo de este m\u00f3dulo, profundizaremos en varias \u00e1reas clave del Big Data, desde conceptos introductorios hasta aplicaciones pr\u00e1cticas. Aqu\u00ed te dejamos una visi\u00f3n general de los contenidos:</p>"},{"location":"#unidad-1-introduccion-a-apache-hadoop","title":"Unidad 1: Introducci\u00f3n a Apache Hadoop","text":"<p>Comenzamos por lo b\u00e1sico: \u00bfQu\u00e9 es Hadoop y por qu\u00e9 es tan importante para el procesamiento de grandes datos?</p> <ul> <li>Motivaci\u00f3n y origen: Exploramos los problemas que dieron origen a Hadoop y c\u00f3mo se convirti\u00f3 en una soluci\u00f3n para el manejo de grandes vol\u00famenes de datos.</li> <li>Apache Hadoop a alto nivel: Nos familiarizamos con los conceptos clave de Hadoop y sus componentes principales.</li> <li>\u00bfQu\u00e9 es Apache Hadoop? Conoce qu\u00e9 es y por qu\u00e9 es crucial para el almacenamiento y procesamiento de grandes cantidades de datos.</li> <li>Ecosistema Hadoop y distribuciones: Descubre c\u00f3mo Hadoop es solo el n\u00facleo de un ecosistema m\u00e1s amplio, que incluye herramientas como Hive, Pig, y HBase.</li> <li>Arquitectura de Hadoop: Examinamos c\u00f3mo Hadoop organiza el procesamiento distribuido a trav\u00e9s de HDFS (Hadoop Distributed File System) y MapReduce.</li> <li>Beneficios, desventajas y dificultades: Analizamos las ventajas que ofrece Hadoop, pero tambi\u00e9n sus limitaciones y retos.</li> </ul>"},{"location":"#unidad-2-almacenamiento-y-procesamiento-en-hadoop","title":"Unidad 2: Almacenamiento y Procesamiento en Hadoop","text":"<p>Profundizamos en c\u00f3mo Hadoop almacena y procesa datos utilizando HDFS, YARN y MapReduce.</p> <ul> <li>HDFS (Hadoop Distributed File System): Es el sistema de archivos distribuido que permite almacenar grandes vol\u00famenes de datos de manera eficiente. Explicaremos c\u00f3mo funciona la lectura y escritura de datos en HDFS.</li> <li>Arquitectura de HDFS: Conoceremos la estructura interna de HDFS y c\u00f3mo los NameNodes y DataNodes se comunican para mantener la integridad de los datos.</li> <li>YARN (Yet Another Resource Negotiator): Esta herramienta gestiona los recursos dentro del cl\u00faster de Hadoop, permitiendo ejecutar m\u00faltiples tareas en paralelo.</li> <li>MapReduce: Modelo de programaci\u00f3n distribuida que divide las tareas en \"Map\" y \"Reduce\". Veremos c\u00f3mo se utiliza para procesar grandes cantidades de datos de manera eficiente y escalable.</li> </ul>"},{"location":"#unidad-3-ecosistema-hadoop","title":"Unidad 3: Ecosistema Hadoop","text":"<p>El ecosistema Hadoop es vasto y est\u00e1 lleno de herramientas dise\u00f1adas para facilitar el procesamiento y an\u00e1lisis de datos.</p> <ul> <li>Apache Pig: Un lenguaje de scripting dise\u00f1ado para analizar grandes conjuntos de datos de manera simple y eficiente.</li> <li>Apache Hive: Un sistema de data warehousing que permite realizar consultas sobre datos almacenados en Hadoop mediante un lenguaje similar a SQL, llamado HQL.</li> <li>Apache Impala: Herramienta de an\u00e1lisis que permite realizar consultas en tiempo real sobre grandes vol\u00famenes de datos.</li> <li>Apache HBase: Una base de datos NoSQL que permite el almacenamiento y acceso a datos estructurados de manera distribuida.</li> <li>Apache Spark: Un motor de procesamiento r\u00e1pido y general para grandes vol\u00famenes de datos, ideal para aplicaciones en tiempo real.</li> <li>Componentes de Ingesta de Datos: Exploraremos herramientas como Apache Sqoop y Apache Flume, dise\u00f1adas para la ingesta eficiente de datos desde fuentes externas a Hadoop.</li> <li>Apache Oozie: Una herramienta de flujo de trabajo que permite coordinar y programar trabajos dentro del ecosistema Hadoop.</li> <li>Procesamiento en Streaming: Veremos c\u00f3mo tecnolog\u00edas como Apache Spark (Structured Streaming), Apache Flink, y Apache Storm manejan datos en tiempo real, permitiendo an\u00e1lisis continuos.</li> </ul>"},{"location":"#unidad-4-administracion-y-monitorizacion-de-sistemas-hadoop","title":"Unidad 4: Administraci\u00f3n y Monitorizaci\u00f3n de Sistemas Hadoop","text":"<p>Gestionar un cl\u00faster Hadoop es crucial para asegurar que todo funcione correctamente, y en esta unidad aprenderemos las mejores pr\u00e1cticas y herramientas.</p> <ul> <li>Interfaz de HDFS y YARN: Exploraremos las interfaces de usuario que permiten monitorizar el funcionamiento del sistema de archivos y la gesti\u00f3n de recursos.</li> <li>Apache Ambari y Cloudera Manager: Dos herramientas poderosas que facilitan la administraci\u00f3n de cl\u00fasteres Hadoop.</li> <li>Ganglia: Un sistema de monitorizaci\u00f3n dise\u00f1ado para analizar el rendimiento y la eficiencia de un cl\u00faster Hadoop en tiempo real.</li> </ul>"},{"location":"#unidad-5-aplicacion-practica-de-tecnologias-big-data","title":"Unidad 5: Aplicaci\u00f3n Pr\u00e1ctica de Tecnolog\u00edas Big Data","text":"<p>Finalmente, aplicaremos los conocimientos adquiridos para resolver problemas reales utilizando tecnolog\u00edas Big Data.</p> <ul> <li>Arquitecturas y Modelos de Despliegue: Exploraremos diferentes arquitecturas de sistemas Big Data y sus modelos de despliegue, incluyendo soluciones on-premise y en la nube.</li> <li>Hadoop en la Pr\u00e1ctica: Veremos ejemplos de implementaci\u00f3n pr\u00e1ctica de Hadoop en diversos sectores y casos de uso.</li> <li>Soluciones Hadoop-as-a-Service: Exploraremos plataformas como Amazon EMR y Microsoft Azure HDInsight, que permiten implementar y gestionar soluciones Hadoop en la nube.</li> </ul>"},{"location":"#a-por-ello","title":"\u00a1A por ello!","text":"<p>Este curso te proporcionar\u00e1 las habilidades y conocimientos necesarios para dise\u00f1ar, implementar y administrar sistemas de Big Data. Al finalizar, ser\u00e1s capaz de gestionar grandes vol\u00famenes de datos de manera eficiente, desde el almacenamiento hasta el procesamiento avanzado. \ud83c\udfc6</p> <p>\ud83d\udca1 Consejo del d\u00eda: El mundo del Big Data est\u00e1 lleno de oportunidades. No dudes en preguntar, explorar y experimentar a lo largo del curso. \u00a1Estamos aqu\u00ed para ayudarte a cada paso del camino!</p> <p>\ud83d\udd25 \u00a1Vamos a dominar el Big Data, un cl\u00faster a la vez! \ud83d\udcaa Espero que est\u00e9s tan emocionado como yo para comenzar este viaje en el mundo del Big Data. \u00a1Es hora de hacer que los datos trabajen para nosotros! \ud83c\udf0d\ud83c\udf93</p>"},{"location":"docker/","title":"Introducci\u00f3n a Docker y Docker-compose \ud83d\udea2","text":""},{"location":"docker/#que-es-docker","title":"\ud83d\ude80 \u00bfQu\u00e9 es Docker?","text":"<p>Docker es una plataforma dise\u00f1ada para desarrollar, desplegar y ejecutar aplicaciones en contenedores. Un contenedor es un paquete liviano, port\u00e1til y aut\u00f3nomo que incluye todo lo necesario para ejecutar una aplicaci\u00f3n: c\u00f3digo, runtime, bibliotecas y configuraciones. La principal ventaja de Docker es que permite garantizar que la aplicaci\u00f3n se ejecutar\u00e1 de la misma manera en cualquier entorno, ya sea tu m\u00e1quina local o un servidor en la nube.</p> <pre><code># Ejemplo de comando para ejecutar un contenedor Docker:\ndocker run -d -p 8080:80 --name mi_contenedor nginx\n</code></pre> <p>En el ejemplo anterior:</p> <ul> <li><code>-d</code> indica que el contenedor se ejecutar\u00e1 en segundo plano (modo detached).</li> <li><code>-p 8080:80</code> enlaza el puerto 8080 de tu m\u00e1quina local al puerto 80 del contenedor.</li> <li><code>--name mi_contenedor</code> le asigna un nombre al contenedor.</li> <li><code>nginx</code> es la imagen que estamos usando para crear el contenedor.</li> </ul>"},{"location":"docker/#componentes-clave-de-docker","title":"\ud83d\udee0 Componentes clave de Docker","text":"<ol> <li>Imagen: Es una plantilla de solo lectura que se utiliza para crear contenedores. Piensa en una imagen como una instant\u00e1nea de un sistema operativo m\u00e1s una aplicaci\u00f3n.</li> </ol> <p>Ejemplo: La imagen <code>nginx</code> es una plantilla que contiene el servidor web NGINX.</p> <ol> <li> <p>Contenedor: Es una instancia ejecutable de una imagen. Cada contenedor es aut\u00f3nomo y aislado.</p> </li> <li> <p>Dockerfile: Es un archivo de texto que contiene todas las instrucciones necesarias para construir una imagen. </p> </li> </ol> <p><pre><code># Ejemplo de un Dockerfile simple:\nFROM node:14\nWORKDIR /app\nCOPY . .\nRUN npm install\nCMD [\"npm\", \"start\"]\n</code></pre> El Dockerfile anterior:</p> <ul> <li>Usa la imagen base <code>node:14</code>.</li> <li>Copia el contenido del directorio actual en el contenedor.</li> <li>Instala las dependencias de Node.js.</li> <li> <p>Ejecuta el comando <code>npm start</code> para iniciar la aplicaci\u00f3n.</p> </li> <li> <p>Docker Hub: Es un registro p\u00fablico donde puedes encontrar im\u00e1genes creadas por otros usuarios o subir las tuyas propias.</p> </li> </ul>"},{"location":"docker/#que-es-docker-compose","title":"\ud83d\udce6 \u00bfQu\u00e9 es Docker-compose?","text":"<p>Docker-compose es una herramienta que te permite definir y ejecutar aplicaciones multi-contenedor. En lugar de ejecutar m\u00faltiples contenedores de forma manual, puedes utilizar un archivo <code>docker-compose.yml</code> para definir c\u00f3mo se deben configurar y ejecutar.</p> <pre><code># Ejemplo b\u00e1sico de un archivo docker-compose.yml:\nversion: '3'\nservices:\n  web:\n    image: nginx\n    ports:\n      - \"8080:80\"\n  db:\n    image: mysql\n    environment:\n      MYSQL_ROOT_PASSWORD: example\n</code></pre> <p>Este archivo YAML define dos servicios:</p> <ul> <li>web: que usa la imagen <code>nginx</code> y mapea el puerto 80 del contenedor al 8080 de la m\u00e1quina local.</li> <li>db: que usa la imagen <code>mysql</code> y define una contrase\u00f1a para el usuario root.</li> </ul> <p>Para iniciar los contenedores definidos en el archivo <code>docker-compose.yml</code>, simplemente ejecutas:</p> <pre><code>docker-compose up\n</code></pre>"},{"location":"docker/#ventajas-de-docker-y-docker-compose","title":"\ud83c\udf00 Ventajas de Docker y Docker-compose","text":"<ol> <li>Portabilidad: Los contenedores se ejecutan de la misma manera en cualquier entorno.</li> <li>Aislamiento: Cada contenedor es independiente, evitando conflictos entre aplicaciones.</li> <li>Escalabilidad: Docker-compose facilita la creaci\u00f3n de aplicaciones complejas al permitir la orquestaci\u00f3n de m\u00faltiples contenedores.</li> <li>Reproducibilidad: Con un <code>Dockerfile</code> y <code>docker-compose</code>, puedes recrear un entorno exacto f\u00e1cilmente.</li> </ol>"},{"location":"docker/#diferencia-entre-imagenes-y-contenedores-en-docker","title":"\ud83d\uddbc\ufe0f Diferencia entre Im\u00e1genes y Contenedores en Docker \ud83d\udea2","text":"<p>Una de las confusiones m\u00e1s comunes al empezar con Docker es la diferencia entre im\u00e1genes y contenedores. Vamos a aclarar esto:</p>"},{"location":"docker/#imagenes","title":"Im\u00e1genes \ud83d\udcf8","text":"<p>Una imagen en Docker es como una plantilla de solo lectura que contiene todo lo necesario para ejecutar una aplicaci\u00f3n, incluyendo:</p> <ul> <li>Sistema operativo (por ejemplo, Ubuntu, Alpine).</li> <li>C\u00f3digo de la aplicaci\u00f3n.</li> <li>Dependencias o librer\u00edas necesarias para que la aplicaci\u00f3n funcione.</li> <li>Configuraciones o variables de entorno.</li> </ul> <p>Las im\u00e1genes son est\u00e1ticas, lo que significa que no cambian una vez que se crean. Son utilizadas para generar contenedores, pero por s\u00ed solas no hacen nada. </p> <p>Piensa en una imagen como una receta o blueprint que no se ejecuta, pero que contiene todas las instrucciones necesarias para crear algo que s\u00ed lo har\u00e1.</p> <pre><code># Ejemplo: Listar las im\u00e1genes disponibles en tu sistema\ndocker images\n</code></pre> <p>Salida t\u00edpica:</p> <pre><code>REPOSITORY       TAG       IMAGE ID       CREATED        SIZE\nnginx            latest    d1a364dc548d   2 weeks ago    133MB\nmysql            5.7       2a3174d2e2f7   1 month ago    450MB\n</code></pre> <p>En el ejemplo anterior:</p> <ul> <li>La imagen <code>nginx</code> est\u00e1 lista para ser utilizada para crear un servidor web.</li> <li>La imagen <code>mysql</code> contiene la base de datos MySQL en su versi\u00f3n 5.7.</li> </ul>"},{"location":"docker/#contenedores","title":"Contenedores \ud83d\udef3\ufe0f","text":"<p>Un contenedor es una instancia en ejecuci\u00f3n de una imagen. Es como si tomaras la plantilla (imagen) y la ejecutaras para crear un entorno real donde tu aplicaci\u00f3n corre. Un contenedor:</p> <ul> <li>Est\u00e1 basado en una imagen.</li> <li>Es din\u00e1mico: puede ejecutar procesos, guardar datos, recibir tr\u00e1fico de red, etc.</li> <li>Puede ser creado, iniciado, detenido y destruido.</li> </ul> <p>Cada contenedor tiene su propio sistema de archivos y entorno de ejecuci\u00f3n aislado del resto. Esto significa que puedes tener m\u00faltiples contenedores ejecutando la misma imagen sin que interfieran entre s\u00ed.</p> <pre><code># Ejemplo: Listar los contenedores en ejecuci\u00f3n\ndocker ps\n</code></pre> <p>Salida t\u00edpica:</p> <pre><code>CONTAINER ID   IMAGE     COMMAND                  CREATED         STATUS         PORTS                  NAMES\na1b2c3d4e5f6   nginx     \"nginx -g 'daemon off\u2026   5 minutes ago   Up 5 minutes   0.0.0.0:8080-&gt;80/tcp   webserver\n</code></pre> <p>En este caso, el contenedor <code>webserver</code> fue creado a partir de la imagen <code>nginx</code> y est\u00e1 en ejecuci\u00f3n mapeando el puerto 80 al 8080 de la m\u00e1quina local.</p>"},{"location":"docker/#diferencia-esencial","title":"\ud83d\udcca Diferencia Esencial","text":"Im\u00e1genes Contenedores Son plantillas est\u00e1ticas. Son instancias en ejecuci\u00f3n basadas en una imagen. No pueden modificarse una vez creadas. Pueden cambiar mientras est\u00e1n en ejecuci\u00f3n (p.ej. archivos creados dentro del contenedor). Se almacenan localmente o en repositorios como Docker Hub. Pueden ser creados, iniciados, detenidos y destruidos. Son inmutables (no cambian). Son din\u00e1micos y pueden ejecutar procesos."},{"location":"docker/#metafora-restaurante-y-recetas","title":"\ud83e\uddd1\u200d\ud83c\udf73 Met\u00e1fora: Restaurante y Recetas","text":"<ul> <li>Imagen: Es como una receta. Tienes todos los ingredientes y pasos necesarios para preparar un plato, pero no puedes comer la receta.</li> <li>Contenedor: Es el plato servido en la mesa. Ya se ha preparado a partir de la receta (imagen) y est\u00e1 listo para que lo disfrutes (o en nuestro caso, para que la aplicaci\u00f3n se ejecute).</li> </ul> <p>\ud83d\udca1 Conclusi\u00f3n:</p> <ul> <li>Imagen = Plantilla (Receta).</li> <li>Contenedor = Ejecuci\u00f3n de la Imagen (Plato Listo).</li> </ul> <p>Las im\u00e1genes son el punto de partida, pero son los contenedores los que realmente ejecutan y manejan tu aplicaci\u00f3n. Cada vez que usas Docker para ejecutar algo, est\u00e1s creando uno o m\u00e1s contenedores basados en im\u00e1genes preexistentes.</p>"},{"location":"docker/#conclusion","title":"\ud83c\udfaf Conclusi\u00f3n","text":"<p>Docker y Docker-compose son herramientas poderosas que simplifican el desarrollo y despliegue de aplicaciones en diferentes entornos. No importa si eres un desarrollador que quiere probar su aplicaci\u00f3n localmente o si administras un servidor en producci\u00f3n, estas herramientas te ayudar\u00e1n a crear entornos confiables y escalables.</p> <p>\ud83d\udca1 Consejo: Comienza por crear un peque\u00f1o contenedor con Docker y luego explora c\u00f3mo usar Docker-compose para manejar aplicaciones m\u00e1s complejas con m\u00faltiples servicios.</p>"},{"location":"tarea-final/","title":"Tarea: An\u00e1lisis de Datos de Taxis en Nueva York con PySpark","text":""},{"location":"tarea-final/#objetivo","title":"Objetivo","text":"<p>El objetivo de esta tarea es que te familiarices con el uso de Apache Spark y PySpark para procesar y analizar datos reales. Trabajar\u00e1s con un conjunto de datos que contiene informaci\u00f3n de viajes en taxi en Nueva York, realizando distintos an\u00e1lisis y visualizando los resultados por consola.</p>"},{"location":"tarea-final/#materiales-proporcionados","title":"Materiales proporcionados","text":""},{"location":"tarea-final/#archivo-template-base-para-tu-script","title":"Archivo template (base para tu script)","text":"<pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, count, avg, max, min, desc, year, month, to_timestamp\n\n# Crear sesi\u00f3n de Spark\nspark = SparkSession.builder \\\n    .appName(\"TaxiAnalysis\") \\\n    .master(\"local[*]\") \\\n    .getOrCreate()\n\n# Leer el CSV (aseg\u00farate de que la ruta al archivo sea v\u00e1lida)\ndf = spark.read.option(\"header\", True).csv(\"file:///root/nyc_taxi_sample.csv\", inferSchema=True)\n\n# Exportar resultados a CSV\noutput_df = df.select(\"passenger_count\", \"trip_distance\", \"total_amount\")\noutput_df.write.mode(\"overwrite\").csv(\"file:///root/nyc_output.csv\", header=True)\nprint(\"Datos exportados a 'nyc_output.csv'.\")\n</code></pre>"},{"location":"tarea-final/#explicacion-del-script","title":"Explicaci\u00f3n del script","text":"<p>El script proporcionado es un punto de partida para realizar an\u00e1lisis de datos utilizando PySpark. A continuaci\u00f3n, se explica cada secci\u00f3n del c\u00f3digo:</p>"},{"location":"tarea-final/#1-creacion-de-la-sesion-de-spark","title":"1. Creaci\u00f3n de la sesi\u00f3n de Spark","text":"<pre><code>spark = SparkSession.builder \\\n     .appName(\"TaxiAnalysis\") \\\n     .master(\"local[*]\") \\\n     .getOrCreate()\n</code></pre> <p>Aqu\u00ed se crea una sesi\u00f3n de Spark, que es el punto de entrada para trabajar con PySpark. El nombre de la aplicaci\u00f3n se define como \"TaxiAnalysis\" y se utiliza el modo local con todos los n\u00facleos disponibles (<code>local[*]</code>).</p>"},{"location":"tarea-final/#2-lectura-del-archivo-csv","title":"2. Lectura del archivo CSV","text":"<pre><code>df = spark.read.option(\"header\", True).csv(\"file:///root/nyc_taxi_sample.csv\", inferSchema=True)\n</code></pre> <p>Se lee un archivo CSV que contiene los datos de los viajes en taxi. La opci\u00f3n <code>header=True</code> indica que la primera fila del archivo contiene los nombres de las columnas, y <code>inferSchema=True</code> permite que PySpark detecte autom\u00e1ticamente los tipos de datos.</p>"},{"location":"tarea-final/#3-exportacion-de-datos","title":"3. Exportaci\u00f3n de datos","text":"<pre><code>output_df = df.select(\"passenger_count\", \"trip_distance\", \"total_amount\")\noutput_df.write.mode(\"overwrite\").csv(\"file:///root/nyc_output.csv\", header=True)\nprint(\"Datos exportados a 'nyc_output.csv'.\")\n</code></pre> <p>Se seleccionan las columnas <code>passenger_count</code>, <code>trip_distance</code> y <code>total_amount</code> del DataFrame original y se exportan a un nuevo archivo CSV llamado <code>nyc_output.csv</code>. El modo <code>overwrite</code> asegura que si el archivo ya existe, ser\u00e1 sobrescrito.</p> <p>Este script es solo un punto de partida y debe ser modificado para incluir los an\u00e1lisis y transformaciones requeridos en las instrucciones de la tarea.</p>"},{"location":"tarea-final/#archivo-csv-nyc_taxi_samplecsv","title":"Archivo CSV (<code>nyc_taxi_sample.csv</code>)","text":"<pre><code>tpep_pickup_datetime,pickup_datetime,dropoff_datetime,passenger_count,trip_distance,fare_amount,total_amount,PULocationID,tip_amount\n2024-01-01 08:15:27,2024-01-01 08:15:27,2024-01-01 08:45:12,1,5.3,18.50,20.35,132,1.85\n2024-01-01 09:12:45,2024-01-01 09:12:45,2024-01-01 09:34:18,2,7.8,24.00,26.40,145,2.40\n2024-01-01 10:05:30,2024-01-01 10:05:30,2024-01-01 10:22:44,1,3.2,12.75,14.03,107,1.28\n2024-01-01 11:40:55,2024-01-01 11:40:55,2024-01-01 12:01:10,3,4.5,15.00,16.50,236,1.50\n2024-01-01 13:22:18,2024-01-01 13:22:18,2024-01-01 13:45:00,1,6.1,20.00,22.00,148,2.00\n2024-01-01 14:07:01,2024-01-01 14:07:01,2024-01-01 14:29:35,2,5.7,17.80,19.58,234,1.78\n2024-01-01 15:10:12,2024-01-01 15:10:12,2024-01-01 15:35:45,1,4.2,14.50,15.95,132,1.45\n2024-01-01 16:20:30,2024-01-01 16:20:30,2024-01-01 16:50:10,2,8.3,25.00,27.50,145,2.50\n2024-01-01 17:05:15,2024-01-01 17:05:15,2024-01-01 17:30:00,1,3.9,13.20,14.52,107,1.32\n2024-01-01 18:12:45,2024-01-01 18:12:45,2024-01-01 18:40:30,3,7.5,22.50,24.75,236,2.25\n</code></pre>"},{"location":"tarea-final/#explicacion-de-las-columnas-del-csv","title":"Explicaci\u00f3n de las columnas del CSV","text":"<p>El archivo <code>nyc_taxi_sample.csv</code> contiene las siguientes columnas:</p> <ol> <li>tpep_pickup_datetime: Fecha y hora en que comenz\u00f3 el viaje en taxi.</li> <li>pickup_datetime: Fecha y hora de recogida (similar a <code>tpep_pickup_datetime</code>).</li> <li>dropoff_datetime: Fecha y hora en que termin\u00f3 el viaje en taxi.</li> <li>passenger_count: N\u00famero de pasajeros en el viaje.</li> <li>trip_distance: Distancia recorrida durante el viaje (en millas).</li> <li>fare_amount: Monto de la tarifa base del viaje (sin incluir propinas ni otros cargos).</li> <li>total_amount: Monto total del viaje, incluyendo tarifa base, propinas y otros cargos.</li> <li>PULocationID: Identificador de la ubicaci\u00f3n de recogida (Pickup Location ID).</li> <li>tip_amount: Monto de la propina otorgada al conductor.</li> </ol> <p>Estas columnas proporcionan informaci\u00f3n detallada sobre cada viaje en taxi, lo que permite realizar an\u00e1lisis como el costo promedio por distancia, la cantidad de pasajeros por viaje, y m\u00e1s.</p>"},{"location":"tarea-final/#instrucciones-de-la-tarea","title":"Instrucciones de la Tarea","text":""},{"location":"tarea-final/#parte-1-exploracion-de-datos","title":"Parte 1: Exploraci\u00f3n de Datos","text":"<ul> <li>Muestra las primeras filas del DataFrame.</li> <li>Muestra el esquema del DataFrame (<code>printSchema()</code>).</li> <li>Imprime el n\u00famero total de registros.</li> <li>Muestra la lista de columnas.</li> </ul>"},{"location":"tarea-final/#parte-2-analisis-de-los-datos","title":"Parte 2: An\u00e1lisis de los Datos","text":"<p>Realiza los siguientes an\u00e1lisis utilizando PySpark:</p> <ol> <li>Muestra las 10 tarifas (<code>total_amount</code>) m\u00e1s altas.</li> <li>Convierte la columna <code>tpep_pickup_datetime</code> en tipo <code>timestamp</code>.</li> <li>Filtra los viajes que costaron m\u00e1s de \\$100.</li> <li>Calcula la tarifa promedio por n\u00famero de pasajeros (<code>passenger_count</code>).</li> <li>Muestra el n\u00famero de viajes por mes.</li> <li>Muestra el top 5 de ubicaciones de recogida (<code>PULocationID</code>) con m\u00e1s viajes.</li> <li>Muestra los viajes que no tienen propina (<code>tip_amount == 0</code>).</li> </ol>"},{"location":"tarea-final/#parte-3-exportacion","title":"Parte 3: Exportaci\u00f3n","text":"<ul> <li>Ya est\u00e1 incluida en el template una exportaci\u00f3n de datos a un nuevo CSV. No necesitas entregar este archivo.</li> </ul>"},{"location":"tarea-final/#entregables","title":"Entregables","text":"<p>Debes entregar:</p> <ol> <li>El script <code>.py</code> con todo el an\u00e1lisis resuelto y bien comentado.</li> <li>Una captura de pantalla (o varias si es necesario) mostrando la salida por consola de tu script.</li> </ol>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/","title":"Big Data: Motivaci\u00f3n, Almacenamiento y Procesamiento \ud83d\ude80\ud83d\udcca","text":"<p>El t\u00e9rmino Big Data ha ganado una gran relevancia en la era digital moderna. No se trata solo de manejar grandes vol\u00famenes de datos, sino de extraer valor y conocimiento de ellos para tomar decisiones informadas y estrat\u00e9gicas. Desde sus or\u00edgenes hasta su integraci\u00f3n con tecnolog\u00edas avanzadas como el Cloud Computing y la Inteligencia Artificial, el Big Data ha revolucionado la forma en que las organizaciones operan en m\u00faltiples sectores.</p>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#motivacion-del-big-data-y-su-origen","title":"\ud83c\udf1f Motivaci\u00f3n del Big Data y su Origen","text":"<p>El Big Data surgi\u00f3 como una respuesta natural a la creciente cantidad de datos generados por dispositivos digitales, redes sociales, sensores IoT, smartphones, y una multitud de otros dispositivos conectados a la red. Este crecimiento exponencial de datos oblig\u00f3 a las organizaciones a buscar soluciones para almacenar, procesar y analizar eficientemente esta informaci\u00f3n masiva.</p> <p>Empresas pioneras como Google, Facebook y Amazon fueron las primeras en desarrollar e implementar infraestructuras capaces de manejar cantidades inmensas de datos. Al hacerlo, comenzaron a descubrir patrones, comportamientos y tendencias que ofrec\u00edan insights valiosos para la toma de decisiones.</p>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#por-que-es-importante-el-big-data","title":"\u00bfPor qu\u00e9 es Importante el Big Data?","text":"<p>El Big Data ha sido un motor clave para la transformaci\u00f3n digital de las empresas. A trav\u00e9s del an\u00e1lisis masivo de datos, las organizaciones pueden descubrir patrones y relaciones que no eran visibles antes, optimizando su rendimiento y abriendo nuevas oportunidades de crecimiento. Las razones clave de su importancia incluyen:</p> <ol> <li> <p>Tomar Decisiones Basadas en Datos:     Gracias al an\u00e1lisis de grandes vol\u00famenes de datos, las empresas pueden identificar patrones y tendencias que antes eran dif\u00edciles de detectar. Esto permite tomar decisiones m\u00e1s r\u00e1pidas y acertadas. Por ejemplo, una empresa minorista puede analizar millones de transacciones de clientes para identificar qu\u00e9 productos tienen una mayor probabilidad de compra en ciertas temporadas.</p> </li> <li> <p>Optimizaci\u00f3n de Procesos:     El Big Data permite optimizar procesos en todos los niveles. Desde la cadena de suministro hasta el marketing, el an\u00e1lisis de datos permite identificar ineficiencias y \u00e1reas de mejora. Un ejemplo claro es el uso de datos en la log\u00edstica, donde el an\u00e1lisis de rutas en tiempo real puede reducir los tiempos de entrega y los costos de transporte.</p> </li> <li> <p>Innovaci\u00f3n y Desarrollo de Nuevos Productos:     El an\u00e1lisis de datos masivos permite identificar nuevas oportunidades de mercado y desarrollar productos personalizados para satisfacer mejor las necesidades del cliente. Por ejemplo, empresas del sector de la salud pueden analizar grandes cantidades de datos m\u00e9dicos para identificar tendencias de consumo en productos saludables, lo que puede llevar al desarrollo de productos m\u00e1s alineados con los intereses del consumidor.</p> </li> </ol>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#almacenamiento-masivo-de-datos-las-vs-del-big-data","title":"\ud83c\udfe2 Almacenamiento Masivo de Datos: Las Vs del Big Data","text":"<p>El almacenamiento de grandes vol\u00famenes de datos es un pilar fundamental en el ecosistema de Big Data. Tradicionalmente, se mencionan las \"4Vs\" para describir las caracter\u00edsticas de Big Data, pero en realidad, existen muchas m\u00e1s Vs que a\u00f1aden complejidad y potencial a su an\u00e1lisis:</p> <ul> <li> <p>Volumen \ud83d\udce6: Se refiere a la enorme cantidad de datos generados y almacenados en sistemas digitales cada segundo. Desde redes sociales hasta dispositivos IoT, cada actividad genera datos que se deben almacenar y analizar. Empresas como Facebook y YouTube generan petabytes de datos diariamente. El reto es manejar este volumen de datos de manera eficiente sin perder velocidad de procesamiento.</p> </li> <li> <p>Velocidad \u26a1: Es la rapidez con la que se generan, recopilan y procesan los datos. Los sistemas de Big Data deben ser capaces de procesar la informaci\u00f3n casi en tiempo real para generar valor. Por ejemplo, empresas financieras utilizan datos en tiempo real para tomar decisiones sobre inversiones.</p> </li> <li> <p>Variedad \ud83c\udf08: Los datos provienen de diferentes fuentes y formatos, como texto, im\u00e1genes, videos, audio, transacciones, sensores IoT, etc. Esto hace necesario el uso de tecnolog\u00edas avanzadas capaces de analizar tanto datos estructurados (como bases de datos) como no estructurados (como publicaciones en redes sociales).</p> </li> <li> <p>Veracidad \ud83d\udee1\ufe0f: La calidad de los datos es crucial para tomar decisiones acertadas. Los datos incorrectos, duplicados o incompletos pueden llevar a conclusiones err\u00f3neas. Garantizar la veracidad de los datos implica establecer controles de calidad y limpieza antes de procesarlos para asegurar su fiabilidad.</p> </li> <li> <p>Variabilidad: No solo los vol\u00famenes de datos crecen, sino que tambi\u00e9n var\u00edan constantemente. Las tendencias y patrones de comportamiento cambian a lo largo del tiempo, y el an\u00e1lisis debe adaptarse a estos cambios.</p> </li> <li> <p>Valor \ud83d\udcb0: Es la capacidad de extraer insights \u00fatiles de grandes vol\u00famenes de datos. De nada sirve almacenar cantidades masivas de datos si no se puede extraer valor de ellos. Las empresas que logran transformar los datos en informaci\u00f3n valiosa pueden mejorar su posici\u00f3n competitiva y tomar mejores decisiones.</p> </li> </ul> <p>Para m\u00e1s informaci\u00f3n sobre las m\u00faltiples Vs del Big Data, puedes consultar esta infograf\u00eda: Infograf\u00eda sobre Big Data.</p>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#sistemas-de-almacenamiento-de-datos","title":"\ud83d\udcc2 Sistemas de Almacenamiento de Datos","text":"<p>El almacenamiento de grandes vol\u00famenes de datos debe cumplir con una serie de requisitos clave para manejar el crecimiento de la informaci\u00f3n de manera eficiente. Estos requisitos son:</p> <ol> <li>Capacidad: Los sistemas de almacenamiento deben ser escalables para soportar el crecimiento continuo de datos sin comprometer el rendimiento del sistema.</li> <li>Rendimiento: El acceso r\u00e1pido y eficiente a los datos es esencial para asegurar un procesamiento eficaz. Los sistemas de almacenamiento deben estar optimizados para acceder a los datos de manera \u00e1gil, sin cuellos de botella.</li> <li>Fiabilidad: Es crucial asegurar que los datos est\u00e9n protegidos contra p\u00e9rdidas y fallos del sistema. La replicaci\u00f3n y redundancia de datos ayudan a garantizar la disponibilidad continua de la informaci\u00f3n.</li> <li>Recuperabilidad: En caso de un fallo o p\u00e9rdida accidental de datos, los sistemas deben facilitar su recuperaci\u00f3n de manera r\u00e1pida y eficiente, minimizando el tiempo de inactividad.</li> </ol>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#dispositivos-mas-usados","title":"\ud83d\ude80 Dispositivos M\u00e1s Usados","text":"<ol> <li> <p>Discos (HDD, SSD, RAID): </p> <ul> <li>Los discos duros (HDD) ofrecen gran capacidad de almacenamiento a bajo costo, pero son m\u00e1s lentos en comparaci\u00f3n con los SSD.</li> <li>Los discos de estado s\u00f3lido (SSD) son mucho m\u00e1s r\u00e1pidos y eficientes, lo que los hace ideales para aplicaciones que requieren procesamiento a alta velocidad.</li> <li>Los arreglos RAID mejoran tanto la fiabilidad como el rendimiento al combinar varios discos en configuraciones redundantes. Esto permite la continuidad del servicio en caso de fallos de un disco individual.</li> </ul> <p></p> </li> <li> <p>Cintas Magn\u00e9ticas \ud83e\uddf2: Aunque puede parecer una tecnolog\u00eda antigua, las cintas magn\u00e9ticas siguen siendo una opci\u00f3n popular para el archivado a largo plazo debido a su bajo costo. Se utilizan com\u00fanmente para almacenar grandes vol\u00famenes de datos que no necesitan ser accedidos con frecuencia, como copias de seguridad.</p> </li> <li> <p>Almacenamiento en Red (NAS, SAN) \ud83c\udf10: </p> <ul> <li>NAS (Network Attached Storage) y SAN (Storage Area Network) permiten compartir almacenamiento a trav\u00e9s de una red, facilitando el acceso a los datos desde m\u00faltiples dispositivos. Estas soluciones son comunes en empresas que manejan grandes cantidades de datos de forma colaborativa.</li> </ul> </li> <li> <p>Almacenamiento en la Nube \u2601\ufe0f: El almacenamiento en la nube se ha convertido en una de las soluciones m\u00e1s populares debido a su escalabilidad, flexibilidad y capacidad para facilitar la recuperaci\u00f3n de datos ante desastres. Adem\u00e1s, permite a las empresas reducir costos al no tener que invertir en infraestructura propia.</p> </li> </ol>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#metodos-avanzados-de-almacenamiento-clusters","title":"\ud83d\udee0\ufe0f M\u00e9todos Avanzados de Almacenamiento: Clusters","text":"<p>Los sistemas distribuidos y clusters permiten gestionar grandes vol\u00famenes de datos de manera m\u00e1s eficiente, segura y escalable. Estos sistemas distribuyen los datos en varios nodos o servidores, asegurando redundancia y mejorando el rendimiento.</p> <ul> <li> <p>Tipos de RAID: Los diferentes niveles de RAID (como RAID 0, RAID 1, RAID 5, RAID 10) ofrecen diversas combinaciones de redundancia y rendimiento. RAID 5, por ejemplo, ofrece un equilibrio entre protecci\u00f3n de datos y rendimiento, siendo ideal para entornos que necesitan redundancia sin sacrificar velocidad.</p> </li> <li> <p>GlusterFS y MooseFS: Son sistemas de archivos distribuidos dise\u00f1ados para manejar grandes vol\u00famenes de datos. Estos sistemas permiten a las organizaciones administrar sus datos a trav\u00e9s de m\u00faltiples servidores, garantizando la disponibilidad y la redundancia de la informaci\u00f3n.</p> </li> <li> <p>CephFileSystem: Es un sistema de almacenamiento distribuido y altamente escalable que ofrece capacidades avanzadas de auto-reparaci\u00f3n y recuperaci\u00f3n. Es utilizado por grandes empresas que necesitan manejar petabytes de datos.</p> </li> <li> <p>DRBD (Distributed Replicated Block Device): Proporciona replicaci\u00f3n de datos en tiempo real entre servidores, asegurando que los datos est\u00e9n siempre disponibles y sincronizados en m\u00faltiples ubicaciones. Esto es vital para sistemas de alta disponibilidad.</p> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#procesamiento-de-datos-de-la-recoleccion-a-la-visualizacion","title":"\ud83d\udd0d Procesamiento de Datos: De la Recolecci\u00f3n a la Visualizaci\u00f3n","text":"<p>El procesamiento de datos en Big Data abarca una serie de etapas clave que transforman los datos brutos en informaci\u00f3n \u00fatil y aplicable. Cada etapa es esencial para obtener insights valiosos. A continuaci\u00f3n, ilustramos cada fase utilizando un ejemplo en el sector de la salud, donde se analizan datos de millones de pacientes para detectar patrones relacionados con enfermedades cr\u00f3nicas como la diabetes o la hipertensi\u00f3n.</p>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#etapas-de-procesamiento","title":"\ud83d\udcdd Etapas de Procesamiento","text":"<pre><code>graph TB\n    A[Recolecci\u00f3n de Datos] --&gt; B[Recopilaci\u00f3n]\n    B --&gt; C[Preprocesamiento o Limpieza de Datos]\n    C --&gt; D[Procesamiento]\n    D --&gt; E[Interpretaci\u00f3n y Visualizaci\u00f3n]\n    E --&gt; F[An\u00e1lisis]\n    F --&gt; G[Almacenamiento]</code></pre> <ol> <li> <p>Recolecci\u00f3n de Datos \ud83d\udce5:     En el caso de la salud, los datos provienen de diversas fuentes como:</p> <ul> <li>Historiales m\u00e9dicos electr\u00f3nicos (EMR).</li> <li>Dispositivos port\u00e1tiles como pulseras de actividad o relojes inteligentes que monitorean constantes vitales.</li> <li>Encuestas y cuestionarios de salud.</li> <li>Bases de datos gen\u00e9ticas.</li> </ul> <p>Ejemplo: Recolectamos datos de los niveles de glucosa, actividad f\u00edsica y dieta de millones de pacientes que utilizan dispositivos m\u00e9dicos y de bienestar.</p> </li> <li> <p>Recopilaci\u00f3n: Una vez recolectados, los datos de diferentes fuentes se consolidan en un almac\u00e9n de datos distribuido (como un sistema Hadoop o un almac\u00e9n en la nube como Amazon S3 o Azure Blob Storage) para su an\u00e1lisis posterior.</p> <p>Ejemplo: Los datos de pacientes de varios hospitales y dispositivos m\u00e9dicos son centralizados en una plataforma de almacenamiento en la nube para ser procesados de manera unificada.</p> </li> <li> <p>Preprocesamiento o Limpieza de Datos \ud83e\uddf9: En esta fase, se eliminan los datos duplicados, inconsistentes o incompletos para asegurar la calidad del an\u00e1lisis. Se estandarizan los formatos de datos para que todas las fuentes utilicen las mismas unidades de medida y estructura.</p> <p>Ejemplo: Se eliminan las entradas duplicadas y se estandarizan las unidades de medida (por ejemplo, convertir los niveles de glucosa de mg/dL a mmol/L) para que los datos sean coherentes en todo el conjunto.</p> </li> <li> <p>Procesamiento \ud83d\udda5\ufe0f: Se aplican algoritmos avanzados como machine learning y t\u00e9cnicas de miner\u00eda de datos para analizar patrones dentro de los datos de los pacientes y predecir la probabilidad de desarrollar enfermedades cr\u00f3nicas.</p> <p>Ejemplo: Un algoritmo de regresi\u00f3n log\u00edstica analiza los datos y predice la probabilidad de que un paciente desarrolle diabetes en los pr\u00f3ximos cinco a\u00f1os en funci\u00f3n de sus niveles de glucosa, actividad f\u00edsica y gen\u00e9tica.</p> </li> <li> <p>Interpretaci\u00f3n y Visualizaci\u00f3n \ud83d\udcca: Los resultados se presentan mediante gr\u00e1ficos interactivos, dashboards y reportes comprensibles que ayudan a los m\u00e9dicos a entender los patrones y tendencias.</p> <p>Ejemplo: Un dashboard interactivo muestra gr\u00e1ficos sobre c\u00f3mo diferentes factores como la obesidad, la falta de ejercicio y los antecedentes familiares influyen en el riesgo de desarrollar diabetes. Los m\u00e9dicos pueden ver f\u00e1cilmente c\u00f3mo var\u00edan estos factores seg\u00fan la regi\u00f3n geogr\u00e1fica o la edad del paciente.</p> </li> <li> <p>An\u00e1lisis \ud83e\udde0: En esta fase se profundiza en los resultados obtenidos para descubrir insights valiosos. Por ejemplo, el an\u00e1lisis puede revelar correlaciones inesperadas entre los h\u00e1bitos alimenticios y la aparici\u00f3n de enfermedades.</p> <p>Ejemplo: El an\u00e1lisis revela que el 80% de los pacientes con obesidad y antecedentes familiares tienen una alta probabilidad de desarrollar diabetes tipo 2 dentro de los pr\u00f3ximos cinco a\u00f1os.</p> </li> <li> <p>Almacenamiento: Finalmente, los datos analizados y sus resultados se almacenan para usos futuros, auditor\u00edas o para ser comparados con nuevos datos en investigaciones posteriores.</p> <p>Ejemplo: Los resultados se almacenan en una base de datos distribuida para su posterior an\u00e1lisis y comparaci\u00f3n con nuevos pacientes a lo largo del tiempo, lo que permite un monitoreo continuo de las tendencias de salud p\u00fablica.</p> </li> </ol>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#analitica-en-tiempo-real","title":"\ud83d\udcc8 Anal\u00edtica en Tiempo Real","text":"<p>Uno de los mayores beneficios del Big Data es la capacidad de realizar an\u00e1lisis en tiempo real. La anal\u00edtica en tiempo real permite a las empresas reaccionar inmediatamente ante eventos que ocurren en el momento. Algunos ejemplos incluyen:</p> <ul> <li> <p>Servicios Financieros: Las instituciones financieras utilizan an\u00e1lisis en tiempo real para monitorear transacciones y detectar posibles fraudes en el momento en que ocurren. Esto permite bloquear transacciones sospechosas antes de que se completen.</p> </li> <li> <p>Plataformas de Streaming: Empresas como Netflix y Spotify analizan en tiempo real el comportamiento de sus usuarios para ofrecer recomendaciones personalizadas sobre qu\u00e9 series, pel\u00edculas o canciones ver o escuchar a continuaci\u00f3n.</p> </li> <li> <p>Smart Cities: Las ciudades inteligentes utilizan sensores distribuidos en toda la infraestructura urbana para monitorear el tr\u00e1fico, los niveles de contaminaci\u00f3n y el consumo de energ\u00eda en tiempo real. Esto permite ajustes autom\u00e1ticos para optimizar el uso de recursos y mejorar la calidad de vida de los ciudadanos.</p> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#big-data-y-cloud-computing","title":"\u2601\ufe0f Big Data y Cloud Computing","text":"<p>El Cloud Computing ha abierto un nuevo mundo de posibilidades para el Big Data. Al combinar ambas tecnolog\u00edas, las empresas pueden escalar sus operaciones sin necesidad de costosas inversiones en infraestructura f\u00edsica.</p>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#ventajas-del-cloud-computing-para-big-data","title":"Ventajas del Cloud Computing para Big Data","text":"<ul> <li> <p>Escalabilidad Ilimitada: El cloud computing permite a las empresas ajustar su capacidad de procesamiento y almacenamiento seg\u00fan sea necesario. Esto significa que pueden escalar vertical y horizontalmente sin comprometer el rendimiento.</p> </li> <li> <p>Costos Bajo Demanda: Las empresas solo pagan por los recursos que utilizan, lo que optimiza los costos operativos y evita gastos innecesarios en infraestructura f\u00edsica.</p> </li> <li> <p>Accesibilidad Global: El almacenamiento en la nube permite que los datos y las aplicaciones sean accesibles desde cualquier parte del mundo, facilitando la colaboraci\u00f3n entre equipos distribuidos geogr\u00e1ficamente.</p> </li> <li> <p>Seguridad y Recuperaci\u00f3n: Las soluciones en la nube ofrecen avanzadas medidas de seguridad y recuperaci\u00f3n ante desastres, asegurando que los datos est\u00e9n protegidos y disponibles incluso en situaciones de emergencia.</p> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/1motivacionyorigen/#conclusion","title":"\ud83d\ude80 Conclusi\u00f3n","text":"<p>El Big Data ha transformado la forma en que las organizaciones capturan, almacenan, procesan y analizan datos. Desde sus or\u00edgenes hasta las avanzadas soluciones de Cloud Computing y Inteligencia Artificial, el Big Data ha proporcionado una plataforma poderosa para la innovaci\u00f3n y la toma de decisiones estrat\u00e9gicas. La combinaci\u00f3n de almacenamiento masivo, procesamiento distribuido y an\u00e1lisis en tiempo real est\u00e1 remodelando industrias enteras, creando nuevas oportunidades y optimizando las operaciones empresariales. \u00a1Es el momento de aprovechar el poder del Big Data para llevar tu organizaci\u00f3n al siguiente nivel!</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/","title":"2.1. \u00bfQu\u00e9 es Apache Hadoop?","text":""},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#que-es-apache-hadoop","title":"\u00bfQu\u00e9 es Apache Hadoop? \ud83d\ude80","text":"<p>Apache Hadoop es un marco de software de c\u00f3digo abierto dise\u00f1ado para el almacenamiento y procesamiento masivo de datos en cl\u00fasteres de computadoras. Gracias a su arquitectura distribuida, Hadoop es capaz de manejar grandes cantidades de informaci\u00f3n de manera eficiente y rentable, convirti\u00e9ndose en un pilar esencial en el mundo del Big Data.</p> <p>Hadoop no solo almacena datos, sino que tambi\u00e9n facilita su procesamiento en paralelo, lo que permite analizar grandes vol\u00famenes de informaci\u00f3n de manera r\u00e1pida. Su capacidad para escalar desde unos pocos servidores hasta miles lo convierte en una herramienta flexible y poderosa para empresas de todos los tama\u00f1os.</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#como-funciona-hadoop","title":"\ud83e\udde0 \u00bfC\u00f3mo Funciona Hadoop?","text":"<p>Hadoop se compone principalmente de cuatro m\u00f3dulos que trabajan en conjunto para proporcionar un ecosistema completo de Big Data:</p> <ol> <li> <p>HDFS (Hadoop Distributed File System) \ud83d\udcc2: Almacena grandes vol\u00famenes de datos distribuidos a trav\u00e9s de m\u00faltiples nodos, garantizando alta disponibilidad y resistencia a fallos.</p> </li> <li> <p>YARN (Yet Another Resource Negotiator) \ud83c\udfaf: Act\u00faa como un administrador de recursos, asignando tareas y gestionando recursos de manera eficiente dentro del cl\u00faster.</p> </li> <li> <p>MapReduce \ud83d\udee0\ufe0f: Es el motor de procesamiento de datos que divide las tareas en subtareas m\u00e1s peque\u00f1as, permitiendo el procesamiento en paralelo de grandes conjuntos de datos.</p> </li> <li> <p>Hadoop Common \u2699\ufe0f: Proporciona las herramientas y utilidades b\u00e1sicas que soportan los dem\u00e1s m\u00f3dulos, facilitando la integraci\u00f3n y el funcionamiento del ecosistema.</p> </li> </ol>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#por-que-elegir-hadoop","title":"\ud83d\udea6 \u00bfPor Qu\u00e9 Elegir Hadoop?","text":""},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#1-escalabilidad-infinita","title":"1. Escalabilidad Infinita \ud83c\udfd7\ufe0f","text":"<p>Hadoop est\u00e1 dise\u00f1ado para crecer junto con tus necesidades. Desde unos pocos nodos hasta miles de m\u00e1quinas, puede manejar crecimientos exponenciales de datos sin perder rendimiento. Su arquitectura permite la adici\u00f3n de nodos sin necesidad de reconfigurar el sistema, lo que facilita la expansi\u00f3n continua.</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#2-rentabilidad","title":"2. Rentabilidad \ud83d\udcb0","text":"<p>El uso de hardware b\u00e1sico y de bajo costo hace que Hadoop sea una soluci\u00f3n asequible para las empresas que necesitan manejar grandes vol\u00famenes de datos. A diferencia de otros sistemas de datos que requieren hardware especializado, Hadoop se ejecuta en servidores comunes, reduciendo significativamente los costos de infraestructura.</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#3-flexibilidad-y-adaptabilidad","title":"3. Flexibilidad y Adaptabilidad \ud83d\udd04","text":"<p>No importa si tus datos son estructurados, no estructurados o semiestructurados; Hadoop puede almacenar y procesar cualquier tipo de informaci\u00f3n. Esto lo hace ideal para un amplio rango de aplicaciones, desde an\u00e1lisis de redes sociales hasta procesamiento de registros de sensores.</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#4-resistencia-a-fallos","title":"4. Resistencia a Fallos \ud83d\udd12","text":"<p>Hadoop est\u00e1 dise\u00f1ado con la seguridad en mente. Al replicar datos en varios nodos dentro del cl\u00faster, garantiza que la informaci\u00f3n est\u00e9 disponible incluso si un nodo falla, asegurando la continuidad operativa sin interrupciones.</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#5-procesamiento-rapido-y-paralelo","title":"5. Procesamiento R\u00e1pido y Paralelo \u26a1","text":"<p>Gracias a MapReduce, Hadoop procesa grandes vol\u00famenes de datos en paralelo, dividiendo tareas complejas en subtareas m\u00e1s peque\u00f1as. Esto ahorra tiempo y mejora la eficiencia al manejar trabajos que, de otro modo, podr\u00edan llevar horas o d\u00edas.</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#procesamiento-distribuido-clusteres-y-tipos-de-nodos","title":"\ud83d\udd27 Procesamiento Distribuido, Cl\u00fasteres y Tipos de Nodos","text":"<p>El procesamiento distribuido es el coraz\u00f3n de Apache Hadoop, permitiendo que grandes vol\u00famenes de datos se dividan en partes m\u00e1s peque\u00f1as y se procesen de manera simult\u00e1nea en varios nodos dentro de un cl\u00faster.</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#procesamiento-distribuido","title":"Procesamiento Distribuido","text":"<p>El procesamiento distribuido implica dividir las tareas de procesamiento de datos en partes m\u00e1s peque\u00f1as, las cuales se ejecutan de forma paralela en diferentes m\u00e1quinas o nodos. En lugar de procesar los datos de forma secuencial en un solo servidor, Hadoop divide los datos en bloques que son procesados simult\u00e1neamente. Esto aumenta la eficiencia y reduce el tiempo necesario para analizar grandes vol\u00famenes de informaci\u00f3n.</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#clusteres-en-hadoop","title":"Cl\u00fasteres en Hadoop","text":"<p>Un cl\u00faster en Hadoop est\u00e1 compuesto por un conjunto de nodos conectados entre s\u00ed, que trabajan en conjunto para almacenar y procesar datos de manera distribuida. Los cl\u00fasteres est\u00e1n dise\u00f1ados para ser escalables, lo que significa que se pueden agregar m\u00e1s nodos seg\u00fan sea necesario, incrementando la capacidad de procesamiento sin afectar el rendimiento general.</p> <ul> <li>Escalabilidad Horizontal: Se logra a\u00f1adiendo m\u00e1s nodos al cl\u00faster.</li> <li>Distribuci\u00f3n de Carga: Los datos y tareas se distribuyen entre todos los nodos para maximizar el uso eficiente de los recursos.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#tipos-de-nodos-en-hadoop","title":"Tipos de Nodos en Hadoop","text":"<p>En un cl\u00faster de Hadoop, existen varios tipos de nodos que desempe\u00f1an funciones cr\u00edticas dentro del sistema:</p> <ol> <li> <p>Nodo Maestro (NameNode):  </p> <ul> <li>Funci\u00f3n: Coordina el almacenamiento de datos y gestiona el sistema de archivos distribuido (HDFS). Supervisa la ubicaci\u00f3n de los bloques de datos y garantiza la replicaci\u00f3n para asegurar la resistencia a fallos.</li> <li>Responsabilidades: Mantiene el registro de d\u00f3nde est\u00e1n almacenados los bloques de datos en los DataNodes. El NameNode es esencial para la administraci\u00f3n general del cl\u00faster y su buen funcionamiento.</li> </ul> </li> <li> <p>Nodos de Datos (DataNode):  </p> <ul> <li>Funci\u00f3n: Son responsables de almacenar los bloques de datos y responder a las solicitudes del NameNode. Estos nodos realizan la mayor parte del trabajo pesado al manejar los datos en bruto.</li> <li>Responsabilidades: Almacenan los bloques de datos y ejecutan las tareas de procesamiento. Cada DataNode puede almacenar varias copias de los datos, asegurando la replicaci\u00f3n y la alta disponibilidad. Esto garantiza la recuperaci\u00f3n de datos en caso de fallos.</li> </ul> </li> <li> <p>Nodos Edge (EdgeNode):  </p> <ul> <li>Funci\u00f3n: Los nodos edge act\u00faan como un puente entre el cl\u00faster Hadoop y la red externa. Son los nodos a trav\u00e9s de los cuales los usuarios pueden interactuar con el cl\u00faster, enviar trabajos y obtener resultados.</li> <li>Responsabilidades: Proporcionan interfaces para que los datos y comandos entren y salgan del cl\u00faster de manera segura. Estos nodos no almacenan ni procesan datos directamente, pero ofrecen una capa de seguridad y control al filtrar el acceso y las solicitudes a los NameNodes y DataNodes.</li> </ul> </li> </ol> <pre><code>graph LR\n    subgraph Cl\u00faster de Hadoop\n        A[NameNode] --&gt; B[DataNode 1]\n        A --&gt; C[DataNode 2]\n        A --&gt; D[DataNode 3]\n    end\n    E[EdgeNode] --&gt; A\n    F[Usuarios] --&gt; E</code></pre>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#tolerancia-a-fallos-en-los-nodos","title":"Tolerancia a Fallos en los Nodos","text":"<p>Hadoop garantiza la tolerancia a fallos mediante la replicaci\u00f3n de los datos en m\u00faltiples DataNodes. Si un nodo falla, los datos a\u00fan est\u00e1n disponibles en otros nodos del cl\u00faster, lo que evita la p\u00e9rdida de informaci\u00f3n y garantiza que el procesamiento contin\u00fae sin interrupciones.</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#casos-de-uso-de-hadoop","title":"\ud83c\udf10 Casos de Uso de Hadoop","text":"<p>Hadoop ha revolucionado m\u00faltiples industrias gracias a su capacidad para manejar grandes vol\u00famenes de datos de manera eficiente. Aqu\u00ed algunos de los sectores donde Hadoop marca la diferencia:</p>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#1-finanzas-y-bancos","title":"1. Finanzas y Bancos \ud83c\udfe6","text":"<ul> <li>Detecci\u00f3n de Fraudes: Analiza patrones en tiempo real para detectar y prevenir actividades fraudulentas.</li> <li>An\u00e1lisis de Riesgos: Procesa grandes vol\u00famenes de datos financieros para identificar y gestionar riesgos.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#2-salud","title":"2. Salud \ud83c\udfe5","text":"<ul> <li>Gen\u00f3mica: Procesa datos de secuenciaci\u00f3n gen\u00e9tica para avanzar en la medicina personalizada.</li> <li>An\u00e1lisis de Im\u00e1genes M\u00e9dicas: Maneja grandes vol\u00famenes de im\u00e1genes para mejorar diagn\u00f3sticos y tratamientos.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#3-telecomunicaciones","title":"3. Telecomunicaciones \ud83d\udce1","text":"<ul> <li>An\u00e1lisis de Redes: Monitorea y optimiza el rendimiento de las redes en tiempo real.</li> <li>Modelos Predictivos: Utiliza datos hist\u00f3ricos para prever fallos y optimizar el servicio al cliente.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#4-retail-y-e-commerce","title":"4. Retail y E-commerce \ud83d\uded2","text":"<ul> <li>An\u00e1lisis del Comportamiento del Cliente: Utiliza datos de navegaci\u00f3n y compra para personalizar ofertas.</li> <li>Gesti\u00f3n de Inventarios: Optimiza la cadena de suministro basada en patrones de compra y demanda.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/21queesapachehadoop/#conclusion","title":"\ud83c\udf10 Conclusi\u00f3n","text":"<p>Apache Hadoop no es solo una tecnolog\u00eda; es una revoluci\u00f3n en la forma en que manejamos y procesamos grandes vol\u00famenes de datos. Desde su capacidad para escalar hasta miles de nodos, hasta su capacidad para procesar datos en paralelo, Hadoop ha transformado la forma en que las organizaciones analizan y gestionan sus datos.</p>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/","title":"Ecosistema Hadoop y Distribuciones \ud83c\udf10\ud83d\ude80","text":"<p>El ecosistema Hadoop ha revolucionado la forma en que las organizaciones manejan y procesan datos a gran escala. Hadoop no es solo un software; es un ecosistema completo de herramientas y tecnolog\u00edas que trabajan juntas para resolver los desaf\u00edos del Big Data. A medida que el volumen, la variedad y la velocidad de los datos contin\u00faan creciendo, el ecosistema Hadoop se expande para incluir m\u00faltiples componentes y distribuciones dise\u00f1adas para aprovechar al m\u00e1ximo esta revoluci\u00f3n de datos.</p>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#que-es-el-ecosistema-hadoop","title":"\ud83e\udde0 \u00bfQu\u00e9 es el Ecosistema Hadoop?","text":"<p>El ecosistema Hadoop es una colecci\u00f3n de proyectos y herramientas que interact\u00faan entre s\u00ed para proporcionar una plataforma integral para el almacenamiento, procesamiento y an\u00e1lisis de grandes vol\u00famenes de datos. Este ecosistema incluye componentes para la ingesti\u00f3n de datos, el procesamiento en tiempo real, el an\u00e1lisis avanzado y la gesti\u00f3n de recursos.</p> <pre><code>graph LR\n    A[Hadoop Ecosystem] --&gt; B[HDFS]\n    A --&gt; C[YARN]\n    A --&gt; D[MapReduce]\n    A --&gt; E[Spark]\n    A --&gt; F[Hive]\n    A --&gt; G[HBase]\n    A --&gt; H[Pig]\n    A --&gt; I[Oozie]\n    A --&gt; J[Sqoop]\n    A --&gt; K[Flume]</code></pre>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#componentes-clave-del-ecosistema-hadoop","title":"Componentes Clave del Ecosistema Hadoop \ud83d\udee0\ufe0f","text":"<ol> <li> <p>HDFS (Hadoop Distributed File System) \ud83d\udcc2: El sistema de archivos distribuido que almacena grandes vol\u00famenes de datos de manera eficiente y segura.</p> </li> <li> <p>YARN (Yet Another Resource Negotiator) \ud83c\udfaf: Gestor de recursos que asigna y administra las tareas dentro del cl\u00faster Hadoop.</p> </li> <li> <p>MapReduce \ud83d\udee0\ufe0f: Modelo de programaci\u00f3n que permite el procesamiento paralelo de datos en un entorno distribuido.</p> </li> <li> <p>Apache Spark \u26a1: Motor de procesamiento r\u00e1pido y en memoria que ofrece una alternativa m\u00e1s \u00e1gil a MapReduce para el an\u00e1lisis de datos en tiempo real.</p> </li> <li> <p>Apache Hive \ud83d\udc1d: Herramienta que facilita la consulta y el an\u00e1lisis de datos almacenados en HDFS utilizando un lenguaje similar a SQL, conocido como HiveQL.</p> </li> <li> <p>Apache HBase \ud83d\udcca: Base de datos NoSQL de alto rendimiento que proporciona acceso en tiempo real a grandes vol\u00famenes de datos distribuidos.</p> </li> <li> <p>Apache Pig \ud83d\udc37: Lenguaje de alto nivel para el procesamiento de grandes conjuntos de datos que simplifica la escritura de scripts complejos en comparaci\u00f3n con MapReduce.</p> </li> <li> <p>Apache Oozie \ud83d\udcc5: Coordinador de flujos de trabajo que permite programar y gestionar trabajos en Hadoop.</p> </li> <li> <p>Apache Sqoop \ud83d\udd04: Herramienta que facilita la transferencia de datos entre Hadoop y bases de datos relacionales.</p> </li> <li> <p>Apache Flume \ud83d\udce5: Sistema de ingesti\u00f3n de datos que permite recopilar, agregar y mover grandes cantidades de datos de eventos a Hadoop.</p> </li> </ol>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#distribuciones-populares-de-hadoop","title":"\ud83c\udf10 Distribuciones Populares de Hadoop","text":"<p>Las distribuciones de Hadoop son paquetes que integran el ecosistema de herramientas Hadoop con caracter\u00edsticas adicionales de administraci\u00f3n y soporte. Estas distribuciones est\u00e1n dise\u00f1adas para simplificar la implementaci\u00f3n, configuraci\u00f3n y mantenimiento de cl\u00fasteres Hadoop. Aqu\u00ed te presentamos algunas de las m\u00e1s populares:</p>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#1-cloudera-distribution-for-hadoop-cdh","title":"1. Cloudera Distribution for Hadoop (CDH) \ud83c\udfe2","text":"<p>Cloudera es una de las distribuciones comerciales m\u00e1s reconocidas de Hadoop. Ofrece una versi\u00f3n completa del ecosistema Hadoop con herramientas adicionales para la gesti\u00f3n, seguridad y an\u00e1lisis de datos.</p> <ul> <li>Gesti\u00f3n Simplificada: Cloudera Manager permite gestionar y monitorear el cl\u00faster de forma centralizada.</li> <li>Seguridad Mejorada: Ofrece encriptaci\u00f3n de datos y autenticaci\u00f3n avanzada.</li> <li>Optimizaci\u00f3n de Desempe\u00f1o: Ajustes autom\u00e1ticos que mejoran la eficiencia de las tareas.</li> </ul> <pre><code>// Ejemplo de conexi\u00f3n a un cl\u00faster de Hadoop usando Cloudera\nconst cloudera = require('cloudera-api');\n\n// Conectar al cl\u00faster de Cloudera\nconst client = new cloudera.Cluster({\n  hostname: 'cloudera-cluster.local',\n  username: 'admin',\n  password: 'password'\n});\n\nclient.getStatus((err, status) =&gt; {\n  if (err) {\n    console.error('Error conectando al cl\u00faster:', err);\n  } else {\n    console.log('Estado del cl\u00faster:', status);\n  }\n});\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#2-hortonworks-data-platform-hdp","title":"2. Hortonworks Data Platform (HDP) \ud83d\udc18","text":"<p>Hortonworks, ahora parte de Cloudera, ofrece una distribuci\u00f3n de Hadoop completamente de c\u00f3digo abierto. HDP se enfoca en la integraci\u00f3n de datos y proporciona un s\u00f3lido conjunto de herramientas para el an\u00e1lisis y la gesti\u00f3n de datos.</p> <ul> <li>Soporte 100% Open Source: Fomenta la innovaci\u00f3n y permite la personalizaci\u00f3n completa de la plataforma.</li> <li>Integraci\u00f3n con la Nube: Compatible con implementaciones en la nube y en entornos h\u00edbridos.</li> <li>Simplificaci\u00f3n de Operaciones: Herramientas para la automatizaci\u00f3n de flujos de trabajo y la gesti\u00f3n de datos.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#3-mapr","title":"3. MapR \ud83c\udf32","text":"<p>MapR destaca por su arquitectura \u00fanica que combina Hadoop con un sistema de archivos distribuido patentado y una base de datos NoSQL integrada. Ofrece una alta disponibilidad y rendimiento superior en comparaci\u00f3n con otras distribuciones.</p> <ul> <li>MapR XD y MapR DB: Proporcionan almacenamiento y gesti\u00f3n de datos avanzados con capacidades empresariales.</li> <li>Soporte de Contenedores y Microservicios: Compatible con Kubernetes para la implementaci\u00f3n de aplicaciones modernas.</li> <li>Procesamiento en Tiempo Real: Capacidades para an\u00e1lisis de flujos de datos en tiempo real.</li> </ul> <pre><code>// Ejemplo de integraci\u00f3n con MapR usando JavaScript\nconst mapr = require('mapr-streams');\n\n// Configuraci\u00f3n de una conexi\u00f3n de flujo de datos en tiempo real\nconst stream = mapr.createStream('/path/to/stream');\n\nstream.on('data', (message) =&gt; {\n  console.log('Mensaje recibido:', message.value.toString());\n});\n\nstream.write({ key: 'sensor1', value: 'temperatura: 22\u00b0C' });\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#4-amazon-emr-elastic-mapreduce","title":"4. Amazon EMR (Elastic MapReduce) \u2601\ufe0f","text":"<p>Amazon EMR es la distribuci\u00f3n basada en la nube de Hadoop ofrecida por AWS. Permite escalar f\u00e1cilmente el cl\u00faster y ajustar los recursos seg\u00fan la demanda de procesamiento de datos.</p> <ul> <li>Escalabilidad Autom\u00e1tica: Ajusta la capacidad del cl\u00faster en funci\u00f3n de la carga de trabajo.</li> <li>Integraci\u00f3n con Servicios AWS: F\u00e1cil integraci\u00f3n con S3, Redshift y otras soluciones de AWS.</li> <li>Costos Bajo Demanda: Paga solo por lo que usas, optimizando los costos operativos.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#casos-de-uso-del-ecosistema-hadoop","title":"\ud83d\udea6 Casos de Uso del Ecosistema Hadoop","text":"<p>El ecosistema Hadoop no solo almacena datos; lo transforma en valor accionable. A continuaci\u00f3n, se presentan algunos casos de uso donde las empresas utilizan Hadoop y sus herramientas para generar impacto:</p>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#1-analisis-de-redes-sociales","title":"1. An\u00e1lisis de Redes Sociales \ud83d\udde8\ufe0f","text":"<p>Las empresas analizan millones de interacciones en redes sociales para entender las tendencias del mercado y la opini\u00f3n del cliente. Herramientas como Spark y Hive se utilizan para procesar estos datos r\u00e1pidamente.</p>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#2-recomendacion-de-productos","title":"2. Recomendaci\u00f3n de Productos \ud83d\udecd\ufe0f","text":"<p>Las plataformas de e-commerce utilizan algoritmos de aprendizaje autom\u00e1tico en Hadoop para analizar el comportamiento del usuario y recomendar productos personalizados en tiempo real.</p> <pre><code>// Ejemplo de recomendaci\u00f3n de productos usando Spark y JavaScript\nconst spark = require('apache-spark');\n\n// Crear un modelo de recomendaci\u00f3n basado en el historial de compras del usuario\nconst recommendations = spark.mllib.recommendation.ALS.train(usersPurchases, 10, 0.01);\n\nrecommendations.predict(user, (err, products) =&gt; {\n  if (err) {\n    console.error('Error generando recomendaciones:', err);\n  } else {\n    console.log('Productos recomendados:', products);\n  }\n});\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#3-prevencion-de-fraudes-financieros","title":"3. Prevenci\u00f3n de Fraudes Financieros \ud83c\udfe6","text":"<p>Bancos y aseguradoras usan Hadoop para analizar transacciones en tiempo real y detectar patrones sospechosos. Hadoop permite combinar m\u00faltiples fuentes de datos para una detecci\u00f3n de fraudes m\u00e1s precisa y r\u00e1pida.</p>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#4-monitoreo-de-infraestructuras","title":"4. Monitoreo de Infraestructuras \ud83d\udce1","text":"<p>Las empresas de telecomunicaciones utilizan el ecosistema Hadoop para monitorear sus infraestructuras de red, detectando fallos y optimizando el rendimiento en tiempo real.</p>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#ejemplo-completo-de-integracion-del-ecosistema-hadoop-en-javascript","title":"\ud83d\udee0\ufe0f Ejemplo Completo de Integraci\u00f3n del Ecosistema Hadoop en JavaScript","text":"<p>Para entender c\u00f3mo funciona el ecosistema Hadoop en la pr\u00e1ctica, consideremos un ejemplo completo que integra varios componentes:</p> <pre><code>const hdfs = require('hdfs');\nconst spark = require('apache-spark');\nconst hive = require('hive-client');\n\n// Configuraci\u00f3n de HDFS\nconst hdfsClient = hdfs({\n  protocol: 'http',\n  hostname: 'localhost',\n  port: 9870\n});\n\n// Subir datos a HDFS\nhdfsClient.createFile('/user/data.txt', 'Datos para an\u00e1lisis', (err) =&gt; {\n  if (err) {\n    console.error('Error al subir archivo:', err);\n  } else {\n    console.log('Archivo subido a HDFS correctamente.');\n  }\n});\n\n// Consultar datos en Hive\nconst hiveClient = hive.createClient({ host: 'localhost', port: 10000 });\n\nhiveClient.connect().then(() =&gt; {\n  hiveClient.query('SELECT * FROM logs WHERE event=\"error\";', (err, results) =&gt; {\n    if (err) {\n      console.error('Error en la consulta Hive:', err);\n    } else {\n\n\n console.log('Resultados de la consulta:', results);\n    }\n  });\n});\n\n// Procesar datos con Spark\nspark.session.builder().getOrCreate().then(session =&gt; {\n  const dataFrame = session.read().format('csv').load('/user/data.txt');\n\n  dataFrame.filter(dataFrame.col('event').equalTo('error'))\n    .show()\n    .then(() =&gt; session.stop());\n});\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/22ecosistemahadoopydistribuciones/#conclusion","title":"\ud83c\udf1f Conclusi\u00f3n","text":"<p>El ecosistema Hadoop y sus diversas distribuciones son fundamentales para cualquier estrategia de Big Data moderna. Ofrecen una soluci\u00f3n integral para almacenar, procesar y analizar datos a gran escala, permitiendo a las empresas tomar decisiones informadas basadas en datos. Con la flexibilidad para manejar todo tipo de datos y la capacidad de escalar a cualquier tama\u00f1o de cl\u00faster, Hadoop contin\u00faa liderando la transformaci\u00f3n digital en todo el mundo. \u00a1Explora el poder del ecosistema Hadoop y descubre c\u00f3mo puede revolucionar tu gesti\u00f3n de datos! \ud83d\ude80\ud83d\udcca</p>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/","title":"Arquitectura de Hadoop: Desentra\u00f1ando la Potencia del Big Data \ud83d\ude80\ud83e\udde0","text":"<p>Apache Hadoop es una plataforma robusta de c\u00f3digo abierto dise\u00f1ada para almacenar y procesar grandes vol\u00famenes de datos de manera eficiente y escalable. Pero \u00bfqu\u00e9 hace que Hadoop sea tan poderoso? La respuesta radica en su arquitectura distribuida, que permite procesar datos a trav\u00e9s de m\u00faltiples nodos de manera paralela. En este art\u00edculo, exploraremos en detalle la arquitectura de Hadoop, desglosando sus componentes y c\u00f3mo trabajan juntos para hacer del Big Data una realidad accesible.</p>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#componentes-principales-de-la-arquitectura-de-hadoop","title":"\ud83e\udde9 Componentes Principales de la Arquitectura de Hadoop","text":"<p>La arquitectura de Hadoop se compone de varios m\u00f3dulos que colaboran para ofrecer un entorno completo de almacenamiento y procesamiento de datos. Los componentes clave son:</p> <pre><code>graph TD\n    A[Arquitectura de Hadoop] --&gt; B[HDFS]\n    A --&gt; C[YARN]\n    A --&gt; D[MapReduce]\n    A --&gt; E[Hadoop Common]\n    A --&gt; F[Componentes de Integraci\u00f3n]</code></pre>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#1-hdfs-hadoop-distributed-file-system","title":"1. HDFS (Hadoop Distributed File System) \ud83d\udcc2","text":"<p>HDFS es el sistema de archivos distribuido de Hadoop, dise\u00f1ado para almacenar datos de manera segura y eficiente en grandes cl\u00fasteres. Se encarga de dividir los archivos grandes en bloques y distribuirlos a trav\u00e9s de diferentes nodos en el cl\u00faster.</p> <pre><code>graph TD\n    HDFS[HDFS] --&gt;|Divide Archivos en| Bloques[Bloques]\n    Bloques --&gt;|Distribuye en| Nodos[Nodos]\n    Nodos --&gt;|Copia y Replicaci\u00f3n| Copias[Copias de Seguridad]</code></pre>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#caracteristicas-de-hdfs","title":"Caracter\u00edsticas de HDFS:","text":"<ul> <li>Alta Disponibilidad: Al replicar los bloques de datos en varios nodos, asegura que los datos est\u00e9n siempre disponibles, incluso si uno de los nodos falla.</li> <li>Escalabilidad: A\u00f1adir nuevos nodos al cl\u00faster incrementa autom\u00e1ticamente la capacidad de almacenamiento.</li> <li>Tolerancia a Fallos: Dise\u00f1ado para detectar y recuperarse autom\u00e1ticamente de fallos de hardware y software.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#2-yarn-yet-another-resource-negotiator","title":"2. YARN (Yet Another Resource Negotiator) \ud83c\udfaf","text":"<p>YARN act\u00faa como el gestor de recursos de Hadoop. Asigna recursos de procesamiento a las aplicaciones y coordina la ejecuci\u00f3n de tareas en el cl\u00faster, asegurando que los trabajos se completen de manera eficiente.</p> <pre><code>graph TD\n    YARN[YARN] --&gt;|Gesti\u00f3n de Recursos| Asignaci\u00f3n[Asignaci\u00f3n de Recursos]\n    YARN --&gt;|Ejecuci\u00f3n de Tareas| Tareas[Tareas del Cl\u00faster]\n    Tareas --&gt;|Optimizaci\u00f3n| Eficiencia[Eficiencia del Cl\u00faster]</code></pre>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#caracteristicas-de-yarn","title":"Caracter\u00edsticas de YARN:","text":"<ul> <li>Asignaci\u00f3n Din\u00e1mica de Recursos: Distribuye recursos seg\u00fan las necesidades de las aplicaciones en tiempo real, optimizando el uso del cl\u00faster.</li> <li>Seguridad y Control: Ofrece control granular sobre la ejecuci\u00f3n de tareas, garantizando la seguridad y estabilidad del sistema.</li> <li>Escalabilidad: Permite la expansi\u00f3n del cl\u00faster sin necesidad de reconfiguraciones complejas.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#3-mapreduce","title":"3. MapReduce \ud83d\udee0\ufe0f","text":"<p>MapReduce es el modelo de programaci\u00f3n de Hadoop que permite el procesamiento paralelo de grandes vol\u00famenes de datos. Consiste en dos fases principales: Map y Reduce.</p> <pre><code>graph TD\n    A[MapReduce] --&gt; B[Map]\n    A --&gt; C[Reduce]\n    B --&gt; D[Procesa Datos en Pares Clave-Valor]\n    C --&gt; E[Combina y Reduce Resultados]</code></pre> <ul> <li>Map: Toma los datos de entrada y los procesa en pares clave-valor.</li> <li>Reduce: Combina estos pares para generar un resultado final.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#ejemplo-de-mapreduce-en-javascript","title":"Ejemplo de MapReduce en JavaScript:","text":"<pre><code>// Ejemplo de MapReduce para contar palabras\nconst map = (text) =&gt; {\n  return text.split(' ').map(word =&gt; ({ key: word, value: 1 }));\n};\n\nconst reduce = (mappedData) =&gt; {\n  return mappedData.reduce((acc, curr) =&gt; {\n    acc[curr.key] = (acc[curr.key] || 0) + curr.value;\n    return acc;\n  }, {});\n};\n\nconst data = \"Hadoop es incre\u00edble, Hadoop es poderoso\";\nconst mapped = map(data);\nconst reduced = reduce(mapped);\n\nconsole.log(reduced); // { Hadoop: 2, es: 2, incre\u00edble: 1, poderoso: 1 }\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#4-hadoop-common","title":"4. Hadoop Common \u2699\ufe0f","text":"<p>Hadoop Common proporciona las bibliotecas y utilidades necesarias que soportan los otros m\u00f3dulos de Hadoop, asegurando la integraci\u00f3n y el funcionamiento adecuado de todo el ecosistema.</p> <ul> <li>Funciones B\u00e1sicas: Ofrece soporte para la gesti\u00f3n de configuraci\u00f3n, registro y acceso remoto.</li> <li>Soporte Multiplataforma: Compatible con diferentes sistemas operativos, lo que facilita su implementaci\u00f3n en cualquier entorno.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#5-componentes-de-integracion","title":"5. Componentes de Integraci\u00f3n \ud83d\udd0c","text":"<p>Hadoop no funciona en solitario. Se integra con varias herramientas y tecnolog\u00edas para ampliar sus capacidades y proporcionar un entorno m\u00e1s completo para la gesti\u00f3n de datos:</p> <pre><code>graph TD\n    Hadoop[Hadoop] --&gt; Spark[Spark]\n    Hadoop --&gt; Hive[Hive]\n    Hadoop --&gt; HBase[HBase]\n    Hadoop --&gt; Pig[Pig]\n    Hadoop --&gt; Sqoop[Sqoop]\n    Hadoop --&gt; Flume[Flume]</code></pre> <ul> <li>Apache Spark \u26a1: Ofrece procesamiento en memoria, lo que acelera las tareas de an\u00e1lisis en comparaci\u00f3n con MapReduce.</li> <li>Apache Hive \ud83d\udc1d: Permite consultas SQL sobre datos almacenados en HDFS, facilitando el an\u00e1lisis de datos.</li> <li>Apache HBase \ud83d\udcca: Proporciona acceso en tiempo real a grandes vol\u00famenes de datos distribuidos, ideal para aplicaciones que requieren baja latencia.</li> <li>Apache Pig \ud83d\udc37: Un lenguaje de alto nivel para escribir scripts que procesen grandes conjuntos de datos de manera m\u00e1s simple que MapReduce.</li> <li>Apache Sqoop \ud83d\udd04: Facilita la transferencia de datos entre Hadoop y bases de datos relacionales.</li> <li>Apache Flume \ud83d\udce5: Recoge, agrega y mueve grandes cantidades de datos de eventos a Hadoop.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#como-trabajan-juntos-los-componentes-de-hadoop","title":"\ud83d\udea6 \u00bfC\u00f3mo Trabajan Juntos los Componentes de Hadoop?","text":"<p>La arquitectura de Hadoop se basa en la sinergia de sus componentes. Cada m\u00f3dulo desempe\u00f1a un papel esencial en el procesamiento y almacenamiento de datos a gran escala, trabajando de manera conjunta para ofrecer un entorno completo y robusto.</p> <ol> <li> <p>Ingesti\u00f3n de Datos: Los datos se recopilan mediante herramientas como Flume o Sqoop y se almacenan en HDFS.</p> </li> <li> <p>Gesti\u00f3n de Recursos: YARN administra los recursos del cl\u00faster, asegurando que las tareas se distribuyan de manera eficiente.</p> </li> <li> <p>Procesamiento de Datos: Se realiza mediante MapReduce, Spark, Pig o Hive, dependiendo del tipo de an\u00e1lisis requerido.</p> </li> <li> <p>Acceso y An\u00e1lisis: Hive proporciona un lenguaje similar a SQL para consultar y analizar datos, mientras que HBase permite el acceso en tiempo real.</p> </li> <li> <p>Automatizaci\u00f3n de Flujos de Trabajo: Oozie coordina la ejecuci\u00f3n de trabajos y la automatizaci\u00f3n de tareas repetitivas en el cl\u00faster.</p> </li> </ol>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#ejemplo-completo-de-integracion-de-la-arquitectura-hadoop-en-javascript","title":"\ud83c\udf1f Ejemplo Completo de Integraci\u00f3n de la Arquitectura Hadoop en JavaScript","text":"<p>Para ilustrar c\u00f3mo todos estos componentes trabajan en conjunto, veamos un ejemplo pr\u00e1ctico de integraci\u00f3n utilizando JavaScript:</p> <pre><code>const hdfs = require('hdfs'); // Interacci\u00f3n con HDFS\nconst yarn = require('yarn-client'); // Gesti\u00f3n de tareas en el cl\u00faster\nconst hive = require('hive-client'); // Consultas en Hive\n\n// Conectar a HDFS\nconst hdfsClient = hdfs({\n  protocol: 'http',\n  hostname: 'localhost',\n  port: 9870\n});\n\n// Subir datos a HDFS\nhdfsClient.createFile('/user/data.txt', 'Hadoop es un sistema distribuido', (err) =&gt; {\n  if (err) {\n    console.error('Error al crear archivo en HDFS:', err);\n  } else {\n    console.log('Archivo creado en HDFS.');\n  }\n});\n\n// Ejecutar tarea con YARN\nconst yarnClient = new yarn.Client();\nyarnClient.submitJob('analyze-data', '/user/data.txt', (err) =&gt; {\n  if (err) {\n    console.error('Error ejecutando trabajo en YARN:', err);\n  } else {\n    console.log('Trabajo completado en YARN.');\n  }\n});\n\n// Consultar resultados en Hive\nconst hiveClient = hive.createClient({ host: 'localhost', port: 10000 });\n\nhiveClient.connect().then(() =&gt; {\n  hiveClient.query('SELECT * FROM logs WHERE event=\"Hadoop\";', (err, results) =&gt; {\n    if (err) {\n      console.error('Error en la consulta Hive:', err);\n    } else {\n      console.log('Resultados de la consulta Hive:', results);\n    }\n  });\n});\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/23arquitectura/#conclusion","title":"\ud83d\ude80 Conclusi\u00f3n","text":"<p>La arquitectura de Hadoop es un ejemplo brillante de c\u00f3mo los sistemas distribuidos pueden transformar la manera en que manejamos y analizamos datos masivos. Con una combinaci\u00f3n de almacenamiento robusto, gesti\u00f3n eficiente de recursos y capacidades avanzadas de procesamiento, Hadoop se ha convertido en la columna vertebral del Big Data moderno. Ya sea que est\u00e9s trabajando en an\u00e1lisis de datos, modelado predictivo o simplemente necesites un sistema escalable y resistente, la arquitectura de Hadoop proporciona las herramientas necesarias para desbloquear el verdadero potencial de tus datos. \ud83c\udf10</p>"},{"location":"ut1-introduccion-a-hadoop/indicedetareas/","title":"\u00cdndice de pr\u00e1cticas y tareas","text":"<p>Este \u00edndice incluye todas las pr\u00e1cticas guiadas y tareas correspondientes al curso. Aseg\u00farate de seguir las instrucciones cuidadosamente para cada pr\u00e1ctica y tarea, y consulta los materiales de apoyo cuando sea necesario.</p>"},{"location":"ut1-introduccion-a-hadoop/indicedetareas/#practicas-guiadas","title":"Pr\u00e1cticas guiadas","text":"<p>Las pr\u00e1cticas guiadas est\u00e1n dise\u00f1adas para que sigas un conjunto de instrucciones paso a paso y te familiarices con el entorno y las herramientas utilizadas en Big Data. Cada pr\u00e1ctica cubre un aspecto clave del entorno de Hadoop y su ecosistema.</p> <ol> <li> <p>Pr\u00e1ctica Instalaci\u00f3n Hadoop Single Node</p> <ul> <li>Descripci\u00f3n: Esta pr\u00e1ctica cubre el proceso de instalaci\u00f3n de Hadoop en un entorno de un solo nodo. El objetivo es configurar un cl\u00faster Hadoop b\u00e1sico en modo pseudo-distribuido en tu m\u00e1quina local.</li> <li>Objetivos: <ul> <li>Configurar Hadoop en un \u00fanico nodo.</li> <li>Ejecutar comandos b\u00e1sicos de HDFS.</li> <li>Verificar la instalaci\u00f3n utilizando los servicios de Hadoop.</li> </ul> </li> </ul> </li> <li> <p>Pr\u00e1ctica Inicial HDFS</p> <ul> <li>Descripci\u00f3n: Esta pr\u00e1ctica introduce el sistema de archivos distribuido de Hadoop (HDFS). Aprender\u00e1s a subir y descargar archivos en el sistema y a utilizar comandos b\u00e1sicos para manipular los datos.</li> <li>Objetivos: <ul> <li>Subir, descargar y listar archivos en HDFS.</li> <li>Utilizar comandos esenciales como <code>hdfs dfs -put</code>, <code>hdfs dfs -get</code>, y <code>hdfs dfs -ls</code>.</li> <li>Verificar la replicaci\u00f3n de los bloques y la ubicaci\u00f3n de los archivos en HDFS.</li> </ul> </li> </ul> </li> </ol>"},{"location":"ut1-introduccion-a-hadoop/indicedetareas/#tareas","title":"Tareas","text":"<p>Las tareas son ejercicios que complementan las pr\u00e1cticas guiadas y tienen como objetivo reforzar los conocimientos adquiridos. Algunas tareas requerir\u00e1n investigaci\u00f3n adicional y la implementaci\u00f3n de soluciones m\u00e1s avanzadas.</p> <ol> <li>Introducci\u00f3n al Big Data<ul> <li>Descripci\u00f3n: Esta tarea introductoria cubre los conceptos clave de Big Data, sus desaf\u00edos, y las tecnolog\u00edas fundamentales en el ecosistema Hadoop.</li> <li>Objetivos:<ul> <li>Definir Big Data y sus caracter\u00edsticas (volumen, velocidad, variedad, etc.).</li> <li>Explicar la importancia de Hadoop en el procesamiento de grandes vol\u00famenes de datos.</li> <li>Realizar una breve investigaci\u00f3n sobre casos de uso de Big Data en la industria.</li> </ul> </li> </ul> </li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/1tareaintroductoria/","title":"Tarea Introducci\u00f3n al Big Data","text":"<ol> <li> <p>Selecciona una Empresa o Tem\u00e1tica:</p> <ul> <li>Elige una empresa real (por ejemplo, Amazon, Facebook, Spotify, etc.) o una tem\u00e1tica espec\u00edfica (por ejemplo, an\u00e1lisis de datos m\u00e9dicos, optimizaci\u00f3n de rutas log\u00edsticas, etc.) que recoja y maneje grandes vol\u00famenes de datos.</li> <li>Explica brevemente c\u00f3mo esa empresa o tem\u00e1tica seleccionada obtiene datos.</li> </ul> </li> <li> <p>Describe Cada Fase del Procesamiento de Datos:</p> <ul> <li> <p>Recolecci\u00f3n de Datos \ud83d\udce5: Explica de d\u00f3nde y c\u00f3mo tu empresa seleccionada recolecta los datos (por ejemplo, redes sociales, sensores, historiales de compra, etc.).</p> </li> <li> <p>Recopilaci\u00f3n: Describe c\u00f3mo los datos recolectados se organizan en un sistema centralizado o base de datos para su posterior an\u00e1lisis.</p> </li> <li> <p>Preprocesamiento o Limpieza de Datos \ud83e\uddf9: Explica c\u00f3mo la empresa limpia los datos (eliminaci\u00f3n de datos duplicados, correcci\u00f3n de errores, conversi\u00f3n de formatos) antes de analizarlos.</p> </li> <li> <p>Procesamiento \ud83d\udda5\ufe0f: Detalla c\u00f3mo se procesan los datos (algoritmos, machine learning, modelos predictivos) para extraer informaci\u00f3n \u00fatil.</p> </li> <li> <p>Interpretaci\u00f3n y Visualizaci\u00f3n \ud83d\udcca: Describe c\u00f3mo se presentan los resultados a los usuarios o tomadores de decisiones (dashboards, gr\u00e1ficos, informes).</p> </li> <li> <p>An\u00e1lisis \ud83e\udde0: Comenta c\u00f3mo la empresa utiliza los resultados del an\u00e1lisis para tomar decisiones estrat\u00e9gicas o mejorar sus operaciones.</p> </li> <li> <p>Almacenamiento: Finalmente, explica c\u00f3mo y d\u00f3nde se almacenan los datos procesados y los resultados para su uso futuro (en la nube, bases de datos, sistemas locales).</p> </li> </ul> </li> <li> <p>Comenta investigando al menos 3Vs m\u00e1s de las mencionadas</p> </li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/","title":"Pr\u00e1ctica Guiada: Implementaci\u00f3n de un Nodo Hadoop con Docker","text":""},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/#1-introduccion","title":"1. Introducci\u00f3n","text":"<p>En esta pr\u00e1ctica, vas a configurar un cl\u00faster Hadoop de un solo nodo utilizando Docker. Hadoop es un framework que permite el procesamiento de grandes cantidades de datos distribuidos en varios nodos. Para fines educativos, configuraremos un cl\u00faster de un solo nodo. Docker facilitar\u00e1 el proceso de instalaci\u00f3n, ya que encapsula todo en un contenedor aislado del sistema operativo principal.</p>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/#2-requisitos-previos","title":"2. Requisitos Previos","text":"<ul> <li>Docker instalado: Aseg\u00farate de tener Docker instalado y funcionando en tu m\u00e1quina. Si no lo tienes instalado, puedes descargarlo desde https://www.docker.com/products/docker-desktop.</li> <li>Git instalado: Se necesita Git para clonar el repositorio de configuraci\u00f3n.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/#3-clonar-el-repositorio-del-proyecto","title":"3. Clonar el repositorio del proyecto","text":"<p>Para empezar, vamos a clonar el repositorio donde ya est\u00e1 configurada la imagen de Hadoop. Este repositorio contiene un archivo <code>Dockerfile</code>, que es un script que define c\u00f3mo se debe crear la imagen de Docker con todos los servicios y configuraciones de Hadoop.</p> <pre><code>git clone https://github.com/rancavil/hadoop-single-node-cluster.git\ncd hadoop-single-node-cluster\n</code></pre> <ul> <li>\u00bfQu\u00e9 es esto?: El comando <code>git clone</code> descarga una copia local del repositorio alojado en GitHub, y <code>cd</code> te lleva al directorio reci\u00e9n clonado.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/#4-construir-la-imagen-docker-de-hadoop","title":"4. Construir la imagen Docker de Hadoop","text":"<p>Ahora que tenemos los archivos de configuraci\u00f3n, es momento de construir una imagen Docker. Esta imagen contendr\u00e1 todo lo necesario para correr Hadoop, incluyendo Java, Hadoop y las configuraciones necesarias para un nodo \u00fanico.</p> <pre><code>docker build -t hadoop .\n</code></pre> <ul> <li>Explicaci\u00f3n: <ul> <li><code>docker build</code> es el comando que construye una imagen a partir de un archivo <code>Dockerfile</code>.</li> <li><code>-t hadoop</code> asigna la etiqueta \"hadoop\" a esta imagen, lo que nos facilitar\u00e1 su referencia m\u00e1s adelante.</li> <li>El <code>.</code> indica que el contexto para la construcci\u00f3n es el directorio actual.</li> </ul> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/#5-crear-y-correr-el-contenedor-hadoop","title":"5. Crear y correr el contenedor Hadoop","text":"<p>Una vez que la imagen est\u00e1 construida, vamos a ejecutar un contenedor basado en ella. Un contenedor es una instancia de la imagen, ejecut\u00e1ndose como si fuera una peque\u00f1a m\u00e1quina virtual, pero m\u00e1s ligera y eficiente.</p> <pre><code>docker run --name hadoop-container -p 9864:9864 -p 9870:9870 -p 8088:8088 -p 9000:9000 --hostname hadoop hadoop\n</code></pre> <ul> <li>Explicaci\u00f3n:<ul> <li><code>docker run</code>: Este comando crea y corre un contenedor.</li> <li><code>--name hadoop-container</code>: Asigna el nombre \"hadoop-container\" al contenedor.</li> <li><code>-p 9864:9864 -p 9870:9870 -p 8088:8088 -p 9000:9000</code>: Mapea los puertos del contenedor a tu m\u00e1quina local. Esto te permite acceder a la interfaz web de Hadoop desde tu navegador y establecer la conexi\u00f3n entre el contenedor y tu m\u00e1quina para el uso de HDFS.<ul> <li><code>9864</code>: Puerto del DataNode.</li> <li><code>9870</code>: Puerto del NameNode (UI para monitoreo de HDFS).</li> <li><code>8088</code>: Puerto del ResourceManager (UI para monitoreo de trabajos en YARN).</li> <li><code>9000</code>: Puerto de HDFS para el NameNode.</li> </ul> </li> <li><code>--hostname hadoop</code>: Asigna el nombre de host \"hadoop\" al contenedor.</li> </ul> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/#6-abrir-una-terminal-dentro-del-contenedor","title":"6. Abrir una terminal dentro del contenedor","text":"<p>Para interactuar con Hadoop dentro del contenedor, necesitas abrir una terminal en su interior. Hay dos formas de hacerlo:</p> <ul> <li> <p>Modo gr\u00e1fico: Si est\u00e1s usando Docker Desktop, puedes acceder al contenedor desde la interfaz gr\u00e1fica seleccionando el contenedor \"hadoop-container\" y abriendo una terminal desde ah\u00ed.</p> </li> <li> <p>Modo terminal: Si prefieres usar la terminal, ejecuta el siguiente comando:</p> </li> </ul> <pre><code>docker exec -i -t hadoop-container /bin/bash\n</code></pre> <ul> <li>Explicaci\u00f3n:<ul> <li><code>docker exec</code>: Permite ejecutar comandos dentro de un contenedor en ejecuci\u00f3n.</li> <li><code>-i -t</code>: Estas opciones te permiten interactuar de manera interactiva con el contenedor (modo terminal).</li> <li><code>hadoop-container</code>: Es el nombre del contenedor que ejecutaste anteriormente.</li> <li><code>/bin/bash</code>: Especifica que quieres abrir una shell Bash dentro del contenedor.</li> </ul> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/#7-probar-hadoop-y-hdfs","title":"7. Probar Hadoop y HDFS","text":"<p>Ahora que est\u00e1s dentro del contenedor, es momento de probar que Hadoop y su sistema de archivos HDFS est\u00e1n funcionando correctamente. Comienza creando un directorio en HDFS.</p> <pre><code>hdfs dfs -mkdir /user\n</code></pre> <p>Luego, lista los archivos del sistema de archivos HDFS recursivamente:</p> <pre><code>hdfs dfs -ls -R /\n</code></pre> <ul> <li>Explicaci\u00f3n:<ul> <li><code>hdfs dfs</code>: Este es el comando que utilizas para interactuar con el sistema de archivos distribuido de Hadoop (HDFS).</li> <li><code>-mkdir /user</code>: Crea un directorio llamado \"user\" en el sistema de archivos HDFS.</li> <li><code>-ls -R /</code>: Lista de manera recursiva todos los archivos en HDFS, comenzando desde la ra\u00edz.</li> </ul> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/#8-acceder-a-la-interfaz-web-de-hadoop","title":"8. Acceder a la interfaz web de Hadoop","text":"<p>Puedes acceder a las interfaces de monitoreo de Hadoop desde tu navegador utilizando las siguientes direcciones:</p> <ul> <li>NameNode (HDFS): http://localhost:9870</li> <li>ResourceManager (YARN): http://localhost:8088</li> </ul> <p>Estas interfaces te permiten ver el estado del cl\u00faster, los nodos disponibles, y los trabajos que se est\u00e1n ejecutando.</p>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/#9-apagar-el-contenedor","title":"9. Apagar el contenedor","text":"<p>Cuando termines de trabajar, puedes detener y eliminar el contenedor. Para detener el contenedor sin eliminarlo, ejecuta:</p> <pre><code>docker stop hadoop-container\n</code></pre> <p>Si deseas eliminar el contenedor completamente:</p> <pre><code>docker rm hadoop-container\n</code></pre> <ul> <li>Explicaci\u00f3n:<ul> <li><code>docker stop</code>: Detiene el contenedor en ejecuci\u00f3n.</li> <li><code>docker rm</code>: Elimina el contenedor de Docker, liberando espacio.</li> </ul> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/1instalacionhadoop/#10-conclusion","title":"10. Conclusi\u00f3n","text":"<p>Has completado la pr\u00e1ctica para configurar un cl\u00faster Hadoop de un solo nodo usando Docker. Este entorno te permite experimentar con las funcionalidades principales de Hadoop sin necesidad de una configuraci\u00f3n compleja de varios nodos o sistemas distribuidos. Con este entorno, puedes practicar la gesti\u00f3n de archivos en HDFS, ejecutar trabajos en YARN y familiarizarte con las interfaces de monitoreo de Hadoop.</p>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/1practicainicial/","title":"Pr\u00e1ctica: Gesti\u00f3n de archivos en HDFS en un escenario real","text":""},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/1practicainicial/#contexto-del-caso","title":"Contexto del Caso","text":"<p>Imagina que eres parte del equipo de an\u00e1lisis de datos de una empresa de telecomunicaciones. Tu tarea consiste en gestionar el almacenamiento de grandes vol\u00famenes de registros de llamadas telef\u00f3nicas. Estos registros se almacenar\u00e1n en HDFS para un posterior an\u00e1lisis distribuido. A lo largo de esta pr\u00e1ctica, simular\u00e1s la subida, consulta y gesti\u00f3n de archivos en HDFS utilizando los registros de llamadas de distintos meses.</p>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/1practicainicial/#entregable","title":"Entregable","text":"<ul> <li>Un documento PDF donde se muestren los diferentes pasos a seguir indicados a continuaci\u00f3n, incluyendo capturas y algo de explicaci\u00f3n.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/1practicainicial/#pasos-a-seguir","title":"Pasos a Seguir","text":""},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/1practicainicial/#1-preparacion-de-archivos-locales","title":"1. Preparaci\u00f3n de archivos locales","text":"<ul> <li>Simula dos archivos de texto que contengan los registros de llamadas:<ul> <li><code>llamadas_enero.txt</code>: Este archivo contiene las llamadas realizadas en enero.</li> <li><code>llamadas_febrero.txt</code>: Contiene los registros de las llamadas de febrero.</li> </ul> </li> </ul> <p>Ejemplo de contenido de cada archivo: <pre><code>ID_Llamada, Fecha, Duraci\u00f3n, N\u00famero_Destino\n001, 2024-01-05 14:30:00, 5 minutos, +34987654321\n002, 2024-01-06 16:45:00, 10 minutos, +34123456789\n</code></pre></p>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/1practicainicial/#2-subir-archivos-a-hdfs","title":"2. Subir archivos a HDFS","text":"<ul> <li> <p>Crea un directorio en HDFS donde se almacenar\u00e1n estos archivos:      <pre><code>hdfs dfs -mkdir /user/empresa_telecom/registros_llamadas\n</code></pre></p> </li> <li> <p>Sube los archivos locales <code>llamadas_enero.txt</code> y <code>llamadas_febrero.txt</code> a HDFS usando el comando <code>copyFromLocal</code>:      <pre><code>hdfs dfs -copyFromLocal /ruta/local/llamadas_enero.txt /user/empresa_telecom/registros_llamadas/\nhdfs dfs -copyFromLocal /ruta/local/llamadas_febrero.txt /user/empresa_telecom/registros_llamadas/\n</code></pre></p> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/1practicainicial/#3-verificacion-de-los-archivos-subidos","title":"3. Verificaci\u00f3n de los archivos subidos","text":"<ul> <li> <p>Lista los contenidos del directorio en HDFS para confirmar que los archivos se han subido correctamente:      <pre><code>hdfs dfs -ls /user/empresa_telecom/registros_llamadas\n</code></pre></p> </li> <li> <p>Muestra el contenido de los archivos para verificar su integridad:      <pre><code>hdfs dfs -cat /user/empresa_telecom/registros_llamadas/llamadas_enero.txt\nhdfs dfs -cat /user/empresa_telecom/registros_llamadas/llamadas_febrero.txt\n</code></pre></p> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/1practicainicial/#4-renombrar-los-archivos-para-una-mejor-organizacion","title":"4. Renombrar los archivos para una mejor organizaci\u00f3n","text":"<ul> <li>Sup\u00f3n que la empresa ha decidido que los archivos deben seguir un formato m\u00e1s organizado con el mes en min\u00fasculas. Renombra los archivos:      <pre><code>hdfs dfs -mv /user/empresa_telecom/registros_llamadas/llamadas_enero.txt /user/empresa_telecom/registros_llamadas/llamadas_enero2024.txt\nhdfs dfs -mv /user/empresa_telecom/registros_llamadas/llamadas_febrero.txt /user/empresa_telecom/registros_llamadas/llamadas_febrero2024.txt\n</code></pre></li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/1practicainicial/#5-descargar-archivos-desde-hdfs-para-analisis-local","title":"5. Descargar archivos desde HDFS para an\u00e1lisis local","text":"<ul> <li> <p>Descarga los archivos de registros desde HDFS a tu sistema local para analizarlos utilizando una herramienta de an\u00e1lisis de datos:      <pre><code>hdfs dfs -get /user/empresa_telecom/registros_llamadas/llamadas_enero2024.txt /ruta/local/analisis/\nhdfs dfs -get /user/empresa_telecom/registros_llamadas/llamadas_febrero2024.txt /ruta/local/analisis/\n</code></pre></p> </li> <li> <p>Verifica que los archivos descargados sean los mismos que los originales comparando su contenido.</p> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/1practicainicial/#6-eliminacion-de-archivos-antiguos-de-hdfs","title":"6. Eliminaci\u00f3n de archivos antiguos de HDFS","text":"<ul> <li> <p>Dado que los archivos han sido descargados y procesados, la empresa decide eliminar los registros de enero del sistema HDFS para liberar espacio. Utiliza el comando <code>rm</code> para eliminar el archivo:      <pre><code>hdfs dfs -rm /user/empresa_telecom/registros_llamadas/llamadas_enero2024.txt\n</code></pre></p> </li> <li> <p>Verifica que el archivo ha sido eliminado correctamente listando de nuevo los contenidos del directorio:      <pre><code>hdfs dfs -ls /user/empresa_telecom/registros_llamadas\n</code></pre></p> </li> <li> <p>Si ya no necesitas el directorio donde se almacenaron los registros de llamadas, elim\u00ednalo tambi\u00e9n:      <pre><code>hdfs dfs -rmdir /user/empresa_telecom/registros_llamadas\n</code></pre></p> </li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/1instalacionhadoopec2/","title":"Instalaci\u00f3n de Hadoop en AWS (EC2)","text":""},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/1instalacionhadoopec2/#paso-1-conectate-a-la-instancia-ec2","title":"Paso 1: Con\u00e9ctate a la instancia EC2","text":"<p>Primero, con\u00e9ctate a tu instancia de EC2 mediante SSH:</p> <pre><code>ssh -i /path/to/your-key.pem ubuntu@&lt;IP_de_la_instancia&gt;\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/1instalacionhadoopec2/#paso-2-actualiza-el-sistema-e-instala-java","title":"Paso 2: Actualiza el sistema e instala Java","text":"<p>Hadoop requiere Java, as\u00ed que primero instala Java antes de continuar.</p> <ol> <li>Actualiza los paquetes de la instancia EC2:    <pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y\n</code></pre></li> <li>Instala Java (Hadoop requiere Java 8):    <pre><code>sudo apt install openjdk-8-jdk -y\n</code></pre></li> <li>Verifica la instalaci\u00f3n de Java:    <pre><code>java -version\n</code></pre></li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/1instalacionhadoopec2/#paso-3-descarga-e-instala-hadoop","title":"Paso 3: Descarga e instala Hadoop","text":"<ol> <li>Descarga la \u00faltima versi\u00f3n de Hadoop desde el sitio oficial de Apache:    <pre><code>wget https://downloads.apache.org/hadoop/common/hadoop-3.3.5/hadoop-3.3.5.tar.gz\n</code></pre></li> <li>Extrae el archivo descargado:    <pre><code>tar -xzvf hadoop-3.3.5.tar.gz\n</code></pre></li> <li>Mueve Hadoop a una ubicaci\u00f3n adecuada, por ejemplo, <code>/usr/local/</code>:    <pre><code>sudo mv hadoop-3.3.5 /usr/local/hadoop\n</code></pre></li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/1instalacionhadoopec2/#paso-4-configura-variables-de-entorno","title":"Paso 4: Configura variables de entorno","text":"<p>Configura las variables de entorno necesarias para Hadoop.</p> <ol> <li>Edita el archivo <code>.bashrc</code> del usuario actual:    <pre><code>nano ~/.bashrc\n</code></pre></li> <li>Agrega las siguientes l\u00edneas al final del archivo:</li> </ol> <pre><code>export HADOOP_HOME=/usr/local/hadoop\nexport HADOOP_INSTALL=$HADOOP_HOME\nexport HADOOP_MAPRED_HOME=$HADOOP_HOME\nexport HADOOP_COMMON_HOME=$HADOOP_HOME\nexport HADOOP_HDFS_HOME=$HADOOP_HOME\nexport YARN_HOME=$HADOOP_HOME\nexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native\nexport PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin\nexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\nexport HADOOP_OPTS=\"$HADOOP_OPTS -Djava.library.path=$HADOOP_HOME/lib/native\"\n</code></pre> <ol> <li> <p>Guarda y cierra el archivo.</p> </li> <li> <p>Aplica los cambios:    <pre><code>source ~/.bashrc\n</code></pre></p> </li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/1instalacionhadoopec2/#paso-5-configura-hadoop","title":"Paso 5: Configura Hadoop","text":"<p>Ahora edita los archivos de configuraci\u00f3n de Hadoop.</p> <ol> <li>Edita el archivo <code>hadoop-env.sh</code> para configurar <code>JAVA_HOME</code>:    <pre><code>sudo nano /usr/local/hadoop/etc/hadoop/hadoop-env.sh\n</code></pre></li> <li>Aseg\u00farate de que la variable <code>JAVA_HOME</code> est\u00e9 configurada correctamente:    <pre><code>export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n</code></pre></li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/1instalacionhadoopec2/#paso-6-configura-los-archivos-core-sitexml-y-hdfs-sitexml","title":"Paso 6: Configura los archivos core-site.xml y hdfs-site.xml","text":"<ol> <li>Edita el archivo <code>core-site.xml</code>:    <pre><code>nano /usr/local/hadoop/etc/hadoop/core-site.xml\n</code></pre> <pre><code>&lt;configuration&gt;\n   &lt;property&gt;\n      &lt;name&gt;fs.defaultFS&lt;/name&gt;\n      &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;\n      &lt;description&gt;NameNode URI&lt;/description&gt;\n   &lt;/property&gt;\n   &lt;property&gt;\n      &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;\n      &lt;value&gt;/usr/local/hadoop/tmp&lt;/value&gt;\n      &lt;description&gt;Directorio temporal de Hadoop&lt;/description&gt;\n   &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre></li> <li>Edita el archivo <code>hdfs-site.xml</code>:</li> </ol> <pre><code>nano /usr/local/hadoop/etc/hadoop/hdfs-site.xml\n</code></pre> <pre><code>&lt;configuration&gt;\n   &lt;property&gt;\n      &lt;name&gt;dfs.replication&lt;/name&gt;\n      &lt;value&gt;1&lt;/value&gt;\n      &lt;description&gt;Factor de replicaci\u00f3n de HDFS (para modo pseudo-distribuido es 1)&lt;/description&gt;\n   &lt;/property&gt;\n\n   &lt;property&gt;\n      &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n      &lt;value&gt;file:///usr/local/hadoop/hdfs/namenode&lt;/value&gt;\n      &lt;description&gt;Directorio para los archivos del NameNode&lt;/description&gt;\n   &lt;/property&gt;\n\n   &lt;property&gt;\n      &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n      &lt;value&gt;file:///usr/local/hadoop/hdfs/datanode&lt;/value&gt;\n      &lt;description&gt;Directorio para los bloques de datos del DataNode&lt;/description&gt;\n   &lt;/property&gt;\n\n   &lt;property&gt;\n      &lt;name&gt;dfs.permissions&lt;/name&gt;\n      &lt;value&gt;false&lt;/value&gt;\n      &lt;description&gt;Desactiva los permisos de HDFS para simplificar la configuraci\u00f3n&lt;/description&gt;\n   &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <ol> <li>Crea las carpetas <code>hdfs/namenode</code> y <code>hdfs/datanode</code>:</li> </ol> <pre><code>mkdir -p /usr/local/hadoop/tmp\nmkdir -p /usr/local/hadoop/hdfs/namenode\nmkdir -p /usr/local/hadoop/hdfs/datanode\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/1instalacionhadoopec2/#paso-6-formatea-el-sistema-de-archivos-de-hadoop","title":"Paso 6: Formatea el sistema de archivos de Hadoop","text":"<p>Antes de iniciar Hadoop, necesitas formatear el sistema de archivos.</p> <ol> <li>Ejecuta el siguiente comando para formatear el sistema de archivos de HDFS:    <pre><code>hdfs namenode -format\n</code></pre></li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/1instalacionhadoopec2/#paso-7-inicia-hadoop","title":"Paso 7: Inicia Hadoop","text":"<p>Para iniciar Hadoop, ejecuta los siguientes comandos:</p> <ol> <li>Inicia el NameNode y el DataNode:    <pre><code>start-dfs.sh\n</code></pre></li> <li>Inicia YARN:    <pre><code>start-yarn.sh\n</code></pre></li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/1instalacionhadoopec2/#paso-9-deten-hadoop","title":"Paso 9: Det\u00e9n Hadoop","text":"<p>Para detener Hadoop, usa los siguientes comandos:</p> <ol> <li>Det\u00e9n HDFS:    <pre><code>stop-dfs.sh\n</code></pre></li> <li>Det\u00e9n YARN:    <pre><code>stop-yarn.sh\n</code></pre></li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/1instalacionhadoopec2/#posibles-errores","title":"Posibles errores","text":""},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/1instalacionhadoopec2/#permission-denied-publickey","title":"Permission denied (publickey).","text":"<p>El error \"Permission denied (publickey)\" al ejecutar <code>start-yarn.sh</code> o similar indica que el sistema est\u00e1 intentando iniciar los nodemanagers y resourcemanager en los nodos locales de Hadoop, pero est\u00e1 fallando la autenticaci\u00f3n SSH.</p> <p>Hadoop, por defecto, intenta comunicarse entre nodos a trav\u00e9s de SSH para iniciar YARN, y parece que no se ha configurado correctamente el acceso SSH entre nodos. Si est\u00e1s ejecutando un cl\u00faster pseudo-distribuido (es decir, en una sola m\u00e1quina), necesitar\u00e1s configurar SSH para que el usuario actual pueda conectarse a localhost sin contrase\u00f1a. Aqu\u00ed est\u00e1n los pasos para solucionar este problema:</p>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/1instalacionhadoopec2/#paso-1-configura-ssh-sin-contrasena","title":"Paso 1: Configura SSH sin contrase\u00f1a","text":"<ol> <li>Genera un par de claves SSH si no tienes uno ya creado:    Ejecuta el siguiente comando y acepta los valores predeterminados:</li> </ol> <pre><code>ssh-keygen -t rsa -P \"\"\n</code></pre> <p>Esto crear\u00e1 una clave SSH p\u00fablica y privada en el directorio <code>~/.ssh/</code> (por defecto).</p> <ol> <li>Agrega la clave p\u00fablica al archivo <code>authorized_keys</code> para permitir la autenticaci\u00f3n SSH sin contrase\u00f1a en localhost:    <pre><code>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys\n</code></pre></li> <li>Aseg\u00farate de que los permisos sean correctos:    <pre><code>chmod 700 ~/.ssh\nchmod 600 ~/.ssh/authorized_keys\n</code></pre></li> <li>Prueba la conexi\u00f3n SSH a localhost para verificar que puedas conectarte sin contrase\u00f1a:</li> </ol> <pre><code>ssh localhost\n</code></pre> <p>Si no te pide una contrase\u00f1a y te deja entrar, la configuraci\u00f3n es correcta. Puedes salir de la sesi\u00f3n con <code>exit</code>.</p>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/1instalacionhadoopec2/#paso-2-inicia-yarn-de-nuevo","title":"Paso 2: Inicia YARN de nuevo","text":"<p>Despu\u00e9s de configurar SSH, intenta iniciar YARN de nuevo:</p> <pre><code>start-yarn.sh\n</code></pre> <p>Esto deber\u00eda iniciar correctamente tanto el ResourceManager como los NodeManagers sin el error de \"Permission denied\".</p>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/2primerospasoshdfs/","title":"Pr\u00e1ctica real con comandos de HDFS en EC2","text":""},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/2primerospasoshdfs/#1-crear-un-directorio-en-hdfs","title":"1. Crear un directorio en HDFS","text":"<p>Crea un directorio llamado <code>/practica_hdfs</code> dentro del sistema de archivos distribuido (HDFS).</p> <pre><code>hdfs dfs -mkdir /practica_hdfs\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/2primerospasoshdfs/#2-subir-un-archivo-desde-el-sistema-local-a-hdfs","title":"2. Subir un archivo desde el sistema local a HDFS","text":"<p>Primero, crea un archivo de texto de prueba en tu sistema local:</p> <pre><code>echo \"Este es un archivo de prueba para HDFS\" &gt; archivo_prueba.txt\n</code></pre> <p>Luego, sube el archivo a HDFS dentro del directorio <code>/practica_hdfs</code>:</p> <pre><code>hdfs dfs -put archivo_prueba.txt /practica_hdfs/\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/2primerospasoshdfs/#3-listar-los-archivos-en-un-directorio-de-hdfs","title":"3. Listar los archivos en un directorio de HDFS","text":"<p>Verifica que el archivo fue subido correctamente listando los contenidos del directorio <code>/practica_hdfs</code>:</p> <pre><code>hdfs dfs -ls /practica_hdfs\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/2primerospasoshdfs/#4-ver-el-contenido-de-un-archivo-en-hdfs","title":"4. Ver el contenido de un archivo en HDFS","text":"<p>Muestra el contenido del archivo directamente en la terminal:</p> <pre><code>hdfs dfs -cat /practica_hdfs/archivo_prueba.txt\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/2primerospasoshdfs/#5-mover-o-renombrar-un-archivo-en-hdfs","title":"5. Mover o renombrar un archivo en HDFS","text":"<p>Renombra o mueve el archivo dentro del directorio de HDFS:</p> <pre><code>hdfs dfs -mv /practica_hdfs/archivo_prueba.txt /practica_hdfs/archivo_renombrado.txt\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/2primerospasoshdfs/#6-descargar-un-archivo-desde-hdfs-al-sistema-local","title":"6. Descargar un archivo desde HDFS al sistema local","text":"<p>Descarga el archivo desde HDFS a tu sistema de archivos local (EC2):</p> <pre><code>hdfs dfs -get /practica_hdfs/archivo_renombrado.txt archivo_descargado.txt\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/2primerospasoshdfs/#7-eliminar-un-archivo-de-hdfs","title":"7. Eliminar un archivo de HDFS","text":"<p>Elimina el archivo de HDFS:</p> <pre><code>hdfs dfs -rm /practica_hdfs/archivo_renombrado.txt\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/2primerospasoshdfs/#8-eliminar-un-directorio-en-hdfs","title":"8. Eliminar un directorio en HDFS","text":"<p>Elimina el directorio <code>/practica_hdfs</code>:</p> <pre><code>hdfs dfs -rm -r /practica_hdfs\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/2primerospasoshdfs/#opcional-comandos-adicionales-para-profundizar","title":"Opcional: Comandos adicionales para profundizar","text":""},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/2primerospasoshdfs/#ver-informacion-sobre-el-espacio-disponible-en-hdfs","title":"Ver informaci\u00f3n sobre el espacio disponible en HDFS","text":"<p>Verifica el uso de espacio en HDFS en formato legible:</p> <pre><code>hdfs dfs -df -h\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/2primerospasoshdfs/#contar-archivos-directorios-y-tamano-total-de-un-directorio-en-hdfs","title":"Contar archivos, directorios y tama\u00f1o total de un directorio en HDFS","text":"<p>Usa este comando para contar el n\u00famero de archivos, directorios y el tama\u00f1o total en un directorio espec\u00edfico de HDFS:</p> <pre><code>hdfs dfs -count /practica_hdfs\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/2primerospasoshdfs/#comprobar-bloques-de-un-archivo-en-hdfs","title":"Comprobar bloques de un archivo en HDFS","text":"<p>Obt\u00e9n detalles sobre los bloques que forman un archivo:</p> <pre><code>hdfs fsck /practica_hdfs/archivo_renombrado.txt -files -blocks\n</code></pre>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/3bloques/","title":"3bloques","text":""},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/3bloques/#practica-gestion-de-archivos-en-hdfs-con-datos-de-movielens","title":"Pr\u00e1ctica: Gesti\u00f3n de archivos en HDFS con datos de MovieLens","text":""},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/3bloques/#objetivo","title":"Objetivo:","text":"<p>En esta pr\u00e1ctica, aprender\u00e1s a gestionar archivos en HDFS. Descargar\u00e1s un dataset grande, lo descomprimir\u00e1s, y cargar\u00e1s uno de sus archivos en HDFS. Posteriormente, verificar\u00e1s el estado de los bloques y la replicaci\u00f3n en el sistema de archivos distribuido.</p>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/3bloques/#requisitos-previos","title":"Requisitos previos:","text":"<ul> <li>Conocimientos b\u00e1sicos de HDFS y comandos de terminal.</li> <li>Acceso a un cl\u00faster de Hadoop (puede ser local o en la nube como AWS EMR).</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/3bloques/#pasos-a-seguir","title":"Pasos a seguir:","text":""},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/3bloques/#parte-1-crear-un-directorio-en-hdfs","title":"Parte 1: Crear un directorio en HDFS","text":"<ol> <li>Crear un directorio en HDFS    El primer paso es crear un directorio donde se almacenar\u00e1n los archivos dentro del HDFS. Ejecuta el siguiente comando para crear un directorio llamado <code>prueba</code> en HDFS:    <pre><code>hdfs dfs -mkdir prueba\n</code></pre></li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/3bloques/#parte-2-descargar-el-dataset-de-movielens","title":"Parte 2: Descargar el dataset de MovieLens","text":"<ol> <li>Descargar el dataset    Ahora descargar\u00e1s un archivo de datos p\u00fablicos de MovieLens (un dataset de pel\u00edculas y valoraciones). Usa <code>wget</code> para descargar el archivo:</li> </ol> <pre><code>wget https://files.grouplens.org/datasets/movielens/ml-25m.zip\n</code></pre> <ol> <li>Verificar la descarga    Para asegurarte de que el archivo se descarg\u00f3 correctamente, lista los archivos en el directorio actual:    <pre><code>ls\n</code></pre>    Deber\u00edas ver el archivo <code>ml-25m.zip</code> en la lista.</li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/3bloques/#parte-3-descomprimir-el-archivo","title":"Parte 3: Descomprimir el archivo","text":"<ol> <li>Descomprimir el archivo zip    Descomprime el archivo descargado usando el comando <code>unzip</code>:</li> </ol> <pre><code>unzip ml-25m.zip\n</code></pre> <ol> <li>Verificar los archivos descomprimidos    Lista los archivos descomprimidos para asegurarte de que todo est\u00e1 correcto:    <pre><code>ls ml-25m/\n</code></pre>    Deber\u00edas ver varios archivos CSV, incluyendo <code>ratings.csv</code>, <code>movies.csv</code>, entre otros.</li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/3bloques/#parte-4-subir-un-archivo-a-hdfs","title":"Parte 4: Subir un archivo a HDFS","text":"<ol> <li>Subir el archivo <code>ratings.csv</code> a HDFS    Ahora, sube uno de los archivos CSV (en este caso, <code>ratings.csv</code>) al directorio <code>prueba</code> en HDFS:    <pre><code>hdfs dfs -put ml-25m/ratings.csv prueba\n</code></pre></li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/3bloques/#parte-5-verificar-la-integridad-del-archivo-en-hdfs","title":"Parte 5: Verificar la integridad del archivo en HDFS","text":"<ol> <li>Verificar el archivo con <code>fsck</code>    Utiliza el comando <code>hdfs fsck</code> para verificar la integridad del archivo <code>ratings.csv</code> en HDFS, mostrando los bloques en los que est\u00e1 dividido:</li> </ol> <pre><code>hdfs fsck prueba -files -blocks\n</code></pre> <p>Este comando te proporcionar\u00e1 detalles sobre:</p> <ul> <li>El tama\u00f1o del archivo.</li> <li>El n\u00famero de bloques en que se ha dividido el archivo.</li> <li>El estado de replicaci\u00f3n de cada bloque.</li> </ul>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/3bloques/#parte-6-analisis-de-la-salida","title":"Parte 6: An\u00e1lisis de la salida","text":"<ol> <li>Interpretar la salida    La salida del comando <code>fsck</code> mostrar\u00e1 informaci\u00f3n sobre los bloques y la replicaci\u00f3n del archivo <code>ratings.csv</code>. Presta atenci\u00f3n a los siguientes puntos:</li> <li>Tama\u00f1o total del archivo.</li> <li>Cantidad de bloques en los que se divide.</li> <li>El n\u00famero de r\u00e9plicas para cada bloque (en este caso, deber\u00eda ser 1, ya que el factor de replicaci\u00f3n predeterminado es 1).</li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/3bloques/#parte-7-limpieza-opcional","title":"Parte 7: Limpieza (Opcional)","text":"<ol> <li>Eliminar los archivos de HDFS (opcional)    Si deseas limpiar el sistema de archivos y eliminar el archivo subido, puedes usar el siguiente comando:    <pre><code>hdfs dfs -rm -r prueba\n</code></pre></li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/3bloques/#cuestionario-de-evaluacion","title":"Cuestionario de evaluaci\u00f3n:","text":"<ol> <li>\u00bfQu\u00e9 representa un bloque en HDFS y c\u00f3mo se distribuye un archivo grande entre ellos?</li> <li>\u00bfPor qu\u00e9 es importante verificar la replicaci\u00f3n y el estado de los bloques en HDFS?</li> <li>\u00bfQu\u00e9 comando usaste para subir el archivo a HDFS? \u00bfQu\u00e9 hace espec\u00edficamente?</li> <li>Explica el prop\u00f3sito del comando <code>fsck</code> en HDFS.</li> </ol>"},{"location":"ut1-introduccion-a-hadoop/tareas/hadoop/hdfs/aws/3bloques/#conclusion","title":"Conclusi\u00f3n:","text":"<p>Esta pr\u00e1ctica te ha permitido descargar, manipular y subir archivos a HDFS, adem\u00e1s de verificar la integridad y el estado de los bloques en un cl\u00faster de Hadoop. La comprensi\u00f3n de c\u00f3mo se gestiona un archivo en HDFS es esencial para trabajar eficientemente con grandes vol\u00famenes de datos.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/1hdfs/","title":"Comprendiendo HDFS: Una Gu\u00eda Completa","text":""},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/1hdfs/#que-es-hdfs","title":"\ud83d\ude80 \u00bfQu\u00e9 es HDFS?","text":"<p>HDFS significa Hadoop Distributed File System (Sistema de Archivos Distribuido de Hadoop), y es la columna vertebral de la capacidad de Hadoop para manejar grandes cantidades de datos. Se origin\u00f3 a partir de GFS (Google File System) y est\u00e1 dise\u00f1ado para almacenar y gestionar grandes vol\u00famenes de datos de manera eficiente, distribuy\u00e9ndolos a trav\u00e9s de varios nodos en un cl\u00faster.</p> <p>HDFS proporciona tolerancia a fallos y acceso de alto rendimiento a grandes conjuntos de datos, lo que lo hace ideal para aplicaciones que manejan procesamiento a gran escala, como miner\u00eda de datos, aprendizaje autom\u00e1tico y an\u00e1lisis.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/1hdfs/#arquitectura-de-hdfs-nodos-y-tipos","title":"\ud83e\udde9 Arquitectura de HDFS: Nodos y Tipos","text":"<p>La arquitectura de HDFS gira en torno a dos componentes principales:</p> <ol> <li>NameNode: El nodo maestro que controla los metadatos de los archivos y la estructura del directorio.</li> <li>DataNodes: Estos son los nodos trabajadores responsables de almacenar los datos reales en bloques. Cada bloque tiene un tama\u00f1o predeterminado de 128 MB, pero este valor es configurable.</li> </ol> <pre><code>graph TD;\n    NameNode --&gt;|Metadatos| DataNode1;\n    NameNode --&gt;|Metadatos| DataNode2;\n    NameNode --&gt;|Metadatos| DataNode3;\n    DataNode1 --&gt; Block1;\n    DataNode2 --&gt; Block2;\n    DataNode3 --&gt; Block3;</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/1hdfs/#tipos-clave-de-nodos","title":"Tipos Clave de Nodos:","text":"<ul> <li>NameNode (Maestro): Almacena metadatos de los archivos (como nombres, permisos, ubicaci\u00f3n de los bloques).</li> <li>DataNode (Trabajador): Almacena bloques de datos y env\u00eda se\u00f1ales de estado (heartbeat) al NameNode para confirmar que est\u00e1 activo.</li> </ul> <p>HDFS sigue una pol\u00edtica de replicaci\u00f3n para mantener la integridad de los datos, donde cada bloque se replica en varios DataNodes para garantizar la redundancia.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/1hdfs/#como-funciona-hdfs","title":"\ud83d\udee0 C\u00f3mo Funciona HDFS:","text":""},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/1hdfs/#proceso-de-escritura","title":"Proceso de Escritura \u270f\ufe0f","text":"<p>Cuando un cliente escribe un archivo en HDFS:</p> <ol> <li>El archivo se divide en bloques (fragmentos de datos) y se distribuye en m\u00faltiples DataNodes.</li> <li>El NameNode almacena los metadatos sobre qu\u00e9 bloques pertenecen al archivo y en qu\u00e9 DataNodes est\u00e1n almacenados.</li> <li>Cada bloque se replica (t\u00edpicamente tres veces) en diferentes DataNodes para asegurar la tolerancia a fallos.</li> </ol> <pre><code>sequenceDiagram\n    participant Cliente\n    participant NameNode\n    participant DataNode1\n    participant DataNode2\n    participant DataNode3\n\n    Cliente-&gt;&gt;NameNode: Solicitud para escribir archivo\n    NameNode-&gt;&gt;Cliente: Informaci\u00f3n de los bloques\n    Cliente-&gt;&gt;DataNode1: Escribir Bloque 1\n    DataNode1-&gt;&gt;DataNode2: Replicar Bloque 1\n    DataNode2-&gt;&gt;DataNode3: Replicar Bloque 1</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/1hdfs/#proceso-de-lectura","title":"Proceso de Lectura \ud83d\udcd6","text":"<p>Cuando un cliente lee un archivo de HDFS:</p> <ol> <li>El cliente solicita al NameNode la ubicaci\u00f3n de los bloques.</li> <li>El cliente recupera los bloques de datos directamente de los DataNodes.</li> <li>Los datos se ensamblan nuevamente en el archivo original.</li> </ol> <pre><code>sequenceDiagram\n    participant Cliente\n    participant NameNode\n    participant DataNode1\n    participant DataNode2\n    participant DataNode3\n\n    Cliente-&gt;&gt;NameNode: Solicitud para leer archivo\n    NameNode-&gt;&gt;Cliente: Ubicaci\u00f3n de los bloques\n    Cliente-&gt;&gt;DataNode1: Leer Bloque 1\n    Cliente-&gt;&gt;DataNode2: Leer Bloque 2</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/1hdfs/#ejemplo-subida-y-descarga-de-un-archivo","title":"\ud83d\udcc2 Ejemplo: Subida y Descarga de un Archivo","text":""},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/1hdfs/#subida-de-un-archivo-500-mb","title":"Subida de un Archivo (500 MB)","text":"<p>El archivo de 500 MB se divide en bloques de 128 MB. Debido a la configuraci\u00f3n predeterminada de HDFS, los bloques se replican en varios DataNodes para garantizar la tolerancia a fallos. En este caso, el archivo se dividir\u00e1 en 4 bloques (3 bloques de 128 MB y 1 de 116 MB).</p> <pre><code>sequenceDiagram\n    participant Cliente\n    participant NameNode\n    participant DataNode1\n    participant DataNode2\n    participant DataNode3\n\n    Cliente-&gt;&gt;NameNode: Solicitud para subir archivo (500 MB)\n    NameNode-&gt;&gt;Cliente: Informaci\u00f3n de bloques y nodos asignados\n\n    Cliente-&gt;&gt;DataNode1: Escribir Bloque 1 (128 MB)\n    Cliente-&gt;&gt;DataNode2: Escribir Bloque 2 (128 MB)\n    Cliente-&gt;&gt;DataNode3: Escribir Bloque 3 (128 MB)\n    Cliente-&gt;&gt;DataNode1: Escribir Bloque 4 (116 MB)\n\n    DataNode1-&gt;&gt;DataNode2: Replicar Bloque 1\n    DataNode2-&gt;&gt;DataNode3: Replicar Bloque 2\n    DataNode3-&gt;&gt;DataNode1: Replicar Bloque 3</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/1hdfs/#descarga-de-un-archivo-500-mb","title":"Descarga de un Archivo (500 MB)","text":"<p>Para leer el archivo, el NameNode le indica al cliente d\u00f3nde se encuentran almacenados los bloques. El cliente recupera los bloques directamente desde los DataNodes y los reensambla para formar el archivo completo.</p> <pre><code>sequenceDiagram\n    participant Cliente\n    participant NameNode\n    participant DataNode1\n    participant DataNode2\n    participant DataNode3\n\n    Cliente-&gt;&gt;NameNode: Solicitud para descargar archivo\n    NameNode-&gt;&gt;Cliente: Ubicaci\u00f3n de los bloques en los DataNodes\n\n    Cliente-&gt;&gt;DataNode1: Descargar Bloque 1 (128 MB)\n    Cliente-&gt;&gt;DataNode2: Descargar Bloque 2 (128 MB)\n    Cliente-&gt;&gt;DataNode3: Descargar Bloque 3 (128 MB)\n    Cliente-&gt;&gt;DataNode1: Descargar Bloque 4 (116 MB)</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/1hdfs/#distribucion-de-los-bloques","title":"Distribuci\u00f3n de los Bloques","text":"<p>En el diagrama, el proceso muestra c\u00f3mo el archivo de 500 MB se divide en 4 bloques, con los bloques replicados en los diferentes DataNodes para garantizar la disponibilidad y redundancia en caso de fallos.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/1hdfs/#ventajas-de-hdfs","title":"Ventajas de HDFS:","text":"<ul> <li>Escalabilidad: Se puede escalar f\u00e1cilmente a\u00f1adiendo m\u00e1s nodos.</li> <li>Tolerancia a fallos: Se recupera autom\u00e1ticamente de fallos de nodos debido a la replicaci\u00f3n de bloques.</li> <li>Alto rendimiento: Optimizado para procesamiento por lotes, lo que permite un alto rendimiento de datos.</li> </ul> <p>HDFS es una parte clave del ecosistema de Hadoop y es esencial para gestionar eficientemente grandes conjuntos de datos.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/2mapreduce/","title":"\ud83d\udda5\ufe0f MapReduce: Procesamiento Distribuido Eficiente en Grandes Vol\u00famenes de Datos","text":""},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/2mapreduce/#que-es-mapreduce","title":"\ud83d\ude80 \u00bfQu\u00e9 es MapReduce?","text":"<p>MapReduce es un modelo de programaci\u00f3n dise\u00f1ado para procesar grandes vol\u00famenes de datos de forma distribuida en cl\u00fasteres de servidores. Fue desarrollado por Google como parte de su infraestructura para manejar y analizar cantidades masivas de informaci\u00f3n en Internet.</p> <p>El modelo MapReduce se compone principalmente de dos fases:</p> <ul> <li>Map: divide una tarea grande en sub-tareas m\u00e1s peque\u00f1as, asign\u00e1ndolas a nodos en el cl\u00faster. Por ejemplo, si se quiere contar el n\u00famero de palabras en un texto, el nodo Map puede dividir el texto en palabras y emitir un par clave-valor <code>(palabra, 1)</code> para cada una de ellas.</li> <li>Reduce: toma los resultados de las sub-tareas y los combina en un resultado final. En el caso de contar palabras, el nodo Reduce puede sumar todos los valores de una palabra determinada y emitir un par clave-valor <code>(palabra, total)</code>.</li> </ul> <p>Es ampliamente utilizado en sistemas distribuidos como Apache Hadoop, donde permite el procesamiento de petabytes de datos de manera eficiente.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/2mapreduce/#como-funciona-mapreduce","title":"\ud83e\udde9 \u00bfC\u00f3mo Funciona MapReduce?","text":"<ol> <li> <p>Map: Los datos de entrada son divididos en fragmentos y procesados en paralelo por diferentes nodos. Cada nodo aplica una funci\u00f3n de mapeo para generar pares clave-valor intermedios. Por ejemplo, si se quiere contar el n\u00famero de palabras en un texto, el nodo Map puede dividir el texto en palabras y emitir un par clave-valor <code>(palabra, 1)</code> para cada una de ellas.</p> </li> <li> <p>Shuffle &amp; Sort: Los resultados de la fase Map son organizados y agrupados por clave. Este proceso permite que todos los valores relacionados con una clave se env\u00eden al mismo nodo en la fase de Reduce.</p> </li> <li> <p>Reduce: El nodo ejecuta una funci\u00f3n que combina los valores asociados con la misma clave, generando un conjunto de resultados finales. En el caso de contar palabras, el nodo Reduce puede sumar todos los valores de una palabra determinada y emitir un par clave-valor <code>(palabra, total)</code>.</p> </li> </ol>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/2mapreduce/#estructura-de-un-programa-mapreduce","title":"\ud83d\udee0\ufe0f Estructura de un Programa MapReduce","text":"<p>Un programa MapReduce t\u00edpico se compone de dos funciones clave: map y reduce.</p> <pre><code>// Funci\u00f3n MAP\nfunction map(key, value) {\n  // Divide el valor de entrada y emite pares clave-valor\n  const words = value.split(' ');\n  for (const word of words) {\n    emit(word, 1);\n  }\n}\n\n// Funci\u00f3n REDUCE\nfunction reduce(key, values) {\n  // Suma todos los valores para cada clave\n  const sum = values.reduce((a, b) =&gt; a + b, 0);\n  emit(key, sum);\n}\n</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/2mapreduce/#diagrama-de-flujo-de-un-programa-mapreduce","title":"\ud83d\udcca Diagrama de Flujo de un Programa MapReduce","text":"<pre><code>graph TD;\n    A[Datos de Entrada] --&gt;|Fragmentar| B[Fase Map]\n    B --&gt; C[Shuffle &amp; Sort]\n    C --&gt; D[Fase Reduce]\n    D --&gt; E[Resultado Final]\n\n    B --&gt;|Claves y Valores Intermedios| C\n    C --&gt;|Agrupaci\u00f3n por Clave| D</code></pre> <p>Este diagrama ilustra c\u00f3mo los datos de entrada son primero procesados por la fase Map, luego se organizan por clave en Shuffle &amp; Sort, y finalmente se combinan en la fase Reduce para generar el resultado final.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/2mapreduce/#ejemplos-populares-de-mapreduce","title":"\ud83c\udf93 Ejemplos Populares de MapReduce","text":""},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/2mapreduce/#1-word-count-conteo-de-palabras","title":"1. Word Count (Conteo de Palabras)","text":"<p>Este programa MapReduce cuenta cu\u00e1ntas veces aparece cada palabra en un conjunto de documentos.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/2mapreduce/#fase-map","title":"Fase Map","text":"<pre><code>function map(key, value) {\n  const words = value.split(' ');\n  for (const word of words) {\n    emit(word, 1);  // (clave, valor) =&gt; (palabra, 1)\n  }\n}\n</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/2mapreduce/#fase-reduce","title":"Fase Reduce","text":"<pre><code>function reduce(key, values) {\n  const total = values.reduce((a, b) =&gt; a + b, 0);\n  emit(key, total);  // (clave, valor) =&gt; (palabra, total)\n}\n</code></pre> <pre><code>graph TD;\n    Texto[Documento de Entrada] --&gt;|Fragmentar en Palabras| Map[Funci\u00f3n Map]\n    Map --&gt; Intermedio[Claves y Valores Intermedios]\n    Intermedio --&gt; Shuffle[Shuffle &amp; Sort]\n    Shuffle --&gt; Reduce[Funci\u00f3n Reduce]\n    Reduce --&gt; Resultado[Frecuencia de Palabras]</code></pre> <p>Explicaci\u00f3n:</p> <ul> <li>La fase Map emite pares clave-valor <code>(palabra, 1)</code> para cada palabra en el texto.</li> <li>La fase Shuffle agrupa todas las instancias de la misma palabra.</li> <li>La fase Reduce suma todas las ocurrencias de una palabra y emite el resultado final.</li> </ul>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/2mapreduce/#2-sumar-ventas-por-producto","title":"2. Sumar Ventas por Producto","text":"<p>Este ejemplo muestra c\u00f3mo sumar el total de ventas por producto a partir de un conjunto de registros de ventas.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/2mapreduce/#datos-de-entrada","title":"Datos de Entrada","text":"<pre><code>ProductoA, 20\nProductoB, 15\nProductoA, 10\nProductoC, 30\nProductoB, 5\n</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/2mapreduce/#fase-map_1","title":"Fase Map","text":"<p>La funci\u00f3n <code>map</code> toma cada l\u00ednea de ventas y emite un par clave-valor donde la clave es el nombre del producto y el valor es la venta.</p> <pre><code>function map(key, value) {\n  const [product, sales] = value.split(', ');\n  emit(product, parseInt(sales));  // (clave, valor) =&gt; (producto, ventas)\n}\n</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/2mapreduce/#fase-reduce_1","title":"Fase Reduce","text":"<p>La funci\u00f3n <code>reduce</code> toma todas las ventas de un producto en particular y las suma.</p> <pre><code>function reduce(key, values) {\n  const totalSales = values.reduce((a, b) =&gt; a + b, 0);\n  emit(key, totalSales);  // (clave, valor) =&gt; (producto, ventas_totales)\n}\n</code></pre> <pre><code>graph TD;\n    Ventas[Datos de Ventas] --&gt;|Leer Producto y Venta| Map[Funci\u00f3n Map]\n    Map --&gt; Intermedio[Claves: Producto, Valores: Ventas]\n    Intermedio --&gt; Shuffle[Shuffle &amp; Sort]\n    Shuffle --&gt; Reduce[Funci\u00f3n Reduce]\n    Reduce --&gt; Resultado[Ventas Totales por Producto]</code></pre> <p>Explicaci\u00f3n:</p> <ul> <li>La fase Map toma cada l\u00ednea de ventas y emite pares clave-valor donde la clave es el producto y el valor es la venta.</li> <li>La fase Shuffle agrupa todas las ventas de cada producto.</li> <li>La fase Reduce suma las ventas totales para cada producto y emite el resultado final.</li> </ul>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/2mapreduce/#por-que-es-importante-mapreduce","title":"\ud83d\udca1 \u00bfPor qu\u00e9 es Importante MapReduce?","text":"<p>MapReduce es fundamental para el procesamiento de grandes cantidades de datos debido a su capacidad para:</p> <ul> <li>Escalabilidad: Procesar datos en paralelo a trav\u00e9s de m\u00faltiples servidores.</li> <li>Tolerancia a fallos: Maneja autom\u00e1ticamente los errores en los nodos, replicando datos para asegurar su disponibilidad.</li> <li>Simplicidad: Permite a los desarrolladores escribir programas para grandes vol\u00famenes de datos sin preocuparse por la infraestructura subyacente.</li> </ul> <p>\u00a1Con MapReduce, el procesamiento distribuido de grandes datos se vuelve accesible y eficiente! \ud83c\udf89</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/","title":"\u00cdndice de pr\u00e1cticas y tareas","text":"<p>Este \u00edndice incluye todas las pr\u00e1cticas guiadas y tareas correspondientes al curso. Aseg\u00farate de seguir las instrucciones cuidadosamente para cada pr\u00e1ctica y tarea, y consulta los materiales de apoyo cuando sea necesario.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/#practicas-guiadas","title":"Pr\u00e1cticas guiadas","text":"<p>Las pr\u00e1cticas guiadas est\u00e1n dise\u00f1adas para que sigas un conjunto de instrucciones paso a paso y te familiarices con el entorno y las herramientas utilizadas en Big Data. Cada pr\u00e1ctica cubre un aspecto clave del entorno de Hadoop y su ecosistema.</p> <ul> <li> <p>WORDCOUNT en Hadoop con MapReduce</p> <ul> <li>Descripci\u00f3n: En esta pr\u00e1ctica aprender\u00e1s a ejecutar el ejemplo cl\u00e1sico de WordCount utilizando Hadoop MapReduce. El objetivo es procesar un archivo de texto para contar la cantidad de veces que aparece cada palabra.</li> <li>Objetivos:<ul> <li>Subir un archivo de texto a HDFS.</li> <li>Ejecutar un trabajo de MapReduce para contar palabras.</li> <li>Verificar la salida del proceso en HDFS.</li> </ul> </li> </ul> </li> <li> <p>An\u00e1lisis de costos por categor\u00eda con MapReduce en Python</p> <ul> <li>Descripci\u00f3n: En esta pr\u00e1ctica aprender\u00e1s a implementar y ejecutar un ejemplo de MapReduce utilizando Python para analizar datos de compras. El objetivo es procesar un archivo de transacciones, agrupar las ventas por tipo de producto y calcular el total de ventas para cada categor\u00eda.</li> <li>Objetivos:<ul> <li>Descargar el proyecto de ejemplo desde Google Drive.</li> <li>Ejecutar un script de shell para descargar los datos de prueba.</li> <li>Explicar el funcionamiento de los archivos <code>mapper.py</code> y <code>reducer.py</code>.</li> <li>Ejecutar el <code>mapper.py</code> para observar la salida intermedia.</li> <li>Ejecutar <code>mapper.py</code> y <code>reducer.py</code> juntos para obtener el total de ventas por categor\u00eda.</li> </ul> </li> </ul> </li> <li> <p>An\u00e1lisis de Visualizaci\u00f3n en Netflix con MapReduce en Python</p> <ul> <li>Descripci\u00f3n: En esta tarea, desarrollar\u00e1s un programa en Python utilizando el paradigma de MapReduce para analizar los datos de visualizaci\u00f3n en Netflix. El objetivo es identificar los d\u00edas de la semana y las franjas horarias con mayor n\u00famero de visualizaciones.</li> <li>Objetivos:<ul> <li>Implementar una funci\u00f3n <code>map</code> que procese las columnas <code>day_of_week</code> y <code>time_of_day</code>.</li> <li>Implementar una funci\u00f3n <code>reduce</code> que acumule el n\u00famero total de visualizaciones para cada combinaci\u00f3n de d\u00eda y franja horaria.</li> <li>Ejecutar el programa para obtener un resumen del patr\u00f3n de visualizaci\u00f3n.</li> <li>Interpretar los resultados y explicar qu\u00e9 d\u00edas y horarios tienen m\u00e1s visualizaciones.</li> </ul> </li> </ul> </li> </ul>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/category-cost-python-sample/","title":"Category cost python sample","text":""},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/category-cost-python-sample/#practica-de-procesamiento-de-datos-con-mapreduce-en-python","title":"Pr\u00e1ctica de Procesamiento de Datos con MapReduce en Python","text":""},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/category-cost-python-sample/#objetivo-de-la-practica","title":"Objetivo de la Pr\u00e1ctica","text":"<p>En esta pr\u00e1ctica, los alumnos aprender\u00e1n a utilizar un proyecto b\u00e1sico de MapReduce en Python, ejecutar scripts de shell, y entender c\u00f3mo funcionan los archivos <code>mapper.py</code> y <code>reducer.py</code> para realizar un procesamiento de datos sencillo. Al finalizar, los estudiantes ser\u00e1n capaces de analizar datos utilizando comandos de Linux y comprender el flujo de datos en un sistema MapReduce.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/category-cost-python-sample/#1-descargar-el-proyecto","title":"1. Descargar el Proyecto","text":"<ul> <li>Acci\u00f3n: Descarga el proyecto desde el siguiente enlace de Google Drive.</li> <li>Enlace del proyecto: Proyecto en Google Drive</li> <li>Objetivo: Obtener los archivos necesarios para realizar la pr\u00e1ctica, que incluyen el c\u00f3digo del Mapper y Reducer, un archivo <code>download_data.sh</code> y un archivo de datos <code>purchases.txt</code> en el que se basar\u00e1 el procesamiento.</li> </ul>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/category-cost-python-sample/#2-ejecutar-el-script-para-descargar-el-archivo-de-prueba","title":"2. Ejecutar el Script para Descargar el Archivo de Prueba","text":"<ul> <li>Acci\u00f3n: Desde la terminal, navega a la carpeta donde descargaste el proyecto y ejecuta el siguiente comando:   <pre><code>bash download_data.sh\n</code></pre></li> <li>Objetivo: Descargar el archivo <code>purchases.txt</code> que contiene datos de prueba de transacciones de compras. Este archivo se encuentra en la carpeta <code>data/</code>.</li> </ul>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/category-cost-python-sample/#3-explicar-el-funcionamiento-del-archivo-mapperpy","title":"3. Explicar el Funcionamiento del Archivo <code>mapper.py</code>","text":"<ul> <li>Tarea: Abre el archivo <code>mapper.py</code> y analiza su contenido.</li> <li>Instrucciones:</li> <li>Lee el c\u00f3digo para entender c\u00f3mo el Mapper procesa cada l\u00ednea de entrada.</li> <li>Describe el prop\u00f3sito de cada parte del c\u00f3digo.</li> <li>Explica c\u00f3mo el Mapper convierte la l\u00ednea de entrada en un par <code>clave-valor</code>, generalmente extrayendo informaci\u00f3n como el nombre del producto y su precio.</li> </ul>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/category-cost-python-sample/#4-explicar-el-funcionamiento-del-archivo-reducerpy","title":"4. Explicar el Funcionamiento del Archivo <code>reducer.py</code>","text":"<ul> <li>Tarea: Abre el archivo <code>reducer.py</code> y analiza su contenido.</li> <li>Instrucciones:</li> <li>Lee el c\u00f3digo para comprender c\u00f3mo el Reducer agrupa y procesa los datos de salida generados por el Mapper.</li> <li>Describe c\u00f3mo el Reducer suma el total de ventas para cada tipo de producto.</li> <li>Explica el uso de variables y estructuras en el c\u00f3digo.</li> </ul>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/category-cost-python-sample/#5-ejecutar-el-mapper-con-el-archivo-de-datos","title":"5. Ejecutar el Mapper con el Archivo de Datos","text":"<ul> <li>Acci\u00f3n: Ejecuta el siguiente comando desde la terminal para procesar los datos usando el Mapper:   <pre><code>cat data/purchases.txt | python2 mapper.py\n</code></pre></li> <li>Objetivo: Verificar que el Mapper est\u00e1 funcionando correctamente y analizar su salida.</li> <li>Tarea:</li> <li>Observa la salida en la terminal y anota qu\u00e9 representa.</li> <li>Explica por qu\u00e9 cada l\u00ednea de salida del Mapper contiene una clave (tipo de producto) y un valor (costo).</li> </ul>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/category-cost-python-sample/#6-ejecutar-mapper-y-reducer-juntos-y-explicar-la-salida","title":"6. Ejecutar Mapper y Reducer Juntos y Explicar la Salida","text":"<ul> <li>Acci\u00f3n: Ejecuta el siguiente comando para procesar los datos con el Mapper y el Reducer combinados:   <pre><code>cat data/purchases.txt | python2 mapper.py | python2 reducer.py\n</code></pre></li> <li>Objetivo: Procesar los datos completamente y obtener el total de ventas por cada tipo de producto.</li> <li>Tarea:</li> <li>Observa la salida en la terminal, que debe mostrar el total de ventas para cada categor\u00eda de producto.</li> <li>Explica c\u00f3mo el Reducer acumula las ventas y por qu\u00e9 cada l\u00ednea de salida representa un total de ventas por categor\u00eda de producto.</li> </ul>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/category-cost-python-sample/#resultados-esperados","title":"Resultados Esperados","text":"<ul> <li>Comprensi\u00f3n de los Conceptos: Los estudiantes deben ser capaces de explicar c\u00f3mo funciona MapReduce y c\u00f3mo el Mapper y el Reducer procesan los datos en etapas.</li> <li>Aplicaci\u00f3n Pr\u00e1ctica: Ejecutar los comandos para ver c\u00f3mo el flujo de datos pasa de un Mapper a un Reducer, simulando un sistema distribuido simple de procesamiento de datos.</li> </ul>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/category-cost-python-sample/#preguntas","title":"Preguntas","text":"<ol> <li>\u00bfQu\u00e9 sucede si una l\u00ednea en <code>purchases.txt</code> tiene un formato incorrecto? \u00bfC\u00f3mo lo maneja el Mapper?</li> <li>\u00bfPor qu\u00e9 es importante verificar el formato de los datos en el Mapper?</li> <li>\u00bfQu\u00e9 modificaciones podr\u00edas hacer al Reducer para agregar m\u00e1s funcionalidad, como calcular el promedio de ventas por categor\u00eda?</li> </ol>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/hashtags-tweets/","title":"Hashtags tweets","text":""},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/hashtags-tweets/#enunciado-para-ejercicio-de-mapreduce","title":"Enunciado para Ejercicio de MapReduce","text":"<p>En este ejercicio, los estudiantes deber\u00e1n implementar una serie de operaciones de MapReduce sobre un archivo CSV de tweets, analizando y extrayendo informaci\u00f3n de inter\u00e9s a partir de grandes vol\u00famenes de datos.</p> <p>Cada estudiante deber\u00e1 responder a las siguientes preguntas de an\u00e1lisis, ejecutando tareas de MapReduce para cada una.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/hashtags-tweets/#formato-del-archivo-csv","title":"Formato del archivo CSV","text":"<p>El archivo <code>tweets_data.csv</code> tiene el siguiente formato de columnas y datos de ejemplo:</p> <pre><code>username;tweet_content;date_written;likes;retweets;replies;location;source\n@carlos_fernandez;La ciencia de datos est\u00e1 cambiando el mundo. #CienciaDeDatos;2024-10-18 16:23:16;182;49;90;Barcelona;Twitter para iPhone\n@carlos_fernandez;Aprendiendo m\u00e1s sobre Python cada d\u00eda.;2024-10-17 16:23:16;382;238;24;M\u00e1laga;Twitter para iPhone\n@carlos_fernandez;Aprendiendo m\u00e1s sobre Python cada d\u00eda.;2024-10-19 16:23:16;285;91;0;Sevilla;Twitter para iPhone\n@maria_garcia;La ciencia de datos est\u00e1 cambiando el mundo. #Python;2024-10-22 16:23:16;76;92;29;M\u00e1laga;Twitter Web App\n@carlos_fernandez;Explorando nuevos modelos de IA. #IA;2024-10-12 16:23:16;56;54;89;Bilbao;Twitter para iPhone\n@maria_garcia;La ciencia de datos est\u00e1 cambiando el mundo. #IA;2024-10-26 16:23:16;36;220;60;Granada;Twitter para iPhone\n@lucia_martinez;Los desaf\u00edos de programaci\u00f3n me mantienen afilado.;2024-10-20 16:23:16;303;155;78;Sevilla;Twitter para Android\n@laura_rodriguez;La ciencia de datos est\u00e1 cambiando el mundo.;2024-10-02 16:23:16;387;242;60;Barcelona;Twitter para iPhone\n@laura_rodriguez;Los desaf\u00edos de programaci\u00f3n me mantienen afilado.;2024-10-19 16:23:16;191;231;35;Bilbao;Twitter para iPhone\n@laura_rodriguez;\u00a1Acabo de terminar una gran sesi\u00f3n de codificaci\u00f3n!;2024-10-25 16:23:16;44;48;20;Granada;Twitter Web App\n</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/hashtags-tweets/#tareas-de-mapreduce","title":"Tareas de MapReduce","text":"<ol> <li>Contar cu\u00e1ntas veces aparece cada hashtag</li> <li> <p>Determinar la frecuencia de cada hashtag en el conjunto de tweets para identificar cu\u00e1les son los temas m\u00e1s populares.</p> </li> <li> <p>Contar cu\u00e1ntos tweets ha escrito cada usuario</p> </li> <li> <p>Obtener el n\u00famero de tweets por cada usuario registrado en el archivo, analizando su nivel de actividad en la plataforma.</p> </li> <li> <p>An\u00e1lisis de popularidad de tweets (likes + retweets)</p> </li> <li> <p>Calcular la popularidad de cada tweet considerando la suma de \"likes\" y \"retweets\".</p> </li> <li> <p>Tweets por ubicaci\u00f3n geogr\u00e1fica</p> </li> <li> <p>Determinar el n\u00famero de tweets enviados desde cada ubicaci\u00f3n para observar las \u00e1reas de mayor actividad.</p> </li> <li> <p>Promedio de interacciones (likes, retweets y replies) por usuario</p> </li> <li> <p>Calcular el promedio de interacciones (considerando \"likes\", \"retweets\" y \"replies\") que recibe cada usuario en sus tweets.</p> </li> <li> <p>Tweets diarios</p> </li> <li>Contabilizar la cantidad de tweets publicados por d\u00eda, analizando la actividad en diferentes fechas.</li> </ol> <p>Cada pregunta representa una tarea de MapReduce. Para cada tarea, los estudiantes deben: 1. Implementar el c\u00f3digo MapReduce. 2. Documentar el proceso. 3. Analizar los resultados obtenidos para identificar patrones o insights.</p> <p>Este ejercicio permitir\u00e1 a los estudiantes comprender y practicar las t\u00e9cnicas de MapReduce aplicadas a un dataset real de redes sociales.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/","title":"Wordcount sample","text":"<p>Aqu\u00ed tienes la pr\u00e1ctica actualizada, incluyendo el comando para visualizar todas las partes del archivo de salida juntas:</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/#practica-ejecucion-del-ejemplo-wordcount-en-hadoop-con-mapreduce","title":"Pr\u00e1ctica: Ejecuci\u00f3n del ejemplo WordCount en Hadoop con MapReduce","text":"<p>En esta pr\u00e1ctica, vas a ejecutar el ejemplo cl\u00e1sico de WordCount utilizando Hadoop MapReduce para contar el n\u00famero de veces que aparece cada palabra en un archivo de texto. Seguir\u00e1s los siguientes pasos para crear un archivo, cargarlo en HDFS, y ejecutar un trabajo de MapReduce que contar\u00e1 las palabras.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/#objetivo","title":"Objetivo:","text":"<p>Familiarizarse con los comandos b\u00e1sicos de HDFS y MapReduce en Hadoop, ejecutar un trabajo de ejemplo y obtener los resultados.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/#requisitos","title":"Requisitos:","text":"<ul> <li>Un cl\u00faster de Hadoop corriendo (local o en la nube, como EMR de AWS).</li> <li>Acceso a la terminal de la m\u00e1quina maestra del cl\u00faster.</li> </ul>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/#implementacion-del-ejemplo","title":"Implementaci\u00f3n del ejemplo:","text":"<p>El c\u00f3digo Java que da lugar al ejemplo que se utiliza en esta pr\u00e1ctica est\u00e1 disponible en WordCount.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/#pasos-a-seguir","title":"Pasos a seguir:","text":""},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/#1-crear-el-archivo-de-texto-de-entrada","title":"1. Crear el archivo de texto de entrada","text":"<p>Primero, crea un archivo de texto en la m\u00e1quina local que contenga el texto que quieres procesar:</p> <pre><code>echo -e \"Hadoop is a distributed system\\nHadoop is scalable\\nHadoop can handle big data\" &gt; word_count.txt\n</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/#2-verificar-el-contenido-del-archivo","title":"2. Verificar el contenido del archivo","text":"<p>Aseg\u00farate de que el archivo contiene el texto deseado ejecutando:</p> <pre><code>cat word_count.txt\n</code></pre> <p>La salida deber\u00eda mostrar el siguiente contenido:</p> <pre><code>Hadoop is a distributed system\nHadoop is scalable\nHadoop can handle big data\n</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/#3-crear-un-directorio-en-hdfs-para-almacenar-el-archivo-de-entrada","title":"3. Crear un directorio en HDFS para almacenar el archivo de entrada","text":"<p>Crea un directorio en HDFS donde almacenar\u00e1s el archivo de entrada:</p> <pre><code>hdfs dfs -mkdir /user/hadoop/input\n</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/#4-subir-el-archivo-de-entrada-a-hdfs","title":"4. Subir el archivo de entrada a HDFS","text":"<p>Sube el archivo <code>word_count.txt</code> a HDFS para que est\u00e9 disponible para el procesamiento:</p> <pre><code>hdfs dfs -put word_count.txt /user/hadoop/input/\n</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/#5-ejecutar-el-trabajo-de-mapreduce-para-wordcount","title":"5. Ejecutar el trabajo de MapReduce para WordCount","text":"<p>Ejecuta el trabajo de MapReduce utilizando el ejemplo de WordCount. Este comando toma el archivo de entrada desde HDFS, lo procesa, y genera un archivo de salida con el conteo de palabras.</p> <pre><code>hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount /user/hadoop/input/word_count.txt /user/hadoop/output\n</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/#6-verificar-la-salida-en-hdfs","title":"6. Verificar la salida en HDFS","text":"<p>El trabajo de MapReduce puede generar varios archivos de salida (<code>part-r-00000</code>, <code>part-r-00001</code>, etc.). Para visualizar todas las partes juntas y concatenarlas en un archivo local, ejecuta el siguiente comando:</p> <pre><code>hdfs dfs -cat /user/hadoop/output/part-r-* &gt; combined_output.txt\n</code></pre> <p>Este comando toma todas las partes del archivo de salida (por ejemplo, <code>part-r-00000</code>, <code>part-r-00001</code>, etc.) y las guarda en un archivo llamado <code>combined_output.txt</code> en el sistema local.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/#7-verificar-el-archivo-de-salida-combinado","title":"7. Verificar el archivo de salida combinado","text":"<p>Una vez concatenado, puedes verificar el contenido del archivo <code>combined_output.txt</code>:</p> <pre><code>cat combined_output.txt\n</code></pre> <p>Deber\u00edas ver la salida con el conteo de palabras, por ejemplo:</p> <pre><code>Hadoop    3\nis        2\na         1\ndistributed 1\nsystem    1\nscalable  1\ncan       1\nhandle    1\nbig       1\ndata      1\n</code></pre>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/#8-copiar-el-archivo-de-salida-a-la-maquina-local-opcional","title":"8. Copiar el archivo de salida a la m\u00e1quina local (opcional)","text":"<p>Si prefieres obtener las partes del archivo de salida por separado y copiarlas desde HDFS a tu sistema local:</p> <pre><code>hdfs dfs -get /user/hadoop/output/part-r-00000 .\n</code></pre> <p>Repite el proceso para todas las partes generadas (por ejemplo, <code>part-r-00001</code>, <code>part-r-00002</code>, etc.), o concat\u00e9nalas usando el paso anterior.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/#conclusion","title":"Conclusi\u00f3n:","text":"<p>En esta pr\u00e1ctica has aprendido a:</p> <ul> <li>Crear un archivo de texto en la m\u00e1quina local.</li> <li>Subir un archivo a HDFS.</li> <li>Ejecutar un trabajo de MapReduce en Hadoop.</li> <li>Concatenar los resultados de salida generados en m\u00faltiples partes utilizando <code>hdfs dfs -cat</code> para visualizar todos los resultados juntos.</li> </ul> <p>Con estos pasos, te has familiarizado con el flujo b\u00e1sico de trabajo de MapReduce en Hadoop y la gesti\u00f3n de archivos en HDFS.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/wordcount-sample/#preguntas-para-los-estudiantes","title":"Preguntas para los estudiantes:","text":"<p>Despu\u00e9s de realizar la pr\u00e1ctica, responde a las siguientes preguntas para evaluar tu comprensi\u00f3n del proceso:</p> <ol> <li> <p>\u00bfCu\u00e1ntas partes resultaron del proceso de Reduce?</p> <ul> <li>Pista: Revisa cu\u00e1ntos archivos <code>part-r-0000x</code> se generaron en el directorio de salida de HDFS.</li> </ul> </li> <li> <p>\u00bfQu\u00e9 comando utilizaste para visualizar todas las partes juntas en un solo archivo?</p> <ul> <li>Describe brevemente el uso del comando <code>hdfs dfs -cat</code> y c\u00f3mo se concatenaron las partes de salida.</li> </ul> </li> <li> <p>\u00bfCu\u00e1l es la funci\u00f3n principal de la fase \"Map\" en el proceso de MapReduce?</p> <ul> <li>Explica brevemente qu\u00e9 hace la fase Map y c\u00f3mo genera los pares clave-valor.</li> </ul> </li> <li> <p>\u00bfCu\u00e1l es la funci\u00f3n principal de la fase \"Reduce\" en el proceso de MapReduce?</p> <ul> <li>Describe el objetivo de la fase Reduce y qu\u00e9 sucede con los pares clave-valor generados por el mapper.</li> </ul> </li> <li> <p>\u00bfPor qu\u00e9 el trabajo de MapReduce genera m\u00faltiples archivos de salida (part-r-00000, part-r-00001, etc.)?</p> <ul> <li>Explica por qu\u00e9 los resultados pueden dividirse en varias partes en lugar de generarse en un solo archivo.</li> </ul> </li> <li> <p>\u00bfQu\u00e9 informaci\u00f3n proporciona el archivo <code>_SUCCESS</code> que aparece en el directorio de salida?</p> <ul> <li>Investiga qu\u00e9 indica la presencia de este archivo despu\u00e9s de la ejecuci\u00f3n de un trabajo de MapReduce.</li> </ul> </li> <li> <p>\u00bfQu\u00e9 pasos adicionales ser\u00edan necesarios si quisieras procesar un archivo mucho m\u00e1s grande en lugar de <code>word_count.txt</code>?</p> <ul> <li>Explica si tendr\u00edas que hacer algo diferente en t\u00e9rminos de configuraci\u00f3n de MapReduce o HDFS para procesar un archivo de mayor tama\u00f1o.</li> </ul> </li> </ol>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/netflix/enunciado/","title":"Ejercicio de MapReduce en Python: An\u00e1lisis de D\u00edas y Horarios de Visualizaci\u00f3n en Netflix","text":"<p>Objetivo: El objetivo de este ejercicio es desarrollar un programa en Python que use el paradigma MapReduce para analizar los datos de visualizaci\u00f3n de contenido en Netflix. Queremos identificar los d\u00edas de la semana y las franjas horarias en las que los usuarios realizan m\u00e1s visualizaciones. El programa debe procesar los datos de visualizaci\u00f3n y devolver una tabla con el n\u00famero de visualizaciones agrupadas por d\u00eda de la semana y franja horaria.</p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/netflix/enunciado/#estructura-de-los-datos","title":"Estructura de los Datos","text":"<p>Para este ejercicio, trabajaremos con un archivo de datos CSV que contiene las siguientes columnas relevantes para el an\u00e1lisis:</p> <ul> <li>day_of_week: D\u00eda de la semana en el que se realiz\u00f3 la visualizaci\u00f3n (por ejemplo, \"Monday\", \"Tuesday\", etc.).</li> <li>time_of_day: Franja horaria en la que se realiz\u00f3 la visualizaci\u00f3n (\"Morning\", \"Afternoon\", \"Evening\", \"Night\").</li> </ul> <p>Un ejemplo de fila de datos podr\u00eda verse as\u00ed: <pre><code>day_of_week,time_of_day,other_columns...\nMonday,Evening,...\nTuesday,Morning,...\n...\n</code></pre></p>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/netflix/enunciado/#pasos-para-resolver-el-ejercicio","title":"Pasos para Resolver el Ejercicio","text":"<ol> <li> <p>Funci\u00f3n Map:    La funci\u00f3n <code>map</code> debe leer cada l\u00ednea del archivo de datos y extraer las columnas <code>day_of_week</code> y <code>time_of_day</code>. Por cada entrada, la funci\u00f3n emitir\u00e1 una clave-valor en el formato <code>(day_of_week, time_of_day) -&gt; 1</code>, donde <code>1</code> representa una visualizaci\u00f3n.</p> </li> <li> <p>Funci\u00f3n Reduce:    La funci\u00f3n <code>reduce</code> debe tomar todas las claves <code>(day_of_week, time_of_day)</code> emitidas por la fase de <code>map</code> y sumar los valores asociados para obtener el total de visualizaciones para cada combinaci\u00f3n de d\u00eda y franja horaria.</p> </li> <li> <p>Formato de Salida:    El programa debe generar un informe que muestre las combinaciones de <code>day_of_week</code> y <code>time_of_day</code>, junto con la cantidad total de visualizaciones. El formato de salida puede ser un archivo CSV o una tabla impresa en pantalla con las columnas <code>day_of_week</code>, <code>time_of_day</code> y <code>count</code>.</p> </li> </ol>"},{"location":"ut2-almacenamiento-y-procesamiento-en-hadoop/tareas/mapreduce/netflix/enunciado/#ejemplo-de-salida-esperada","title":"Ejemplo de Salida Esperada","text":"<pre><code>day_of_week, time_of_day, count\nMonday, Evening, 450\nMonday, Morning, 300\nTuesday, Afternoon, 350\n...\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/","title":"Index","text":"<p>Unidad_3_Ecosistema_Hadoop/ \u251c\u2500\u2500 1_Introduccion/ \u2502   \u2514\u2500\u2500 introduccion.md \u251c\u2500\u2500 2_Componentes_Acceso_Procesamiento/ \u2502   \u251c\u2500\u2500 Apache_Pig.md \u2502   \u251c\u2500\u2500 Apache_Hive/ \u2502   \u2502   \u251c\u2500\u2500 conceptos_generales.md \u2502   \u2502   \u251c\u2500\u2500 arquitectura.md \u2502   \u2502   \u2514\u2500\u2500 HQL.md \u2502   \u251c\u2500\u2500 Apache_Impala.md \u2502   \u251c\u2500\u2500 Apache_HBase.md \u2502   \u251c\u2500\u2500 Apache_Phoenix.md \u2502   \u2514\u2500\u2500 Apache_Spark/ \u2502       \u251c\u2500\u2500 arquitectura_componentes.md \u2502       \u251c\u2500\u2500 detalle_componentes.md \u2502       \u2514\u2500\u2500 ventajas_desventajas.md \u251c\u2500\u2500 3_Componentes_Ingesta_Flujos_Trabajo/ \u2502   \u251c\u2500\u2500 Apache_Sqoop.md \u2502   \u251c\u2500\u2500 Apache_Flume.md \u2502   \u2514\u2500\u2500 Apache_Oozie.md \u251c\u2500\u2500 4_Interfaces_Herramientas/ \u2502   \u251c\u2500\u2500 Hue.md \u2502   \u251c\u2500\u2500 Apache_Zeppelin.md \u2502   \u2514\u2500\u2500 Ambari_Cloudera_Manager.md \u2514\u2500\u2500 5_Procesamiento_Streaming/     \u251c\u2500\u2500 Apache_Spark_Structured_Streaming.md     \u251c\u2500\u2500 Apache_Flink.md     \u2514\u2500\u2500 Apache_Storm.md</p>"},{"location":"ut3-ecosistema-hadoop/1introduccion/","title":"Unidad 3: Introducci\u00f3n al Ecosistema Hadoop \ud83d\ude80","text":"<p>Bienvenido a la Unidad 3, donde nos adentraremos en el fascinante ecosistema Hadoop. Esta unidad est\u00e1 dise\u00f1ada para que comprendas las m\u00faltiples herramientas que conforman el sistema Hadoop, permiti\u00e9ndote manejar grandes vol\u00famenes de datos de forma eficiente y a gran escala. \u00bfEst\u00e1s listo? \u00a1Vamos a explorar juntos! \ud83c\udf10</p>"},{"location":"ut3-ecosistema-hadoop/1introduccion/#que-encontraras-en-esta-unidad","title":"\u00bfQu\u00e9 encontrar\u00e1s en esta unidad? \ud83d\udcda","text":"<p>El ecosistema Hadoop es vasto y complejo, compuesto por m\u00faltiples componentes que facilitan la ingesta, almacenamiento, procesamiento y an\u00e1lisis de datos. Aqu\u00ed encontrar\u00e1s una estructura que desglosa cada uno de estos elementos clave, desde los componentes de acceso y procesamiento hasta las herramientas de ingesta y flujo de trabajo, sin olvidar las interfaces y el procesamiento en tiempo real.</p>"},{"location":"ut3-ecosistema-hadoop/1introduccion/#estructura-de-la-unidad-3","title":"Estructura de la Unidad 3:","text":"<pre><code>Unidad_3_Ecosistema_Hadoop/\n\u251c\u2500\u2500 1_Introduccion/\n\u2502   \u2514\u2500\u2500 introduccion.md\n\u251c\u2500\u2500 2_Componentes_Acceso_Procesamiento/\n\u2502   \u251c\u2500\u2500 Apache_Pig.md\n\u2502   \u251c\u2500\u2500 Apache_Hive/\n\u2502   \u2502   \u251c\u2500\u2500 conceptos_generales.md\n\u2502   \u2502   \u251c\u2500\u2500 arquitectura.md\n\u2502   \u2502   \u2514\u2500\u2500 HQL.md\n\u2502   \u251c\u2500\u2500 Apache_Impala.md\n\u2502   \u251c\u2500\u2500 Apache_HBase.md\n\u2502   \u251c\u2500\u2500 Apache_Phoenix.md\n\u2502   \u2514\u2500\u2500 Apache_Spark/\n\u2502       \u251c\u2500\u2500 arquitectura_componentes.md\n\u2502       \u251c\u2500\u2500 detalle_componentes.md\n\u2502       \u2514\u2500\u2500 ventajas_desventajas.md\n\u251c\u2500\u2500 3_Componentes_Ingesta_Flujos_Trabajo/\n\u2502   \u251c\u2500\u2500 Apache_Sqoop.md\n\u2502   \u251c\u2500\u2500 Apache_Flume.md\n\u2502   \u2514\u2500\u2500 Apache_Oozie.md\n\u251c\u2500\u2500 4_Interfaces_Herramientas/\n\u2502   \u251c\u2500\u2500 Hue.md\n\u2502   \u251c\u2500\u2500 Apache_Zeppelin.md\n\u2502   \u2514\u2500\u2500 Ambari_Cloudera_Manager.md\n\u2514\u2500\u2500 5_Procesamiento_Streaming/\n    \u251c\u2500\u2500 Apache_Spark_Structured_Streaming.md\n    \u251c\u2500\u2500 Apache_Flink.md\n    \u2514\u2500\u2500 Apache_Storm.md\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/1introduccion/#que-es-el-ecosistema-hadoop","title":"\u00bfQu\u00e9 es el Ecosistema Hadoop? \ud83d\udc18","text":"<p>Hadoop es mucho m\u00e1s que un simple sistema de almacenamiento distribuido. Este ecosistema nos ofrece un conjunto de herramientas para procesar grandes vol\u00famenes de datos de manera r\u00e1pida y eficiente en entornos de Big Data. Desde el almacenamiento en HDFS hasta el procesamiento en tiempo real, Hadoop es fundamental para gestionar datos en escalas masivas.</p>"},{"location":"ut3-ecosistema-hadoop/1introduccion/#principales-componentes-de-hadoop","title":"Principales componentes de Hadoop","text":"<ol> <li>Almacenamiento Distribuido (HDFS): Permite guardar y distribuir grandes cantidades de datos.</li> <li>Procesamiento (MapReduce): Modelo de programaci\u00f3n para procesar grandes datos.</li> <li>Administraci\u00f3n de Recursos (YARN): Distribuye y organiza los recursos necesarios para la ejecuci\u00f3n de aplicaciones.</li> </ol>"},{"location":"ut3-ecosistema-hadoop/1introduccion/#explorando-cada-seccion","title":"Explorando cada secci\u00f3n \ud83d\udd0d","text":"<p>Cada archivo en esta unidad tiene un prop\u00f3sito y rol espec\u00edfico dentro del ecosistema:</p> <ul> <li>Componentes de Acceso y Procesamiento: Incluye herramientas como Apache Pig y Hive para consultar y procesar datos almacenados en Hadoop.</li> <li>Componentes de Ingesta y Flujos de Trabajo: Con Apache Sqoop, Flume y Oozie, aprender\u00e1s a transferir datos y gestionar workflows.</li> <li>Interfaces y Herramientas: Apache Zeppelin y Ambari facilitan la interacci\u00f3n y administraci\u00f3n de los recursos en Hadoop.</li> <li>Procesamiento en Tiempo Real: Herramientas como Apache Spark y Flink ofrecen opciones para el an\u00e1lisis de datos en tiempo real.</li> </ul>"},{"location":"ut3-ecosistema-hadoop/1introduccion/#diagrama-de-flujo-de-componentes-con-mermaid","title":"Diagrama de flujo de componentes (con mermaid)","text":"<pre><code>graph TD\n    A[Datos Crudos] --&gt; B[Ingesta - Sqoop/Flume]\n    B --&gt; C[Almacenamiento - HDFS]\n    C --&gt; D[Procesamiento - MapReduce/Spark]\n    D --&gt; E[Acceso - Hive/Pig/Impala]\n    E --&gt; F[Visualizaci\u00f3n - Zeppelin]\n    F --&gt; G[Administraci\u00f3n - Ambari]</code></pre>"},{"location":"ut3-ecosistema-hadoop/1introduccion/#objetivos-de-esta-unidad","title":"Objetivos de esta Unidad \ud83c\udfaf","text":"<p>Al finalizar esta unidad, podr\u00e1s:</p> <ol> <li>Identificar los componentes clave del ecosistema Hadoop y sus funciones.</li> <li>Aplicar herramientas de ingesta y procesamiento de datos para construir flujos de trabajo eficientes.</li> <li>Configurar interfaces y herramientas de administraci\u00f3n para mejorar la accesibilidad y control de los datos.</li> <li>Implementar t\u00e9cnicas de procesamiento en tiempo real para datos en streaming.</li> </ol>"},{"location":"ut3-ecosistema-hadoop/1introduccion/#por-que-es-importante-aprender-sobre-hadoop","title":"\u00bfPor qu\u00e9 es importante aprender sobre Hadoop? \ud83c\udf10","text":"<p>En el mundo actual, la gesti\u00f3n de grandes datos es crucial para muchas industrias. Hadoop te da la capacidad de almacenar y procesar cantidades masivas de datos de una forma econ\u00f3mica y escalable, lo cual es fundamental para los negocios basados en datos.</p>"},{"location":"ut3-ecosistema-hadoop/IDEAS/","title":"IDEAS","text":"<p>crear un cheatsheet para apache pig</p> <p>introduccion ventajas  componentes de pig     - pig latin script language      - runtime engine pig operations:   1. load data and write pig script  2. pig operations traduce and plans execution plan, optimize, submit to hadoop  3. execution of the plan, results are dumped on screen or stored in hdfs data model:     atom, tuple, bag or lists and map execution modes:     local mode     mapreduce mode two interactive modes:     interactive mode     batch mode comparation with sql examples of pig scripts: at least 5 from easy to complex</p>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/21apache-pig/","title":"Apache Pig: Procesamiento de Datos a Gran Escala en Hadoop \ud83d\udc16","text":"<p>Apache Pig es una plataforma de an\u00e1lisis de datos de alto nivel en Hadoop, dise\u00f1ada para procesar y transformar grandes vol\u00famenes de datos de manera eficiente. Mediante el lenguaje Pig Latin, permite descripciones detalladas y expresivas de transformaciones complejas y flujos de datos. Adem\u00e1s, soporta el an\u00e1lisis de datos no estructurados y semi-estructurados, optimizando autom\u00e1ticamente la ejecuci\u00f3n de los scripts en Hadoop y permitiendo una mayor flexibilidad y potencia en el procesamiento de datos a gran escala sin necesidad de MapReduce en Java.</p>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/21apache-pig/#ventajas-clave-de-apache-pig-para-profesionales-de-big-data","title":"\ud83d\ude80 Ventajas Clave de Apache Pig para Profesionales de Big Data","text":"<ol> <li>Optimizaci\u00f3n Autom\u00e1tica de Consultas: Pig optimiza los flujos de trabajo durante la fase de compilaci\u00f3n del plan de ejecuci\u00f3n, mejorando la eficiencia y reduciendo el tiempo de procesamiento.</li> <li>Interoperabilidad con HDFS y HCatalog: Apache Pig facilita la lectura y escritura directa en Hadoop Distributed File System (HDFS) y en HCatalog, permitiendo una integraci\u00f3n fluida con otros servicios y herramientas de datos.</li> <li>Extensibilidad y Funciones Definidas por el Usuario (UDFs): Los usuarios avanzados pueden personalizar Pig mediante UDFs en varios lenguajes, como Java, Python y JavaScript, adapt\u00e1ndolo a casos de procesamiento de datos espec\u00edficos.</li> </ol>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/21apache-pig/#componentes-de-pig","title":"Componentes de Pig \ud83e\udde9","text":"<p>Explicaci\u00f3n de los Componentes:</p> <ul> <li>Parser: Interpreta el script de Pig Latin y genera un \u00e1rbol l\u00f3gico que define las operaciones solicitadas por el usuario.</li> <li>Optimizer: A partir del \u00e1rbol l\u00f3gico, aplica optimizaciones como el reordenamiento de operaciones, el filtrado en proyecci\u00f3n y la reducci\u00f3n de datos redundantes.</li> <li>Compiler: Convierte el plan l\u00f3gico optimizado en un plan f\u00edsico, donde cada operaci\u00f3n se traduce en tareas de MapReduce u operaciones en modo local.</li> <li>Execution Engine: Ejecuta el plan f\u00edsico, organizando las tareas y gestionando la comunicaci\u00f3n entre nodos del cl\u00faster Hadoop, y genera el resultado final.</li> </ul>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/21apache-pig/#flujo-complejo-de-operaciones-en-apache-pig","title":"\ud83d\udee0 Flujo Complejo de Operaciones en Apache Pig","text":"<p><pre><code>sequenceDiagram\n    participant Usuario\n    participant Parser\n    participant Optimizer\n    participant Compiler\n    participant ExecutionEngine\n    Usuario-&gt;&gt;Parser: Cargar y Crear Script en Pig Latin\n    Parser-&gt;&gt;Optimizer: Generar \u00e1rbol l\u00f3gico\n    Optimizer-&gt;&gt;Compiler: Optimizaci\u00f3n del plan de ejecuci\u00f3n\n    Compiler-&gt;&gt;ExecutionEngine: Traducci\u00f3n a tareas MapReduce\n    ExecutionEngine-&gt;&gt;Usuario: Resultados en pantalla o HDFS</code></pre> Apache Pig ejecuta sus tareas mediante un flujo estructurado de operaciones avanzadas:</p> <ol> <li>Carga de datos y creaci\u00f3n del script en Pig Latin: permite importar datos de distintas fuentes, facilitando el procesamiento y la transformaci\u00f3n.</li> <li>Optimizaci\u00f3n y planificaci\u00f3n: Pig transforma el script en un plan de ejecuci\u00f3n optimizado, utilizando t\u00e9cnicas avanzadas como proyecci\u00f3n anticipada y reordenamiento de operaciones para maximizar la eficiencia.</li> <li>Ejecuci\u00f3n del plan de ejecuci\u00f3n: Pig realiza las tareas en Hadoop, mostrando los resultados en pantalla o almacen\u00e1ndolos en HDFS.</li> </ol>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/21apache-pig/#modelo-de-datos-en-pig-profundizacion-en-la-jerarquia","title":"Modelo de Datos en Pig: Profundizaci\u00f3n en la Jerarqu\u00eda \ud83d\udcbe","text":"<p>Apache Pig maneja un modelo de datos jer\u00e1rquico, ideal para gestionar datos no estructurados y semi-estructurados. Este modelo incluye las siguientes estructuras:</p> <ul> <li>Atom: Representa valores individuales (n\u00fameros, cadenas, etc.).</li> <li>Tuple: Un conjunto ordenado de valores, semejante a una fila en una tabla de datos.</li> <li>Bag: Colecci\u00f3n no ordenada de tuplas, \u00fatil para agrupar datos y realizar an\u00e1lisis de agregaci\u00f3n.</li> <li>Map: Estructura clave-valor que permite almacenar pares de valores asociados.</li> </ul> <p>Ejemplo de datos complejos en Pig:</p> <pre><code>// Modelo de datos avanzado en Pig\ntupla_compleja = (42, \"Big Data\", [(\"clave1\": \"valor1\"), (\"clave2\": \"valor2\")]);\nmi_bag = {(\"apple\", 1), (\"banana\", 2), (\"cherry\", 3)};\nmi_map = [\"id\" # \"1001\", \"nombre\" # \"John Doe\", \"edad\" # 30];\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/21apache-pig/#modos-de-ejecucion-avanzados-de-pig","title":"Modos de Ejecuci\u00f3n Avanzados de Pig \ud83e\uddd1\u200d\ud83d\udcbb","text":"<p><pre><code>flowchart LR\n    Local[Modo Local] --&gt; |Ideal para pruebas| Pig;\n    MapReduce[Modo MapReduce] --&gt; |Distribuido en cl\u00faster de Hadoop| Pig;</code></pre> Apache Pig puede ejecutarse en dos modos distintos, adapt\u00e1ndose a diferentes necesidades de procesamiento:</p> <ol> <li>Modo Local: Ejecuta los scripts en la m\u00e1quina local, ideal para pruebas y desarrollo.</li> <li>Modo MapReduce: Ejecuta el flujo de trabajo como tareas distribuidas en un cl\u00faster de Hadoop, optimizando el procesamiento a gran escala.</li> </ol>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/21apache-pig/#interactividad-en-pig-modos-de-ejecucion-interactivos-y-en-lote","title":"Interactividad en Pig: Modos de Ejecuci\u00f3n Interactivos y en Lote","text":"<ul> <li>Modo Interactivo (Grunt Shell): Permite ejecutar comandos de Pig en tiempo real, facilitando el an\u00e1lisis exploratorio y la depuraci\u00f3n de errores. <pre><code>grunt&gt; registros = LOAD 'data.csv' AS (nombre:chararray, edad:int);\ngrunt&gt; adultos = FILTER registros BY edad &gt;= 18;\ngrunt&gt; DUMP adultos;\n</code></pre></li> <li>Modo Batch: Ejecuta scripts completos de Pig sin intervenci\u00f3n, ideal para flujos de procesamiento automatizados y pipelines de datos. <pre><code>pig -x mapreduce mi_script.pig\n</code></pre></li> </ul>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/21apache-pig/#comparacion-profunda-pig-latin-vs-sql","title":"Comparaci\u00f3n Profunda: Pig Latin vs SQL \ud83d\udd0d","text":"Aspecto Pig Latin SQL Paradigma Flujo de datos Declarativo Estructura de datos Basado en Bag y Tuples Basado en Tablas Flexibilidad Alta, con soporte para datos jer\u00e1rquicos Limitada a datos relacionales Tareas soportadas ETL, transformaci\u00f3n compleja Consultas de bases de datos"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/21apache-pig/#guia-de-comandos-principales-en-apache-pig","title":"Gu\u00eda de Comandos Principales en Apache Pig \ud83d\udee0\ufe0f","text":"<p>Apache Pig ofrece una serie de comandos b\u00e1sicos para cargar, transformar, filtrar y almacenar datos. A continuaci\u00f3n, se presenta una gu\u00eda r\u00e1pida de los comandos esenciales de Pig Latin:</p> <ol> <li> <p>Cargar Datos (<code>LOAD</code>): Importa datos desde archivos externos en HDFS.    <pre><code>data = LOAD 'ruta/del/archivo.csv' USING PigStorage(',') AS (columna1:tipo, columna2:tipo);\n</code></pre></p> </li> <li> <p>Almacenar Datos (<code>STORE</code>): Guarda los datos procesados en HDFS.    <pre><code>STORE data INTO 'ruta/de/salida' USING PigStorage(',');\n</code></pre></p> </li> <li> <p>Filtrar Datos (<code>FILTER</code>): Filtra los datos en funci\u00f3n de una condici\u00f3n.    <pre><code>filtered_data = FILTER data BY columna1 &gt; 10;\n</code></pre></p> </li> <li> <p>Agrupar Datos (<code>GROUP</code>): Agrupa los datos en funci\u00f3n de una o varias columnas.    <pre><code>grouped_data = GROUP data BY columna1;\n</code></pre></p> </li> <li> <p>Aplicar Transformaciones (<code>FOREACH ... GENERATE</code>): Genera un nuevo conjunto de datos transformando los datos existentes.    <pre><code>transformed_data = FOREACH data GENERATE columna1, columna2 * 2;\n</code></pre></p> </li> <li> <p>Ordenar Datos (<code>ORDER BY</code>): Ordena los datos en funci\u00f3n de una columna en orden ascendente o descendente.    <pre><code>ordered_data = ORDER data BY columna1 DESC;\n</code></pre></p> </li> <li> <p>Unir Tablas (<code>JOIN</code>): Une dos o m\u00e1s conjuntos de datos en funci\u00f3n de una clave com\u00fan.    <pre><code>joined_data = JOIN data1 BY columna1, data2 BY columna1;\n</code></pre></p> </li> </ol>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/21apache-pig/#ejemplos-basicos-de-scripts-en-pig","title":"Ejemplos B\u00e1sicos de Scripts en Pig \ud83d\udda5\ufe0f","text":"<p>Estos ejemplos ilustran c\u00f3mo aplicar los comandos de Pig Latin para realizar operaciones comunes en el procesamiento de datos:</p> <ol> <li> <p>Cargar y Filtrar Datos:    <pre><code>// Cargar el archivo y filtrar por edades mayores de 18\nusuarios = LOAD 'usuarios.csv' AS (nombre:chararray, edad:int);\nadultos = FILTER usuarios BY edad &gt;= 18;\n</code></pre></p> </li> <li> <p>Contar Palabras en Documentos:    <pre><code>// Tokenizar las palabras y contar su frecuencia\ndocumentos = LOAD 'docs.txt' AS (linea:chararray);\npalabras = FOREACH documentos GENERATE FLATTEN(TOKENIZE(linea)) AS palabra;\ngrupo_palabras = GROUP palabras BY palabra;\nfrecuencia = FOREACH grupo_palabras GENERATE group AS palabra, COUNT(palabras) AS total;\n</code></pre></p> </li> <li> <p>Calcular el Total de Ventas por Producto:    <pre><code>// Cargar y calcular el total de ventas\nventas = LOAD 'ventas.csv' AS (producto:chararray, cantidad:int, precio:float);\ntotal_ventas = FOREACH (GROUP ventas BY producto) GENERATE group AS producto, SUM(ventas.cantidad * ventas.precio) AS total;\n</code></pre></p> </li> <li> <p>Ordenar los Datos de Ventas:    <pre><code>// Ordenar las ventas por total en orden descendente\nventas_ordenadas = ORDER total_ventas BY total DESC;\n</code></pre></p> </li> <li> <p>Pipeline de ETL Simple:    <pre><code>// ETL: Cargar, filtrar, transformar y almacenar resultados\nraw_data = LOAD 'raw_data.csv' AS (id:int, valor:float);\nfiltered_data = FILTER raw_data BY valor &gt; 50;\ntransformed_data = FOREACH filtered_data GENERATE id, valor * 0.8 AS valor_transformado;\nSTORE transformed_data INTO 'datos_procesados' USING PigStorage(',');\n</code></pre></p> </li> </ol>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/21apache-pig/#conclusion","title":"Conclusi\u00f3n","text":"<p>Apache Pig es una herramienta poderosa y flexible para el procesamiento avanzado de datos en Big Data, ofreciendo un entorno optimizado y adaptable para transformar grandes vol\u00famenes de informaci\u00f3n. Con su capacidad para optimizar consultas, integrar UDFs y manejar modelos de datos jer\u00e1rquicos, Pig se convierte en un componente esencial para los profesionales de datos que trabajan en Hadoop, permitiendo la transformaci\u00f3n de datos de manera eficiente sin la complejidad de MapReduce manual.</p>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/0introduccion/","title":"Introducci\u00f3n a Apache Hive \ud83d\ude80","text":""},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/0introduccion/#que-es-apache-hive","title":"\u00bfQu\u00e9 es Apache Hive? \ud83d\udc1d","text":""},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/0introduccion/#historia-y-proposito","title":"Historia y Prop\u00f3sito \ud83d\udcd6","text":"<p>Apache Hive fue desarrollado inicialmente por Facebook en 2007 para simplificar el procesamiento de datos masivos en Hadoop. Su objetivo principal es permitir consultas de estilo SQL sobre datos almacenados en sistemas distribuidos, eliminando la complejidad de trabajar directamente con MapReduce. En 2010, pas\u00f3 a ser parte de la Apache Software Foundation, consolid\u00e1ndose como una herramienta esencial en el ecosistema de Big Data.</p>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/0introduccion/#comparacion-con-bases-de-datos-tradicionales","title":"Comparaci\u00f3n con Bases de Datos Tradicionales \u2696\ufe0f","text":"<p>Aunque Hive utiliza un lenguaje similar a SQL llamado HiveQL, no es una base de datos tradicional. En lugar de manejar transacciones en tiempo real o trabajar con estructuras completamente normalizadas, Hive est\u00e1 dise\u00f1ado para el an\u00e1lisis y procesamiento de grandes vol\u00famenes de datos en sistemas distribuidos.  </p> Caracter\u00edstica Apache Hive Bases de Datos Tradicionales Uso principal An\u00e1lisis de datos a gran escala Transacciones en tiempo real Escalabilidad Altamente escalable en cl\u00fasteres Limitada a un solo servidor Procesamiento Lote (Batch Processing) Tiempo real (OLTP) Estructura de datos Datos semi/no estructurados Datos estructurados"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/0introduccion/#casos-de-uso","title":"Casos de Uso \ud83d\udd0d","text":"<ol> <li> <p>An\u00e1lisis de logs de servidores web: Empresas como Facebook o Amazon lo utilizan para analizar millones de registros diarios.  </p> <ul> <li>Ejemplo de registros: <pre><code>192.168.1.1 - - [27/Nov/2024:10:30:00 +0000] \"GET /home HTTP/1.1\" 200 2048  \n192.168.1.2 - - [27/Nov/2024:10:30:05 +0000] \"POST /login HTTP/1.1\" 302 512  \n</code></pre>     Informaci\u00f3n extra\u00edda: Direcciones IP, rutas solicitadas, c\u00f3digos de estado, y tama\u00f1os de respuesta.  </li> </ul> </li> <li> <p>Agrupaci\u00f3n de datos para informes: Ideal para reportes hist\u00f3ricos en proyectos de Data Warehousing.  </p> <ul> <li>\u00bfQu\u00e9 es Data Warehousing?     Es un sistema de almacenamiento dise\u00f1ado para consolidar y analizar datos hist\u00f3ricos provenientes de diferentes fuentes, optimizando la toma de decisiones empresariales.</li> </ul> </li> <li> <p>Transformaci\u00f3n de datos en pipelines: Se emplea en procesos por lotes para limpiar y transformar datos en entornos distribuidos.  </p> <ul> <li> <p>\u00bfQu\u00e9 es un pipeline de datos?     Es una serie de pasos o procesos automatizados que convierten datos crudos en informaci\u00f3n lista para an\u00e1lisis.  </p> <p>Diagrama del Pipeline: <pre><code>graph TD;\n    A[Datos Brutos] --&gt; B[Limpieza];\n    B --&gt; C[Transformaci\u00f3n];\n    C --&gt; D[Almacenamiento Anal\u00edtico];\n    D --&gt; E[An\u00e1lisis/Visualizaci\u00f3n];</code></pre></p> </li> </ul> </li> <li> <p>Integraci\u00f3n con herramientas de visualizaci\u00f3n: Hive puede servir como backend para herramientas como Tableau o Power BI.</p> </li> </ol>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/0introduccion/#arquitectura-de-hive","title":"Arquitectura de Hive \ud83c\udfd7\ufe0f","text":"<p>Hive se compone de varios componentes que trabajan juntos para procesar y analizar datos masivos de manera eficiente.</p>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/0introduccion/#1-metastore","title":"1. Metastore \ud83d\udcc2","text":"<p>El Metastore es el n\u00facleo de la arquitectura de Hive. Este almac\u00e9n de metadatos contiene informaci\u00f3n sobre:  </p> <ul> <li>Esquema de tablas: Columnas, tipos de datos, particiones.  </li> <li>Ubicaci\u00f3n de datos: Rutas dentro de Hadoop Distributed File System (HDFS) o fuentes externas como Amazon S3.  </li> <li>Informaci\u00f3n de permisos: Control de acceso a tablas y datos.  </li> </ul> <p>Ejemplo: Si una tabla de Hive utiliza particiones por fecha, el Metastore almacena los detalles de estas particiones para facilitar las consultas.</p>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/0introduccion/#2-query-compiler","title":"2. Query Compiler \ud83e\uddee","text":"<p>El Query Compiler toma las consultas en HiveQL y las traduce en planes de ejecuci\u00f3n compatibles con Hadoop.</p> <ul> <li>Divide las consultas en etapas l\u00f3gicas.  </li> <li>Genera tareas de MapReduce o Spark para ejecutarlas.  </li> <li>Optimiza el procesamiento para mejorar el rendimiento.  </li> </ul> <p>Dato Interesante: Gracias al Query Compiler, usuarios sin experiencia en MapReduce pueden ejecutar an\u00e1lisis complejos con facilidad.</p>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/0introduccion/#3-execution-engine","title":"3. Execution Engine \u2699\ufe0f","text":"<p>El Execution Engine ejecuta las tareas generadas por el Query Compiler en el cl\u00faster de Hadoop.  </p> <ul> <li>Coordina las tareas de procesamiento.  </li> <li>Administra los recursos del cl\u00faster para garantizar que las consultas se ejecuten de manera eficiente.  </li> </ul> <p>El motor utiliza herramientas como Tez, MapReduce, o incluso Spark, dependiendo de la configuraci\u00f3n.</p>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/0introduccion/#4-integracion-con-hadoop-y-hdfs","title":"4. Integraci\u00f3n con Hadoop y HDFS \ud83c\udf10","text":"<p>Hive est\u00e1 profundamente integrado con el Hadoop Distributed File System (HDFS).  </p> <ul> <li>Los datos se almacenan en HDFS, lo que asegura escalabilidad y alta disponibilidad.  </li> <li>Las tareas se ejecutan utilizando la infraestructura de Hadoop, como MapReduce o Tez.  </li> </ul> <p>Adem\u00e1s, Hive se conecta con otras herramientas del ecosistema, como YARN para la gesti\u00f3n de recursos y HBase para almacenamiento de datos NoSQL.</p>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/0introduccion/#diagrama-de-la-arquitectura-de-hive","title":"Diagrama de la Arquitectura de Hive \ud83d\uddbc\ufe0f","text":"<p>El siguiente diagrama muestra c\u00f3mo los diferentes componentes interact\u00faan en Apache Hive:  </p> <pre><code>graph TD;\n    A[User Queries HiveQL] --&gt; B[Query Compiler];\n    B --&gt; C[Execution Engine];\n    C --&gt; D[MapReduce/Spark/Tez];\n    C --&gt; E[Metastore];\n    D --&gt; F[HDFS];\n    F --&gt; G[Hadoop Cluster];\n    G --&gt; E;\n    G --&gt; H[YARN];</code></pre> <p>Hive combina la facilidad de consultas estilo SQL con la potencia del procesamiento distribuido. Es una herramienta clave para empresas que manejan grandes vol\u00famenes de datos y buscan maximizar el potencial de sus cl\u00fasteres de Hadoop. \ud83d\ude80</p>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/1conceptos-fundamentales/","title":"Conceptos Fundamentales de Apache Hive \ud83d\udc1d","text":""},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/1conceptos-fundamentales/#bases-de-datos-y-tablas-en-hive","title":"Bases de Datos y Tablas en Hive \ud83d\udcc2","text":""},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/1conceptos-fundamentales/#bases-de-datos","title":"Bases de Datos","text":"<p>En Apache Hive, una base de datos es una agrupaci\u00f3n l\u00f3gica que organiza tablas relacionadas. Es \u00fatil para mantener separados los datos de diferentes proyectos o aplicaciones, especialmente en grandes sistemas de datos compartidos.  </p> <p>Caracter\u00edsticas clave de las bases de datos en Hive: - Cada base de datos tiene su propio espacio de nombres. - Los datos se almacenan en directorios espec\u00edficos de HDFS, generalmente dentro de <code>/user/hive/warehouse/&lt;nombre_base&gt;</code>.</p> <p>Ejemplo de creaci\u00f3n de una base de datos: <pre><code>CREATE DATABASE empresa;\n</code></pre></p>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/1conceptos-fundamentales/#tablas","title":"Tablas","text":"<p>Las tablas en Hive son similares a las tablas en bases de datos relacionales. Cada tabla tiene un esquema definido (columnas y tipos de datos) y puede incluir particiones y buckets para optimizar el rendimiento.</p>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/1conceptos-fundamentales/#particiones","title":"Particiones","text":"<ul> <li>Dividen los datos en subdirectorios basados en valores de una columna.  </li> <li>Ejemplo: Una tabla de ventas puede estar particionada por <code>fecha</code>.</li> </ul> <pre><code>CREATE TABLE ventas (\n  id INT,\n  producto STRING,\n  cantidad INT\n) PARTITIONED BY (fecha DATE);\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/1conceptos-fundamentales/#buckets","title":"Buckets","text":"<ul> <li>Dividen los datos en archivos dentro de una partici\u00f3n seg\u00fan un valor hash de una columna.</li> <li>Mejora el rendimiento de ciertas consultas como uniones.</li> </ul> <pre><code>CREATE TABLE usuarios (\n  id INT,\n  nombre STRING\n) CLUSTERED BY (id) INTO 4 BUCKETS;\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/1conceptos-fundamentales/#lenguaje-de-consulta-hiveql-hql","title":"Lenguaje de Consulta HiveQL (HQL) \ud83d\udcdd","text":"<p>HiveQL es el lenguaje de consultas utilizado en Hive. Similar a SQL, est\u00e1 optimizado para el procesamiento por lotes y consultas anal\u00edticas en grandes vol\u00famenes de datos distribuidos.</p>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/1conceptos-fundamentales/#caracteristicas-principales-de-hiveql","title":"Caracter\u00edsticas principales de HiveQL:","text":"<ol> <li>Compatibilidad con SQL: Los usuarios con experiencia en SQL pueden usar HiveQL f\u00e1cilmente.</li> <li>Soporte para funciones agregadas y anal\u00edticas: Incluye operaciones como <code>SUM</code>, <code>COUNT</code>, <code>AVG</code>, <code>GROUP BY</code>, entre otras.</li> <li>Extensibilidad: Los usuarios pueden definir sus propias funciones (UDFs) en Java o Python.</li> </ol>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/1conceptos-fundamentales/#ejemplo-de-consultas-en-hiveql","title":"Ejemplo de consultas en HiveQL:","text":""},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/1conceptos-fundamentales/#crear-una-tabla","title":"Crear una tabla:","text":"<pre><code>CREATE TABLE empleados (\n  id INT,\n  nombre STRING,\n  salario FLOAT\n);\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/1conceptos-fundamentales/#insertar-datos","title":"Insertar datos:","text":"<pre><code>INSERT INTO empleados VALUES (1, 'Ana', 3500.0), (2, 'Luis', 4500.0);\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/1conceptos-fundamentales/#consultar-datos","title":"Consultar datos:","text":"<pre><code>SELECT nombre, salario FROM empleados WHERE salario &gt; 4000;\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/1conceptos-fundamentales/#agregar-funciones","title":"Agregar funciones:","text":"<pre><code>SELECT AVG(salario) AS salario_promedio FROM empleados;\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/1conceptos-fundamentales/#limitaciones-de-hive","title":"Limitaciones de Hive \u26a0\ufe0f","text":"<p>Aunque Apache Hive es una herramienta poderosa para el procesamiento de datos distribuidos, tiene limitaciones que es importante considerar:</p>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/1conceptos-fundamentales/#1-no-es-para-consultas-en-tiempo-real","title":"1. No es para Consultas en Tiempo Real","text":"<p>Hive est\u00e1 dise\u00f1ado para procesamiento por lotes y no para consultas transaccionales o de baja latencia. Esto significa que:</p> <ul> <li>Las consultas pueden tardar varios segundos o minutos en ejecutarse.  </li> <li>No es adecuado para aplicaciones que requieren tiempos de respuesta inmediatos.  </li> </ul>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/1conceptos-fundamentales/#2-falta-de-soporte-completo-para-transacciones","title":"2. Falta de Soporte Completo para Transacciones","text":"<p>Hive es m\u00e1s adecuado para operaciones de lectura y an\u00e1lisis que para operaciones de escritura transaccional. </p> <ul> <li>No permite <code>ROLLBACK</code> o <code>SAVEPOINT</code>.  </li> <li>Soporta transacciones solo en tablas ACID configuradas espec\u00edficamente.</li> </ul>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/1conceptos-fundamentales/#3-latencia-por-dependencia-de-mapreduce","title":"3. Latencia por Dependencia de MapReduce","text":"<p>Aunque se pueden usar motores m\u00e1s r\u00e1pidos como Tez o Spark, muchas operaciones en Hive dependen de MapReduce, lo que introduce latencias significativas.</p>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/1conceptos-fundamentales/#4-limitaciones-en-el-modelado-de-datos","title":"4. Limitaciones en el Modelado de Datos","text":"<ul> <li>Hive no es ideal para datos muy normalizados.  </li> <li>Dise\u00f1ado para manejar datos semiestructurados y desnormalizados.</li> </ul>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/1conceptos-fundamentales/#5-complejidad-de-configuracion-y-mantenimiento","title":"5. Complejidad de Configuraci\u00f3n y Mantenimiento","text":"<ul> <li>Configurar y administrar un cl\u00faster de Hadoop junto con Hive puede ser desafiante, especialmente para equipos sin experiencia previa en infraestructura de Big Data.</li> </ul> <p>Apache Hive es una herramienta incre\u00edblemente \u00fatil para an\u00e1lisis a gran escala, pero es fundamental evaluar estas limitaciones al dise\u00f1ar soluciones basadas en Hive. \ud83d\udc1d</p>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/2gestion-de-bds-apachehive/","title":"Gesti\u00f3n de Datos en Apache Hive: Una Gu\u00eda Atractiva para Desarrolladores","text":""},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/2gestion-de-bds-apachehive/#1-creacion-de-bases-de-datos-y-tablas","title":"1. Creaci\u00f3n de Bases de Datos y Tablas \ud83c\udfd7\ufe0f","text":""},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/2gestion-de-bds-apachehive/#tipos-de-tablas-internas-vs-externas","title":"Tipos de Tablas: Internas vs Externas","text":"<p>En Apache Hive, puedes trabajar con dos tipos principales de tablas:  </p> <ol> <li> <p>Tablas Internas (Gestionadas) \ud83d\udee0\ufe0f  </p> <ul> <li>Hive controla tanto el esquema como los datos.  </li> <li>Los datos se almacenan en la ubicaci\u00f3n predeterminada del warehouse de Hive (<code>/user/hive/warehouse/</code>).  </li> <li>Advertencia: Si eliminas la tabla, \u00a1los datos tambi\u00e9n se eliminan!  </li> </ul> <pre><code>CREATE TABLE empleados (\n  id INT,\n  nombre STRING,\n  salario FLOAT\n);\n</code></pre> <p>\u00dasalo cuando Hive deba gestionar completamente los datos.</p> </li> <li> <p>Tablas Externas \ud83c\udf10  </p> <ul> <li>Hive solo administra el esquema, pero los datos permanecen en una ubicaci\u00f3n externa como S3 o HDFS.  </li> <li>Al eliminar la tabla, los datos no se borran.  </li> </ul> <pre><code>CREATE EXTERNAL TABLE ventas (\n  id INT,\n  producto STRING,\n  cantidad INT\n)\nSTORED AS TEXTFILE\nLOCATION '/data/ventas/';\n</code></pre> <p>Perfecto para datos que ya existen y no deseas duplicar.</p> </li> </ol>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/2gestion-de-bds-apachehive/#sintaxis-basica-para-crear-tablas","title":"Sintaxis B\u00e1sica para Crear Tablas","text":"<p>Crear tablas en Hive es similar a SQL, pero con soporte para particiones y formatos avanzados.</p> <pre><code>CREATE TABLE productos (\n  id INT,\n  nombre STRING,\n  precio FLOAT\n)\nPARTITIONED BY (categoria STRING)\nSTORED AS PARQUET;\n</code></pre> <ul> <li><code>PARTITIONED BY</code>: Divide los datos por columnas espec\u00edficas, como <code>categoria</code>.</li> <li><code>STORED AS PARQUET</code>: Especifica el formato de almacenamiento.</li> </ul>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/2gestion-de-bds-apachehive/#configuracion-de-ubicaciones-en-s3-o-hdfs","title":"Configuraci\u00f3n de Ubicaciones en S3 o HDFS \ud83c\udf0d","text":"<p>Hive puede trabajar con datos en diferentes ubicaciones.  </p>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/2gestion-de-bds-apachehive/#ejemplo-almacenar-en-s3","title":"Ejemplo: Almacenar en S3","text":"<pre><code>CREATE EXTERNAL TABLE logs (\n  fecha STRING,\n  nivel STRING,\n  mensaje STRING\n)\nSTORED AS TEXTFILE\nLOCATION 's3://mi-bucket/logs/';\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/2gestion-de-bds-apachehive/#ejemplo-almacenar-en-hdfs","title":"Ejemplo: Almacenar en HDFS","text":"<pre><code>CREATE EXTERNAL TABLE transacciones (\n  id INT,\n  monto FLOAT\n)\nSTORED AS ORC\nLOCATION '/data/warehouse/transacciones/';\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/2gestion-de-bds-apachehive/#2-formatos-de-datos-soportados","title":"2. Formatos de Datos Soportados \ud83d\udce6","text":"<p>Apache Hive soporta varios formatos para optimizar el almacenamiento y la consulta. Aqu\u00ed est\u00e1n los m\u00e1s comunes:</p> Formato Ventajas Desventajas Ideal Para TextFile Simple, ampliamente compatible. Ineficiente en espacio y procesamiento. Datos crudos o peque\u00f1os. Parquet Almacenamiento columnar, compresi\u00f3n eficiente, r\u00e1pido en lectura. Costoso en actualizaciones. Consultas anal\u00edticas. ORC Optimizado para Hive, excelente compresi\u00f3n. Limitado fuera del ecosistema Hadoop. Data Warehousing. Avro Compatible con esquemas, ideal para intercambio entre sistemas. Menos eficiente para consultas anal\u00edticas. Interoperabilidad."},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/2gestion-de-bds-apachehive/#como-elegir-el-formato-adecuado","title":"C\u00f3mo Elegir el Formato Adecuado","text":"<ul> <li>Datos peque\u00f1os y simples: Usa TextFile.  </li> <li>Consultas anal\u00edticas r\u00e1pidas: Prefiere Parquet o ORC.  </li> <li>Intercambio entre sistemas: Opta por Avro.  </li> </ul>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/2gestion-de-bds-apachehive/#comparacion-visual-de-formatos","title":"Comparaci\u00f3n Visual de Formatos","text":"<pre><code>graph TD;\n    A[TextFile] --&gt;|Flexible| B[General];\n    C[Parquet] --&gt;|R\u00e1pido y Eficiente| B;\n    D[ORC] --&gt;|Compacto y Optimizado| B;\n    E[Avro] --&gt;|Compatibilidad| B;</code></pre>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/2gestion-de-bds-apachehive/#3-cargar-datos-en-tablas","title":"3. Cargar Datos en Tablas \ud83d\udce5","text":""},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/2gestion-de-bds-apachehive/#comando-load-data","title":"Comando <code>LOAD DATA</code>","text":"<p>El comando <code>LOAD DATA</code> permite cargar datos desde HDFS o el sistema local directamente en una tabla de Hive.</p>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/2gestion-de-bds-apachehive/#desde-hdfs","title":"Desde HDFS","text":"<pre><code>LOAD DATA INPATH '/hdfs/datos/ventas.csv' INTO TABLE ventas;\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/2gestion-de-bds-apachehive/#desde-el-sistema-local","title":"Desde el Sistema Local","text":"<pre><code>LOAD DATA LOCAL INPATH '/local/datos/ventas.csv' INTO TABLE ventas;\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/2gestion-de-bds-apachehive/#importar-datos-desde-s3","title":"Importar Datos desde S3","text":"<p>Hive tambi\u00e9n puede cargar datos almacenados en S3. Aseg\u00farate de configurar las credenciales de AWS correctamente.  </p> <pre><code>LOAD DATA INPATH 's3://mi-bucket/ventas.csv' INTO TABLE ventas;\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/2gestion-de-bds-apachehive/#creacion-de-tablas-a-partir-de-archivos-existentes","title":"Creaci\u00f3n de Tablas a Partir de Archivos Existentes","text":"<p>Hive puede crear autom\u00e1ticamente una tabla basada en la estructura de un archivo existente.</p> <pre><code>CREATE EXTERNAL TABLE logs (\n  fecha STRING,\n  nivel STRING,\n  mensaje STRING\n)\nSTORED AS TEXTFILE\nLOCATION '/hdfs/logs/';\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/2gestion-de-bds-apachehive/#diagrama-del-proceso","title":"Diagrama del Proceso","text":"<pre><code>graph TD;\n    A[Archivos Existentes] --&gt; B[Crear Tabla];\n    B --&gt; C[Definir Esquema];\n    C --&gt; D[Consultar con HiveQL];</code></pre> <p>\u00a1Con estas herramientas, puedes gestionar datos en Apache Hive de manera eficiente y optimizada, ya sea que trabajes con datos locales, en HDFS o S3! \ud83d\ude80</p>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/3consulta-y-optimizacion/","title":"Consultas y Optimizaci\u00f3n en Apache Hive con HiveQL \ud83d\ude80","text":""},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/3consulta-y-optimizacion/#1-consulta-basica-con-hiveql","title":"1. Consulta B\u00e1sica con HiveQL \ud83d\udee0\ufe0f","text":""},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/3consulta-y-optimizacion/#sentencias-select-y-where","title":"Sentencias <code>SELECT</code> y <code>WHERE</code>","text":"<p>El comando <code>SELECT</code> es la base de cualquier consulta en HiveQL. Puedes seleccionar columnas espec\u00edficas y usar <code>WHERE</code> para filtrar datos.</p>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/3consulta-y-optimizacion/#ejemplo-en-javascript-simulacion","title":"Ejemplo en JavaScript (Simulaci\u00f3n):","text":"<pre><code>const empleados = [\n  { id: 1, nombre: \"Ana\", salario: 3000 },\n  { id: 2, nombre: \"Luis\", salario: 4000 },\n  { id: 3, nombre: \"Marta\", salario: 2500 },\n];\n\n// Filtrar empleados con salario mayor a 3000\nconst filtrados = empleados.filter(emp =&gt; emp.salario &gt; 3000);\nconsole.log(filtrados);\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/3consulta-y-optimizacion/#consulta-equivalente-en-hiveql","title":"Consulta Equivalente en HiveQL:","text":"<pre><code>SELECT nombre, salario \nFROM empleados\nWHERE salario &gt; 3000;\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/3consulta-y-optimizacion/#ordenamiento-y-agrupamiento","title":"Ordenamiento y Agrupamiento","text":"<p>HiveQL soporta <code>ORDER BY</code>, <code>GROUP BY</code> y <code>HAVING</code> para organizar y analizar datos:</p> <ol> <li><code>ORDER BY</code>: Ordena los resultados.</li> <li><code>GROUP BY</code>: Agrupa datos.</li> <li><code>HAVING</code>: Filtra grupos despu\u00e9s del agrupamiento.</li> </ol>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/3consulta-y-optimizacion/#ejemplo-visual","title":"Ejemplo Visual:","text":"<pre><code>SELECT departamento, AVG(salario) AS salario_promedio\nFROM empleados\nGROUP BY departamento\nHAVING AVG(salario) &gt; 3500\nORDER BY salario_promedio DESC;\n</code></pre> Departamento Salario Promedio IT 4500 Marketing 3600"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/3consulta-y-optimizacion/#funciones-basicas-count-avg-sum","title":"Funciones B\u00e1sicas: <code>COUNT</code>, <code>AVG</code>, <code>SUM</code>","text":"<p>HiveQL incluye funciones para an\u00e1lisis b\u00e1sico de datos:</p> <pre><code>SELECT COUNT(*) AS total_empleados, SUM(salario) AS salario_total, AVG(salario) AS salario_promedio\nFROM empleados;\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/3consulta-y-optimizacion/#2-consultas-avanzadas","title":"2. Consultas Avanzadas \ud83c\udfaf","text":""},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/3consulta-y-optimizacion/#subconsultas","title":"Subconsultas","text":"<p>Usa subconsultas para realizar consultas dentro de otras consultas:</p> <pre><code>SELECT nombre, salario\nFROM empleados\nWHERE salario &gt; (\n    SELECT AVG(salario) \n    FROM empleados\n);\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/3consulta-y-optimizacion/#uniones-join-left-join-right-join-full-outer-join","title":"Uniones: <code>JOIN</code>, <code>LEFT JOIN</code>, <code>RIGHT JOIN</code>, <code>FULL OUTER JOIN</code>","text":"<p>Une tablas para combinarlas en un solo conjunto de datos.</p>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/3consulta-y-optimizacion/#ejemplo","title":"Ejemplo:","text":"<pre><code>SELECT e.nombre, e.salario, d.nombre AS departamento\nFROM empleados e\nJOIN departamentos d ON e.depto_id = d.id;\n</code></pre> Nombre Salario Departamento Ana 3000 IT Luis 4000 Marketing"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/3consulta-y-optimizacion/#funciones-analiticas","title":"Funciones Anal\u00edticas","text":"<p>Las funciones anal\u00edticas como <code>ROW_NUMBER</code>, <code>RANK</code> y <code>WINDOW</code> permiten realizar an\u00e1lisis avanzados.</p>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/3consulta-y-optimizacion/#ejemplo_1","title":"Ejemplo:","text":"<pre><code>SELECT nombre, salario, RANK() OVER (ORDER BY salario DESC) AS ranking\nFROM empleados;\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/3consulta-y-optimizacion/#3-particionamiento-y-bucketing","title":"3. Particionamiento y Bucketing \ud83d\udcc2","text":""},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/3consulta-y-optimizacion/#conceptos-basicos","title":"Conceptos B\u00e1sicos","text":"<ul> <li>Particionamiento: Divide una tabla en subdirectorios seg\u00fan el valor de una columna.  </li> <li>Bucketing: Divide particiones en peque\u00f1os archivos basados en un valor hash.</li> </ul>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/3consulta-y-optimizacion/#creacion-de-tablas-particionadas","title":"Creaci\u00f3n de Tablas Particionadas","text":"<pre><code>CREATE TABLE ventas (\n  id INT,\n  producto STRING,\n  cantidad INT\n) PARTITIONED BY (fecha STRING);\n</code></pre> <ul> <li>Los datos se dividen por fecha: <pre><code>/user/hive/warehouse/ventas/fecha=2024-11-01/\n</code></pre></li> </ul>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/3consulta-y-optimizacion/#implementacion-de-buckets","title":"Implementaci\u00f3n de Buckets","text":"<p>Los buckets son \u00fatiles para mejorar el rendimiento de consultas:</p> <pre><code>CREATE TABLE usuarios (\n  id INT,\n  nombre STRING\n)\nCLUSTERED BY (id) INTO 4 BUCKETS;\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/3consulta-y-optimizacion/#4-optimizacion-de-consultas","title":"4. Optimizaci\u00f3n de Consultas \u26a1","text":""},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/3consulta-y-optimizacion/#uso-de-indices","title":"Uso de \u00cdndices","text":"<p>Aunque no es com\u00fan, Hive soporta \u00edndices simples para acelerar consultas:</p> <pre><code>CREATE INDEX idx_ventas_fecha ON TABLE ventas (fecha)\nAS 'COMPACT' WITH DEFERRED REBUILD;\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/3consulta-y-optimizacion/#configuracion-de-motores-de-ejecucion-tez-y-spark","title":"Configuraci\u00f3n de Motores de Ejecuci\u00f3n: Tez y Spark","text":"<p>Para mejorar el rendimiento, puedes usar motores avanzados como Tez o Spark en lugar de MapReduce.</p> <pre><code>SET hive.execution.engine=tez;\n-- O\nSET hive.execution.engine=spark;\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/3consulta-y-optimizacion/#optimizacion-con-tablas-orc-y-parquet","title":"Optimizaci\u00f3n con Tablas ORC y Parquet","text":"<ul> <li>ORC y Parquet son formatos columnar optimizados para Big Data.</li> <li>Ofrecen:<ul> <li>Compresi\u00f3n eficiente.</li> <li>Lectura r\u00e1pida para consultas anal\u00edticas.</li> </ul> </li> </ul> <pre><code>CREATE TABLE optimizada (\n  id INT,\n  datos STRING\n)\nSTORED AS ORC;\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/3consulta-y-optimizacion/#comparacion-visual-de-rendimiento","title":"Comparaci\u00f3n Visual de Rendimiento:","text":"<pre><code>graph LR;\n    A[TextFile] --&gt;|Lento| B[ORC];\n    C[Parquet] --&gt;|R\u00e1pido| B;\n    B --&gt;|Optimizado| D[Consulta Eficiente];</code></pre> <p>Con estas herramientas y t\u00e9cnicas, \u00a1puedes realizar an\u00e1lisis avanzados y optimizar tus consultas en Apache Hive como un profesional! \ud83d\ude80</p>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/index-temp/","title":"Index temp","text":""},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/index-temp/#modulo-6-temas-avanzados-y-mejoras","title":"M\u00f3dulo 6: Temas Avanzados y Mejoras","text":"<ol> <li>Uso de UDFs (User Defined Functions)</li> <li>Creaci\u00f3n de funciones personalizadas en Java/Python</li> <li> <p>Registro e integraci\u00f3n con Hive</p> </li> <li> <p>Seguridad en Hive</p> </li> <li>Control de acceso y permisos</li> <li> <p>Integraci\u00f3n con Kerberos</p> </li> <li> <p>Optimizaci\u00f3n y Depuraci\u00f3n</p> </li> <li>Herramientas de monitoreo</li> <li>An\u00e1lisis de planes de ejecuci\u00f3n</li> <li>Identificaci\u00f3n de cuellos de botella</li> </ol>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/index-temp/#recursos-y-materiales-complementarios","title":"Recursos y Materiales Complementarios","text":"<ol> <li>Documentaci\u00f3n oficial de Hive</li> <li>Tutoriales y gu\u00edas de configuraci\u00f3n en AWS</li> <li>Datasets para pr\u00e1ctica</li> <li>Ejemplos de consultas avanzadas</li> </ol>"},{"location":"ut3-ecosistema-hadoop/2componentes-acceso-procesamiento/22apache-hive/index-temp/#duracion-estimada","title":"Duraci\u00f3n Estimada","text":"<ul> <li>Duraci\u00f3n total: 20 horas (distribuidas en sesiones de 2 horas)</li> <li>Pr\u00e1cticas incluidas: 40% del tiempo total</li> </ul>"},{"location":"ut3-ecosistema-hadoop/tareas/","title":"\u00cdndice de pr\u00e1cticas y tareas","text":"<p>Este \u00edndice incluye todas las pr\u00e1cticas guiadas y tareas correspondientes al curso. Aseg\u00farate de seguir las instrucciones cuidadosamente para cada pr\u00e1ctica y tarea, y consulta los materiales de apoyo cuando sea necesario.</p>"},{"location":"ut3-ecosistema-hadoop/tareas/#practicas-guiadas","title":"Pr\u00e1cticas guiadas","text":"<p>Las pr\u00e1cticas guiadas est\u00e1n dise\u00f1adas para que sigas un conjunto de instrucciones paso a paso y te familiarices con el entorno y las herramientas utilizadas en Big Data. Cada pr\u00e1ctica cubre un aspecto clave del entorno de Hadoop y su ecosistema.</p>"},{"location":"ut3-ecosistema-hadoop/tareas/#1-introduccion-a-apache-pig-en-modo-local","title":"1. Introducci\u00f3n a Apache Pig en Modo Local","text":"<ul> <li>Descripci\u00f3n: Aprende a utilizar Apache Pig en modo local para procesar un archivo CSV de ejemplo. Cargar\u00e1s, filtrar\u00e1s, ordenar\u00e1s y guardar\u00e1s datos en el sistema de archivos local.  </li> <li>Objetivos:<ul> <li>Cargar un archivo CSV en Pig.</li> <li>Filtrar registros de usuarios seg\u00fan un criterio de publicaciones.</li> <li>Ordenar los usuarios en funci\u00f3n del n\u00famero de publicaciones.</li> <li>Guardar los resultados filtrados y ordenados en el sistema de archivos local.</li> </ul> </li> </ul>"},{"location":"ut3-ecosistema-hadoop/tareas/#2-introduccion-a-apache-hive","title":"2. Introducci\u00f3n a Apache Hive","text":"<ul> <li>Descripci\u00f3n: Aprende a manejar datos con Apache Hive, desde la creaci\u00f3n de bases de datos hasta consultas avanzadas. Incluye el uso de particiones y operaciones b\u00e1sicas en HDFS.  </li> <li>Objetivos:<ul> <li>Crear una base de datos y tablas particionadas.</li> <li>Insertar, actualizar y eliminar registros en tablas.</li> <li>Ejecutar consultas para analizar datos particionados.</li> <li>Validar las particiones en HDFS y documentar el proceso.</li> </ul> </li> </ul>"},{"location":"ut3-ecosistema-hadoop/tareas/#tareas-de-pig","title":"Tareas de Pig","text":"<p>Las tareas de Pig son actividades pr\u00e1cticas donde aplicar\u00e1s tus conocimientos y resolver\u00e1s problemas utilizando Apache Pig.</p>"},{"location":"ut3-ecosistema-hadoop/tareas/#1-tarea-basica-de-pig-para-ventas","title":"1. Tarea B\u00e1sica de Pig para Ventas","text":"<ul> <li>Descripci\u00f3n: Trabaja con un archivo CSV de datos de ventas y realiza consultas de filtrado y ordenaci\u00f3n usando Apache Pig.  </li> <li>Objetivos:<ul> <li>Cargar y explorar el archivo <code>ventas.csv</code>.</li> <li>Filtrar y ordenar los datos seg\u00fan criterios espec\u00edficos.</li> <li>Practicar la manipulaci\u00f3n de datos en Pig mediante operadores b\u00e1sicos.</li> </ul> </li> </ul>"},{"location":"ut3-ecosistema-hadoop/tareas/#2-tarea-de-pig-para-peliculas","title":"2. Tarea de Pig para Pel\u00edculas","text":"<ul> <li>Descripci\u00f3n: Usa un archivo CSV de pel\u00edculas para realizar consultas y an\u00e1lisis en Apache Pig.  </li> <li>Objetivos:<ul> <li>Cargar el archivo <code>peliculas_streaming.csv</code>.</li> <li>Filtrar y agrupar datos por atributos como g\u00e9nero y puntuaci\u00f3n.</li> <li>Calcular m\u00e9tricas como promedios y totales.</li> </ul> </li> </ul>"},{"location":"ut3-ecosistema-hadoop/tareas/#3-tarea-de-analisis-de-dataset-con-pig","title":"3. Tarea de An\u00e1lisis de Dataset con Pig","text":"<ul> <li>Descripci\u00f3n: Elige un dataset real desde Kaggle, procesa los datos y ejecuta consultas en Apache Pig para generar reportes \u00fatiles.  </li> <li>Objetivos:<ul> <li>Seleccionar y descargar un dataset en formato CSV.</li> <li>Realizar filtros, agrupaciones y c\u00e1lculos relevantes.</li> <li>Ordenar los datos seg\u00fan criterios espec\u00edficos y guardar resultados procesados.</li> </ul> </li> </ul>"},{"location":"ut3-ecosistema-hadoop/tareas/#tareas-de-hive","title":"Tareas de Hive","text":"<p>Las tareas de Hive son actividades pr\u00e1cticas donde aplicar\u00e1s tus conocimientos y resolver\u00e1s problemas utilizando Apache Hive.</p>"},{"location":"ut3-ecosistema-hadoop/tareas/#1-tarea-de-consulta-sobre-empleados-con-hive","title":"1. Tarea de Consulta sobre Empleados con Hive","text":"<ul> <li>Descripci\u00f3n: Usa Apache Hive para realizar consultas sobre un conjunto de datos de empleados.</li> <li>Objetivos:<ul> <li>Crear una base de datos y tablas en Hive.</li> <li>Ejecutar consultas para obtener informaci\u00f3n relevante.</li> </ul> </li> </ul>"},{"location":"ut3-ecosistema-hadoop/tareas/#2-tarea-analisis-avanzado-de-datos-de-ventas-con-hive","title":"2. Tarea: An\u00e1lisis Avanzado de Datos de Ventas con Hive","text":"<ul> <li>Descripci\u00f3n: Realiza un an\u00e1lisis avanzado de un conjunto de datos de ventas utilizando Apache Hive. Se trabajar\u00e1 con tablas particionadas y clusterizadas para optimizar las consultas.  </li> <li>Objetivos:<ul> <li>Crear tablas particionadas y clusterizadas.</li> <li>Cargar datos desde una tabla externa en HDFS.</li> <li>Realizar consultas avanzadas que incluyan funciones de fecha y an\u00e1lisis por categor\u00edas.</li> <li>Generar reportes detallados sobre el desempe\u00f1o de las ventas por regi\u00f3n, producto y cliente.</li> <li>Documentar el proceso mediante capturas de pantalla de cada paso.</li> </ul> </li> </ul>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/1_introduccion_apache_hive/","title":"Tarea: Manejo de Datos en Apache Hive \ud83d\udc1d","text":""},{"location":"ut3-ecosistema-hadoop/tareas/hive/1_introduccion_apache_hive/#objetivo","title":"Objetivo:","text":"<p>Practicar la creaci\u00f3n y gesti\u00f3n de bases de datos y tablas en Apache Hive, incluyendo el uso de particiones, ejecuci\u00f3n de consultas b\u00e1sicas y avanzadas, y verificaci\u00f3n de datos en HDFS.</p>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/1_introduccion_apache_hive/#parte-1-creacion-de-la-base-de-datos","title":"Parte 1: Creaci\u00f3n de la Base de Datos","text":"<ol> <li>Crea una base de datos llamada <code>tienda_online</code>:    <pre><code>CREATE DATABASE tienda_online;\n</code></pre></li> <li>Comprueba que la base de datos se haya creado correctamente:    <pre><code>SHOW DATABASES;\n</code></pre></li> </ol>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/1_introduccion_apache_hive/#parte-2-creacion-de-tablas-con-particiones","title":"Parte 2: Creaci\u00f3n de Tablas con Particiones","text":"<ol> <li>Dentro de la base de datos <code>tienda_online</code>, crea una tabla llamada <code>ventas</code> con las siguientes columnas:</li> <li><code>id INT</code></li> <li><code>producto STRING</code></li> <li><code>cantidad INT</code></li> <li>Particionada por <code>fecha STRING</code>.</li> </ol> <pre><code>USE tienda_online;\n\nCREATE TABLE ventas (\n  id INT,\n  producto STRING,\n  cantidad INT\n)\nPARTITIONED BY (fecha STRING)\nSTORED AS PARQUET;\n</code></pre> <ol> <li>Comprueba que la tabla se haya creado correctamente:    <pre><code>SHOW TABLES;\nDESCRIBE FORMATTED ventas;\n</code></pre></li> </ol>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/1_introduccion_apache_hive/#parte-3-insertar-datos","title":"Parte 3: Insertar Datos","text":"<ol> <li>Inserta los siguientes registros en las particiones correspondientes:</li> <li><code>(1, 'Laptop', 10, '2024-11-01')</code></li> <li><code>(2, 'Mouse', 25, '2024-11-02')</code></li> <li><code>(3, 'Teclado', 15, '2024-11-03')</code></li> </ol> <pre><code>INSERT INTO TABLE ventas PARTITION (fecha='2024-11-01') VALUES (1, 'Laptop', 10);\nINSERT INTO TABLE ventas PARTITION (fecha='2024-11-02') VALUES (2, 'Mouse', 25);\nINSERT INTO TABLE ventas PARTITION (fecha='2024-11-03') VALUES (3, 'Teclado', 15);\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/1_introduccion_apache_hive/#parte-4-actualizacion-y-eliminacion-de-datos","title":"Parte 4: Actualizaci\u00f3n y Eliminaci\u00f3n de Datos","text":"<ol> <li> <p>Actualiza el registro del producto <code>Mouse</code> para cambiar la cantidad a <code>30</code>:    <pre><code>UPDATE ventas\nSET cantidad = 30\nWHERE producto = 'Mouse' AND fecha = '2024-11-02';\n</code></pre></p> </li> <li> <p>Elimina el registro del producto <code>Teclado</code>:    <pre><code>DELETE FROM ventas\nWHERE producto = 'Teclado' AND fecha = '2024-11-03';\n</code></pre></p> </li> </ol>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/1_introduccion_apache_hive/#parte-5-ejecucion-de-consultas","title":"Parte 5: Ejecuci\u00f3n de Consultas","text":"<ol> <li> <p>Muestra todas las ventas realizadas en la fecha <code>2024-11-01</code>:    <pre><code>SELECT * FROM ventas WHERE fecha = '2024-11-01';\n</code></pre></p> </li> <li> <p>Muestra el total de productos vendidos agrupados por fecha:    <pre><code>SELECT fecha, SUM(cantidad) AS total_cantidad\nFROM ventas\nGROUP BY fecha;\n</code></pre></p> </li> <li> <p>Ordena las ventas por cantidad en orden descendente:    <pre><code>SELECT * FROM ventas\nORDER BY cantidad DESC;\n</code></pre></p> </li> </ol>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/1_introduccion_apache_hive/#parte-6-validacion-en-hdfs","title":"Parte 6: Validaci\u00f3n en HDFS","text":"<ol> <li> <p>Lista las particiones de la tabla <code>ventas</code> en HDFS:    <pre><code>hdfs dfs -ls /user/hive/warehouse/tienda_online.db/ventas\n</code></pre></p> </li> <li> <p>Toma capturas de pantalla del contenido del directorio de la base de datos y de las particiones de la tabla <code>ventas</code>. Por ejemplo:</p> </li> <li><code>/user/hive/warehouse/tienda_online.db/ventas/fecha=2024-11-01/</code></li> </ol> <p>\u00a1Con estas instrucciones completas, estar\u00e1s listo para manejar datos en Apache Hive de forma eficiente! \ud83d\udc1d</p>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/2_empleados/","title":"Pr\u00e1ctica: Consultas sobre empleados","text":""},{"location":"ut3-ecosistema-hadoop/tareas/hive/2_empleados/#csv-de-ejemplo","title":"CSV de Ejemplo","text":"<pre><code>id,nombre,edad,departamento,salario\n1,Ana,30,IT,50000\n2,Luis,45,Finanzas,60000\n3,Mar\u00eda,28,IT,55000\n4,Pablo,35,RH,45000\n5,Clara,50,Finanzas,70000\n6,Jorge,40,IT,52000\n7,Sof\u00eda,25,Ventas,48000\n8,Carlos,32,RH,46000\n9,Laura,29,IT,54000\n10,Marta,41,Ventas,47000\n11,Pedro,38,Finanzas,68000\n12,Isabel,27,RH,44000\n13,Antonio,33,IT,53000\n14,Luc\u00eda,26,RH,49000\n15,Fernando,48,Finanzas,72000\n16,Raquel,37,Ventas,56000\n17,Esteban,31,IT,51000\n18,Patricia,24,RH,45000\n19,Rosa,39,Ventas,58000\n20,Manuel,44,IT,60000\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/2_empleados/#parte-1-crear-tablas-y-cargar-datos-en-hive","title":"Parte 1: Crear tablas y cargar datos en Hive","text":""},{"location":"ut3-ecosistema-hadoop/tareas/hive/2_empleados/#1-iniciar-el-cliente-hive","title":"1. Iniciar el cliente Hive","text":"<ul> <li>Abre la interfaz de l\u00ednea de comandos de Hive.</li> </ul>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/2_empleados/#2-crear-una-base-de-datos","title":"2. Crear una base de datos","text":"<ul> <li>Crea una nueva base de datos llamada <code>empresa</code> y selecciona esta base de datos para trabajar en ella.</li> </ul>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/2_empleados/#3-crear-una-tabla-en-hive","title":"3. Crear una tabla en Hive","text":"<ul> <li>Crea una tabla externa que haga referencia al archivo CSV cargado en HDFS. Define los campos de la tabla bas\u00e1ndote en los datos de ejemplo.<ul> <li>Pista: Ejemplo de tabla externa guardada en un fichero en HDFS <pre><code>CREATE EXTERNAL TABLE productos (\n    producto_id INT,\n    nombre_producto STRING,\n    categoria STRING,\n    precio FLOAT,\n    cantidad_en_stock INT\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ','\nSTORED AS TEXTFILE\nLOCATION '/user/hive_data/productos/';\n</code></pre></li> </ul> </li> </ul>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/2_empleados/#4-validar-que-los-datos-se-han-cargado-correctamente","title":"4. Validar que los datos se han cargado correctamente","text":"<ul> <li>Realiza una consulta para verificar que los datos est\u00e1n disponibles en la tabla.</li> </ul>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/2_empleados/#parte-2-consultas-basicas","title":"Parte 2: Consultas b\u00e1sicas","text":""},{"location":"ut3-ecosistema-hadoop/tareas/hive/2_empleados/#1-consultas-de-seleccion","title":"1. Consultas de selecci\u00f3n","text":"<ol> <li>Lista todos los empleados.</li> <li>Selecciona solo los nombres y salarios de los empleados.</li> <li>Selecciona empleados que pertenecen al departamento \"IT\".</li> </ol>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/2_empleados/#2-filtros","title":"2. Filtros","text":"<ol> <li>Encuentra empleados mayores de 40 a\u00f1os.</li> <li>Encuentra empleados del departamento \"Finanzas\" que ganan m\u00e1s de 60000.</li> <li>Encuentra empleados cuyos nombres empiezan con la letra 'M'.</li> </ol>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/2_empleados/#3-agrupaciones","title":"3. Agrupaciones","text":"<ol> <li>Calcula el salario promedio por departamento.</li> <li>Encuentra el n\u00famero total de empleados por departamento.</li> <li>Encuentra el salario m\u00e1ximo y m\u00ednimo en cada departamento.</li> </ol>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/2_empleados/#4-ordenacion","title":"4. Ordenaci\u00f3n","text":"<ol> <li>Ordena a los empleados por salario en orden descendente.</li> <li>Ordena a los empleados por edad en orden ascendente y por salario en orden descendente.</li> </ol>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/2_empleados/#parte-3-consultas-avanzadas","title":"Parte 3: Consultas avanzadas","text":""},{"location":"ut3-ecosistema-hadoop/tareas/hive/2_empleados/#1-subconsultas","title":"1. Subconsultas","text":"<ol> <li>Encuentra los empleados que ganan m\u00e1s que el salario promedio de su departamento.</li> <li>Encuentra los departamentos con un salario promedio mayor a 55000.</li> </ol>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/2_empleados/#2-funciones-integradas","title":"2. Funciones integradas","text":"<ol> <li>Calcula la edad promedio de los empleados.</li> <li>Encuentra el total de salarios en toda la empresa.</li> <li>Encuentra el salario m\u00e1s alto y m\u00e1s bajo en toda la empresa.</li> <li>Calcula el porcentaje de empleados en cada departamento respecto al total.</li> </ol>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/2_empleados/#3-operaciones-con-fechas","title":"3. Operaciones con fechas","text":"<ol> <li>Encuentra empleados contratados en los \u00faltimos 5 a\u00f1os.<ul> <li>Pista: Usa funciones de fechas como <code>CURRENT_DATE</code> y operadores para calcular diferencias de a\u00f1os.</li> </ul> </li> </ol>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/2_empleados/#4-uso-de-case","title":"4. Uso de CASE","text":"<ol> <li>Clasifica a los empleados en categor\u00edas como \"Junior\", \"Mid-Level\", y \"Senior\" seg\u00fan su salario.<ul> <li>Pista: Usa la cl\u00e1usula <code>CASE</code> para establecer rangos de clasificaci\u00f3n.</li> </ul> </li> </ol>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/2_empleados/#parte-4-exportar-los-resultados","title":"Parte 4: Exportar los resultados","text":"<ol> <li> <p>Exporta los nombres y salarios de los empleados del departamento \"IT\" a un archivo en HDFS.</p> <ul> <li>Pista: Usa <code>INSERT OVERWRITE DIRECTORY</code> junto con el formato de salida especificado.</li> </ul> </li> <li> <p>Exporta una consulta que agrupa datos por departamento, incluyendo el promedio de salario y el n\u00famero total de empleados.</p> <ul> <li>Pista: Combina la agrupaci\u00f3n (<code>GROUP BY</code>) con la exportaci\u00f3n usando <code>INSERT OVERWRITE</code>.</li> </ul> </li> </ol>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/3_ventas/","title":"Tarea: An\u00e1lisis Avanzado de Datos de Ventas con Hive","text":""},{"location":"ut3-ecosistema-hadoop/tareas/hive/3_ventas/#escenario","title":"Escenario","text":"<p>Una empresa multinacional desea analizar los datos de ventas almacenados en un sistema de almacenamiento distribuido (HDFS). Los datos contienen informaci\u00f3n sobre ventas, clientes y productos. Necesitamos realizar consultas avanzadas para extraer insights clave que ayuden a optimizar estrategias comerciales y operativas.</p>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/3_ventas/#objetivos","title":"Objetivos","text":"<ol> <li>Crear tablas particionadas y clusterizadas (bucketizadas).</li> <li>Cargar y manipular datos desde tablas externas.</li> <li>Realizar consultas avanzadas usando funciones de Hive.</li> <li>Generar informes agregados de ventas por regi\u00f3n y producto.</li> <li>Documentar cada paso mediante capturas de pantalla para su evaluaci\u00f3n.</li> </ol>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/3_ventas/#pasos","title":"Pasos","text":""},{"location":"ut3-ecosistema-hadoop/tareas/hive/3_ventas/#1-crear-una-tabla-externa","title":"1. Crear una Tabla Externa","text":"<p>Crea una tabla externa para cargar los datos iniciales desde HDFS. La ruta estar\u00e1 ubicada en el directorio <code>/user/hive/</code>.</p> <pre><code>CREATE EXTERNAL TABLE ventas_raw (\n    venta_id INT,\n    cliente_id INT,\n    producto_id INT,\n    cantidad INT,\n    precio FLOAT,\n    fecha STRING,\n    region STRING\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ','\nSTORED AS TEXTFILE\nLOCATION '/user/hive/data/ventas_raw/';\n</code></pre> <p>Captura requerida:</p> <ul> <li>Muestra el comando ejecutado y el resultado de la creaci\u00f3n de la tabla (<code>Table created successfully</code>).</li> <li>Explica c\u00f3mo esta tabla sirve como punto de partida para las siguientes operaciones.</li> </ul>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/3_ventas/#2-crear-una-tabla-particionada","title":"2. Crear una Tabla Particionada","text":"<p>Crea una tabla particionada por regi\u00f3n y clusterizada (bucketizada) por cliente. La ruta de almacenamiento ser\u00e1 gestionada autom\u00e1ticamente por Hive en HDFS, bajo el directorio por defecto <code>/user/hive/warehouse/</code>.</p> <pre><code>CREATE TABLE ventas_particionadas (\n    venta_id INT,\n    cliente_id INT,\n    producto_id INT,\n    cantidad INT,\n    precio FLOAT,\n    fecha STRING\n)\nPARTITIONED BY (region STRING)\nCLUSTERED BY (cliente_id) INTO 8 BUCKETS\nSTORED AS ORC;\n</code></pre> <p>Captura requerida:</p> <ul> <li>Muestra el comando ejecutado y el resultado de la creaci\u00f3n de la tabla particionada (<code>Table created successfully</code>).</li> <li>Explica c\u00f3mo el particionamiento y la clusterizaci\u00f3n optimizan las consultas en esta tabla.</li> </ul>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/3_ventas/#3-insertar-datos-con-particionamiento","title":"3. Insertar Datos con Particionamiento","text":"<p>Carga los datos de la tabla sin procesar (<code>ventas_raw</code>) a la tabla particionada (<code>ventas_particionadas</code>).</p> <pre><code>SET hive.exec.dynamic.partition=true;\nSET hive.exec.dynamic.partition.mode=nonstrict;\n\nINSERT OVERWRITE TABLE ventas_particionadas PARTITION (region)\nSELECT\n    venta_id,\n    cliente_id,\n    producto_id,\n    cantidad,\n    precio,\n    fecha,\n    region\nFROM ventas_raw;\n</code></pre> <p>Captura requerida:</p> <ul> <li>Muestra el comando ejecutado y el resultado del proceso de inserci\u00f3n.</li> <li>Explica c\u00f3mo la partici\u00f3n se refleja en la estructura del directorio en HDFS.</li> </ul>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/3_ventas/#4-consultas-avanzadas","title":"4. Consultas Avanzadas","text":"<p>Realiza las siguientes consultas para extraer insights clave:</p>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/3_ventas/#a-calcular-el-ingreso-total-por-region-y-producto","title":"a) Calcular el Ingreso Total por Regi\u00f3n y Producto","text":"<pre><code>SELECT\n    region,\n    producto_id,\n    SUM(cantidad * precio) AS ingreso_total\nFROM ventas_particionadas\nGROUP BY region, producto_id\nORDER BY ingreso_total DESC;\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/3_ventas/#b-identificar-los-clientes-con-mayor-gasto","title":"b) Identificar los Clientes con Mayor Gasto","text":"<pre><code>SELECT\n    cliente_id,\n    SUM(cantidad * precio) AS gasto_total\nFROM ventas_particionadas\nGROUP BY cliente_id\nORDER BY gasto_total DESC\nLIMIT 10;\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/3_ventas/#c-uso-de-funciones-de-ventana-para-el-ranking","title":"c) Uso de Funciones de Ventana para el Ranking","text":"<pre><code>SELECT\n    cliente_id,\n    region,\n    RANK() OVER (PARTITION BY region ORDER BY SUM(cantidad * precio) DESC) AS ranking\nFROM ventas_particionadas\nGROUP BY cliente_id, region;\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/3_ventas/#d-consultas-para-que-resuelvan-los-estudiantes","title":"d) Consultas para que resuelvan los estudiantes","text":"<ol> <li> <p>Ingreso Total por Mes:    Calcula el ingreso total generado en cada mes, ordenado de mayor a menor ingreso.</p> <p>Pista:</p> <ul> <li>Usa la funci\u00f3n <code>month(fecha)</code> para extraer el n\u00famero del mes de la columna <code>fecha</code>.</li> <li>Agrupa los resultados por mes y utiliza la funci\u00f3n <code>SUM()</code> para calcular el ingreso total.</li> </ul> </li> <li> <p>Productos M\u00e1s Vendidos:    Identifica los 5 productos m\u00e1s vendidos (por cantidad total) en cada regi\u00f3n.</p> <p>Pista:</p> <ul> <li>Agrupa los datos por <code>producto_id</code> y <code>region</code> para sumar las cantidades vendidas con <code>SUM(cantidad)</code>.</li> <li>Usa la funci\u00f3n <code>RANK()</code> o <code>ROW_NUMBER()</code> en una subconsulta para seleccionar los 5 primeros productos por regi\u00f3n.</li> </ul> </li> <li> <p>Desempe\u00f1o Regional:    Encuentra la regi\u00f3n con el ingreso promedio m\u00e1s alto por transacci\u00f3n.</p> <p>Pista:</p> <ul> <li>Calcula el ingreso promedio por transacci\u00f3n dividiendo <code>SUM(cantidad * precio)</code> entre <code>COUNT(venta_id)</code> para cada regi\u00f3n.</li> <li>Ordena los resultados en orden descendente para identificar la regi\u00f3n con el ingreso m\u00e1s alto.</li> </ul> </li> <li> <p>Clientes Frecuentes:    Determina los clientes que han realizado m\u00e1s de 10 compras en un a\u00f1o, mostrando la cantidad de compras por cliente.</p> <p>Pista:</p> <ul> <li>Usa la funci\u00f3n <code>year(fecha)</code> para agrupar las ventas por a\u00f1o.</li> <li>Agrupa por <code>cliente_id</code> y el a\u00f1o, y utiliza <code>COUNT(venta_id)</code> para contar las compras.</li> <li>Filtra los resultados para incluir solo aquellos con m\u00e1s de 10 compras.</li> </ul> </li> <li> <p>An\u00e1lisis Temporal:    Calcula el ingreso promedio por producto para cada d\u00eda de la semana, agrupando las ventas por d\u00eda.</p> <p>Pista:</p> <ul> <li>Usa la funci\u00f3n <code>dayofweek(fecha)</code> para extraer el d\u00eda de la semana (1 para domingo, 2 para lunes, etc.).</li> <li>Agrupa los resultados por <code>dayofweek(fecha)</code> y <code>producto_id</code>, y utiliza <code>AVG(cantidad * precio)</code> para calcular el ingreso promedio.</li> </ul> </li> </ol> <p>Captura requerida (para todas las consultas):</p> <ul> <li>Muestra la ejecuci\u00f3n del c\u00f3digo SQL y una vista parcial de los resultados.</li> <li>Explica qu\u00e9 aspectos del an\u00e1lisis aportan valor al negocio.</li> </ul>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/3_ventas/#datos-de-ejemplo","title":"Datos de Ejemplo","text":"<p>Usa los siguientes comandos <code>INSERT INTO</code> para generar datos de prueba en la tabla <code>ventas_raw</code>.</p> <pre><code>INSERT INTO ventas_raw VALUES \n(1, 101, 201, 3, 150.00, '2024-01-15', 'Norte'),\n(2, 102, 202, 1, 250.00, '2024-01-16', 'Sur'),\n(3, 103, 201, 2, 100.00, '2024-01-17', 'Norte'),\n(4, 104, 203, 5, 500.00, '2024-01-18', 'Este'),\n(5, 105, 204, 1, 300.00, '2024-01-19', 'Oeste'),\n(6, 106, 202, 3, 750.00, '2024-01-20', 'Norte'),\n(7, 107, 205, 4, 200.00, '2024-01-21', 'Sur'),\n(8, 108, 201, 2, 150.00, '2024-01-22', 'Este');\n</code></pre> <p>Captura requerida:</p> <ul> <li>Muestra el comando de inserci\u00f3n y los datos correctamente cargados en la tabla <code>ventas_raw</code>.</li> </ul>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/3_ventas/#explicacion-particionamiento-y-clusterizacion","title":"Explicaci\u00f3n: Particionamiento y Clusterizaci\u00f3n","text":""},{"location":"ut3-ecosistema-hadoop/tareas/hive/3_ventas/#particionamiento","title":"Particionamiento","text":"<ul> <li> <p>\u00bfQu\u00e9 es?   Divide f\u00edsicamente los datos en diferentes directorios basados en el valor de una o m\u00e1s columnas (por ejemplo, <code>region</code> en este caso).</p> </li> <li> <p>\u00bfPara qu\u00e9 sirve?   Optimiza el acceso a los datos al permitir que las consultas solo lean las particiones relevantes, reduciendo el tiempo de ejecuci\u00f3n y el costo de lectura.</p> </li> </ul>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/3_ventas/#clusterizacion-bucketization","title":"Clusterizaci\u00f3n (Bucketization)","text":"<ul> <li> <p>\u00bfQu\u00e9 es?   Agrupa los datos dentro de una partici\u00f3n utilizando una columna clave (por ejemplo, <code>cliente_id</code>) en un n\u00famero fijo de buckets.</p> </li> <li> <p>\u00bfPara qu\u00e9 sirve? </p> </li> <li>Mejora la eficiencia de las uniones (JOINs) y agrupaciones (GROUP BY).</li> <li>Facilita el trabajo con datos m\u00e1s equilibrados y distribuidos.</li> </ul>"},{"location":"ut3-ecosistema-hadoop/tareas/hive/3_ventas/#resultados-esperados","title":"Resultados Esperados","text":"<ol> <li>Una tabla particionada y clusterizada que facilite consultas m\u00e1s eficientes.</li> <li>Consultas avanzadas que respondan preguntas clave del negocio.</li> <li>Informes detallados sobre las ventas agrupados por regi\u00f3n y producto.</li> <li>Capturas de cada paso que expliquen el flujo completo del an\u00e1lisis.</li> </ol>"},{"location":"ut3-ecosistema-hadoop/tareas/pig/1_introduccion_apache_pig_local/","title":"Pr\u00e1ctica: Introducci\u00f3n a Apache Pig en Modo Local","text":""},{"location":"ut3-ecosistema-hadoop/tareas/pig/1_introduccion_apache_pig_local/#objetivo","title":"Objetivo","text":"<p>Crear un script de Pig que procese un conjunto de datos sencillo y realice operaciones b\u00e1sicas de filtrado y ordenaci\u00f3n, ejecut\u00e1ndose l\u00ednea por l\u00ednea en modo local.</p>"},{"location":"ut3-ecosistema-hadoop/tareas/pig/1_introduccion_apache_pig_local/#descripcion","title":"Descripci\u00f3n","text":"<p>En esta pr\u00e1ctica, el estudiante desarrollar\u00e1 un script en Apache Pig para analizar un archivo de datos de muestra. El objetivo es comprender la estructura de un script en Pig y familiarizarse con funciones b\u00e1sicas como <code>LOAD</code>, <code>FILTER</code>, <code>ORDER</code>, y <code>STORE</code>, ejecut\u00e1ndolo en modo local (<code>pig -x local</code>).</p>"},{"location":"ut3-ecosistema-hadoop/tareas/pig/1_introduccion_apache_pig_local/#requisitos-previos","title":"Requisitos previos","text":"<ul> <li>Conocimientos b\u00e1sicos de Hadoop.</li> <li>Apache Pig instalado en el entorno local.</li> <li>Archivo de datos de ejemplo en formato CSV cargado en el sistema de archivos local.</li> </ul>"},{"location":"ut3-ecosistema-hadoop/tareas/pig/1_introduccion_apache_pig_local/#instrucciones","title":"Instrucciones","text":""},{"location":"ut3-ecosistema-hadoop/tareas/pig/1_introduccion_apache_pig_local/#1-archivo-de-datos","title":"1. Archivo de datos","text":"<p>Crea un archivo llamado <code>usuarios.csv</code> con el siguiente contenido, que contiene datos sobre usuarios de una red social (sin la columna \"genero\" y sin tildes).</p> <pre><code>nombre,edad,localidad,publicaciones\nJuan,25,Madrid,8\nMaria,30,Valencia,15\nJose,22,Barcelona,5\nAna,28,Sevilla,12\nPedro,33,Malaga,9\nLuisa,29,Zaragoza,4\n</code></pre> <p>Guarda el archivo en el sistema de archivos local, en la misma carpeta desde la cual ejecutar\u00e1s Pig.</p>"},{"location":"ut3-ecosistema-hadoop/tareas/pig/1_introduccion_apache_pig_local/#2-objetivos-del-script-de-pig","title":"2. Objetivos del script de Pig","text":"<p>El script de Pig debe realizar las siguientes operaciones en el archivo <code>usuarios.csv</code>:</p> <ul> <li>Cargar los datos: Utiliza la instrucci\u00f3n <code>LOAD</code> para cargar el archivo <code>usuarios.csv</code> y define los tipos de datos correspondientes.</li> <li>Filtrar los datos: Filtra los registros de usuarios que tengan m\u00e1s de 10 publicaciones.</li> <li>Ordenar por n\u00famero de publicaciones: Ordena los usuarios en funci\u00f3n de la cantidad de publicaciones en orden descendente.</li> <li>Guardar los resultados: Guarda los resultados filtrados y ordenados en el sistema de archivos local.</li> </ul>"},{"location":"ut3-ecosistema-hadoop/tareas/pig/1_introduccion_apache_pig_local/#3-ejecucion-linea-por-linea-en-modo-local","title":"3. Ejecuci\u00f3n l\u00ednea por l\u00ednea en modo local","text":"<p>Para ejecutar cada l\u00ednea del script en modo local, abre la terminal y ejecuta <code>pig -x local</code>. Aseg\u00farate de que est\u00e1s en el directorio donde se encuentra <code>usuarios.csv</code> y ejecuta cada l\u00ednea del script una a una en el shell de Pig.</p>"},{"location":"ut3-ecosistema-hadoop/tareas/pig/1_introduccion_apache_pig_local/#cargar-el-archivo","title":"Cargar el archivo","text":"<pre><code>-- Cargar el archivo 'usuarios.csv' desde el sistema de archivos local, usando comas como delimitador.\n-- Definir los campos con sus respectivos tipos de datos: 'nombre' como cadena de caracteres (chararray),\n-- 'edad' como entero (int), 'localidad' como cadena de caracteres y 'publicaciones' como entero.\nusuarios = LOAD 'usuarios.csv' USING PigStorage(',') AS (nombre:chararray, edad:int, localidad:chararray, publicaciones:int);\n\n-- Verificar que los datos se hayan cargado correctamente\nDUMP usuarios;\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/tareas/pig/1_introduccion_apache_pig_local/#filtrar-usuarios-con-mas-de-10-publicaciones","title":"Filtrar usuarios con m\u00e1s de 10 publicaciones","text":"<pre><code>-- Filtrar los registros de la relacion 'usuarios' para obtener solo aquellos que tienen\n-- mas de 10 publicaciones. Esto crea una nueva relacion llamada 'usuarios_activos'.\nusuarios_activos = FILTER usuarios BY publicaciones &gt; 10;\n\n-- Verificar que el filtro ha funcionado\nDUMP usuarios_activos;\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/tareas/pig/1_introduccion_apache_pig_local/#ordenar-por-numero-de-publicaciones","title":"Ordenar por n\u00famero de publicaciones","text":"<pre><code>-- Ordenar la relacion 'usuarios_activos' en funcion del numero de publicaciones en orden descendente.\nusuarios_ordenados = ORDER usuarios_activos BY publicaciones DESC;\n\n-- Verificar que los datos estan ordenados\nDUMP usuarios_ordenados;\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/tareas/pig/1_introduccion_apache_pig_local/#guardar-los-resultados","title":"Guardar los resultados","text":"<pre><code>-- Almacenar los resultados de 'usuarios_activos' en el sistema de archivos local en un directorio\n-- llamado 'salida/usuarios_activos', usando comas como delimitador.\nSTORE usuarios_activos INTO 'salida/usuarios_activos' USING PigStorage(',');\n\n-- Almacenar los resultados de 'usuarios_ordenados' en el sistema de archivos local en un directorio\n-- llamado 'salida/usuarios_ordenados', usando comas como delimitador.\nSTORE usuarios_ordenados INTO 'salida/usuarios_ordenados' USING PigStorage(',');\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/tareas/pig/1_introduccion_apache_pig_local/#4-verificar-los-resultados-en-el-sistema-de-archivos-local","title":"4. Verificar los resultados en el sistema de archivos local","text":"<p>Para confirmar que los datos se han almacenado correctamente, revisa los archivos de salida en el sistema de archivos local:</p> <pre><code>ls salida/usuarios_activos\ncat salida/usuarios_activos/part-m-00000\n</code></pre> <pre><code>ls salida/usuarios_ordenados\ncat salida/usuarios_ordenados/part-m-00000\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/tareas/pig/1_introduccion_apache_pig_local/#script-de-pig-completo","title":"Script de Pig Completo","text":"<pre><code>usuarios = LOAD 'usuarios.csv' USING PigStorage(',') AS (nombre:chararray, edad:int, localidad:chararray, publicaciones:int);\nDUMP usuarios;\n\nusuarios_activos = FILTER usuarios BY publicaciones &gt; 10;\nDUMP usuarios_activos;\n\nusuarios_ordenados = ORDER usuarios_activos BY publicaciones DESC;\nDUMP usuarios_ordenados;\n\nSTORE usuarios_activos INTO 'salida/usuarios_activos' USING PigStorage(',');\nSTORE usuarios_ordenados INTO 'salida/usuarios_ordenados' USING PigStorage(',');\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/tareas/pig/1_introduccion_apache_pig_local/#posibles-errores-y-soluciones","title":"Posibles errores y soluciones","text":""},{"location":"ut3-ecosistema-hadoop/tareas/pig/1_introduccion_apache_pig_local/#1-error-output-directory-already-exists","title":"1. Error: <code>output directory already exists</code>","text":"<ul> <li>Descripci\u00f3n: Este error ocurre cuando intentas almacenar resultados en un directorio que ya existe.</li> <li>Soluci\u00f3n: Elimina el directorio de salida existente antes de volver a ejecutar el script.      <pre><code>rm -r salida/usuarios_activos\nrm -r salida/usuarios_ordenados\n</code></pre></li> </ul>"},{"location":"ut3-ecosistema-hadoop/tareas/pig/1_introduccion_apache_pig_local/#2-error-cannot-cast-from-chararray-to-int","title":"2. Error: <code>Cannot cast from chararray to int</code>","text":"<ul> <li>Descripci\u00f3n: Puede ocurrir si hay valores no enteros en las columnas <code>edad</code> o <code>publicaciones</code>.</li> <li>Soluci\u00f3n: Verifica que los datos en estas columnas son num\u00e9ricos y est\u00e1n correctamente formateados en el archivo CSV.</li> </ul>"},{"location":"ut3-ecosistema-hadoop/tareas/pig/1_introduccion_apache_pig_local/#3-error-file-not-found-usuarioscsv","title":"3. Error: <code>File not found: usuarios.csv</code>","text":"<ul> <li>Descripci\u00f3n: Ocurre cuando Pig no encuentra el archivo especificado en la ruta indicada.</li> <li>Soluci\u00f3n: Confirma que <code>usuarios.csv</code> est\u00e1 en el directorio desde el cual est\u00e1s ejecutando el script o proporciona la ruta completa.</li> </ul>"},{"location":"ut3-ecosistema-hadoop/tareas/pig/2_ventas/","title":"Tarea: An\u00e1lisis de Datos de Ventas usando Apache Pig","text":""},{"location":"ut3-ecosistema-hadoop/tareas/pig/2_ventas/#objetivo","title":"Objetivo","text":"<p>En esta tarea, usar\u00e1s Apache Pig para analizar un conjunto de datos de ventas de una tienda. El objetivo es realizar filtrado y ordenaci\u00f3n de los datos a partir de un archivo CSV proporcionado. Este archivo contiene informaci\u00f3n sobre las transacciones diarias de la tienda, incluyendo detalles como fecha, cliente, producto, cantidad y precio unitario.</p>"},{"location":"ut3-ecosistema-hadoop/tareas/pig/2_ventas/#archivo-csv-de-entrada","title":"Archivo CSV de Entrada","text":"<p>El archivo CSV de entrada <code>ventas.csv</code> tiene el siguiente formato y contenido de ejemplo:</p> <pre><code>fecha,id_cliente,producto,cantidad,precio_unitario\n2024-01-01,1,televisor,1,500\n2024-01-01,2,laptop,1,800\n2024-01-02,3,tablet,2,200\n2024-01-02,1,smartphone,1,300\n2024-01-03,4,televisor,1,500\n2024-01-03,5,laptop,1,800\n2024-01-03,2,tablet,3,200\n2024-01-04,3,smartphone,2,300\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/tareas/pig/2_ventas/#ejercicios","title":"Ejercicios","text":"<ol> <li> <p>Filtrar ventas de productos espec\u00edficos y ordenar por cantidad vendida</p> <p>Filtra las ventas de productos espec\u00edficos, en este caso <code>laptop</code> y <code>tablet</code>, y ord\u00e9nalas por la cantidad vendida de mayor a menor.</p> <ul> <li>Instrucciones: <ul> <li>Carga los datos desde el archivo <code>ventas.csv</code>.</li> <li>Filtra solo las filas que corresponden a los productos <code>laptop</code> y <code>tablet</code>.</li> <li>Ordena el resultado por la columna <code>cantidad</code> en orden descendente.</li> </ul> </li> <li>Resultado esperado: Una lista ordenada de ventas de <code>laptop</code> y <code>tablet</code> con las mayores cantidades vendidas al inicio.</li> </ul> </li> <li> <p>Filtrar ventas que superen un valor de ingreso espec\u00edfico y ordenar por ingreso</p> <p>Filtra las ventas que generaron ingresos superiores a <code>$400</code> y ord\u00e9nalas de mayor a menor seg\u00fan el ingreso total, calculado como <code>cantidad * precio_unitario</code>.</p> <ul> <li>Instrucciones:<ul> <li>Carga los datos desde el archivo <code>ventas.csv</code>.</li> <li>Calcula una nueva columna llamada <code>ingreso</code> como el producto de <code>cantidad</code> y <code>precio_unitario</code>.</li> <li>Filtra las ventas donde <code>ingreso</code> es mayor a <code>$400</code>.</li> <li>Ordena el resultado por la columna <code>ingreso</code> en orden descendente.</li> </ul> </li> <li>Pista: Puedes calcular la columna <code>ingreso</code> con el siguiente comando:     <pre><code>ventas_con_ingreso = FOREACH ventas GENERATE fecha, id_cliente, producto, cantidad, precio_unitario, (cantidad * precio_unitario) AS ingreso;\n</code></pre></li> <li>Resultado esperado: Una lista ordenada de ventas con <code>ingreso &gt; $400</code>, desde los ingresos m\u00e1s altos hasta los m\u00e1s bajos.</li> </ul> </li> <li> <p>Filtrar por fecha espec\u00edfica y ordenar por precio unitario</p> <p>Filtra las ventas realizadas en una fecha espec\u00edfica, por ejemplo, <code>2024-01-03</code>, y ord\u00e9nalas de menor a mayor seg\u00fan el <code>precio_unitario</code>.</p> <ul> <li>Instrucciones:<ul> <li>Carga los datos desde el archivo <code>ventas.csv</code>.</li> <li>Filtra solo las ventas realizadas en la fecha <code>2024-01-03</code>.</li> <li>Ordena el resultado por <code>precio_unitario</code> en orden ascendente.</li> </ul> </li> <li>Resultado esperado: Una lista de ventas realizadas el <code>2024-01-03</code>, ordenada desde el precio unitario m\u00e1s bajo al m\u00e1s alto.</li> </ul> </li> </ol>"},{"location":"ut3-ecosistema-hadoop/tareas/pig/2_ventas/#ejercicios-de-ampliacion","title":"Ejercicios de Ampliaci\u00f3n","text":"<ol> <li> <p>Ventas Totales por Cliente</p> <p>Calcula el total de ventas realizadas por cada cliente, sumando el ingreso total generado por cada uno.</p> <ul> <li>Instrucciones:<ul> <li>Calcula una columna <code>ingreso</code> como <code>cantidad * precio_unitario</code>.</li> <li>Agrupa las ventas por <code>id_cliente</code> y suma el <code>ingreso</code> para cada cliente.</li> <li>Ordena los resultados en orden descendente de acuerdo con el ingreso total por cliente.</li> </ul> </li> <li>Resultado esperado: Una lista de clientes con el total de ingresos que han generado, ordenada de mayor a menor.</li> </ul> </li> <li> <p>Producto M\u00e1s Vendido por Mes</p> <p>Determina el producto m\u00e1s vendido por mes basado en la cantidad total.</p> <ul> <li>Instrucciones:<ul> <li>Extrae el mes de la columna <code>fecha</code> para agrupar los datos mensualmente.</li> <li>Agrupa las ventas por <code>mes</code> y <code>producto</code>.</li> <li>Calcula la cantidad total vendida de cada producto por mes.</li> <li>Ordena los resultados para mostrar el producto m\u00e1s vendido primero para cada mes.</li> </ul> </li> <li>Resultado esperado: Una lista de productos agrupados por mes, con el producto m\u00e1s vendido de cada mes en la parte superior.</li> </ul> </li> <li> <p>Promedio de Ingresos por D\u00eda de la Semana</p> <p>Calcula el ingreso promedio por cada d\u00eda de la semana para identificar patrones de ventas.</p> <ul> <li>Instrucciones:<ul> <li>Calcula una columna <code>ingreso</code> como <code>cantidad * precio_unitario</code>.</li> <li>Extrae el d\u00eda de la semana de la columna <code>fecha</code>.</li> <li>Agrupa las ventas por d\u00eda de la semana y calcula el promedio de <code>ingreso</code>.</li> </ul> </li> <li>Resultado esperado: Una lista con el ingreso promedio para cada d\u00eda de la semana.</li> </ul> </li> <li> <p>Top 3 Clientes por Ingreso</p> <p>Identifica los tres clientes que generaron el mayor ingreso total en el periodo registrado en el archivo.</p> <ul> <li>Instrucciones:<ul> <li>Calcula una columna <code>ingreso</code> como <code>cantidad * precio_unitario</code>.</li> <li>Agrupa las ventas por <code>id_cliente</code> y suma el <code>ingreso</code> para cada cliente.</li> <li>Ordena los resultados en orden descendente y selecciona los tres primeros clientes.</li> </ul> </li> <li>Resultado esperado: Una lista con los tres clientes que generaron el mayor ingreso total.</li> </ul> </li> <li> <p>An\u00e1lisis de Productos No Vendidos</p> <p>Identifica qu\u00e9 productos no se vendieron en un rango de fechas espec\u00edfico, por ejemplo, entre <code>2024-01-01</code> y <code>2024-01-04</code>.</p> <ul> <li>Instrucciones:<ul> <li>Lista todos los productos disponibles en el archivo usando <code>DISTINCT</code>.</li> <li>Filtra las ventas dentro del rango de fechas especificado.</li> <li>Encuentra los productos que no aparecen en las ventas del rango de fechas.</li> </ul> </li> <li>Resultado esperado: Una lista de productos que no tuvieron ninguna venta en el rango de fechas dado.</li> </ul> </li> </ol> <p>Para cada ejercicio, proporciona el c\u00f3digo de Pig que hayas utilizado y una captura de pantalla o salida de los resultados obtenidos. Aseg\u00farate de que el c\u00f3digo est\u00e9 documentado y se entienda claramente.</p>"},{"location":"ut3-ecosistema-hadoop/tareas/pig/3_peliculas/","title":"Archivo CSV: <code>peliculas_streaming.csv</code>","text":"<pre><code>id_pelicula,titulo,genero,anio,duracion_min,puntuacion,vistas\n1,Inception,Ciencia Ficci\u00f3n,2010,148,8.8,2000000\n2,The Dark Knight,Acci\u00f3n,2008,152,9.0,2500000\n3,Interstellar,Ciencia Ficci\u00f3n,2014,169,8.6,1800000\n4,Parasite,Drama,2019,132,8.6,3000000\n5,Avengers: Endgame,Acci\u00f3n,2019,181,8.4,4000000\n6,La La Land,Musical,2016,128,8.0,1500000\n7,The Matrix,Ciencia Ficci\u00f3n,1999,136,8.7,2200000\n8,The Godfather,Drama,1972,175,9.2,3500000\n9,Spirited Away,Animaci\u00f3n,2001,125,8.6,1200000\n10,The Lion King,Animaci\u00f3n,1994,88,8.5,1800000\n</code></pre>"},{"location":"ut3-ecosistema-hadoop/tareas/pig/3_peliculas/#consultas-con-ejemplos-y-comandos-de-pig","title":"Consultas con ejemplos y comandos de Pig","text":""},{"location":"ut3-ecosistema-hadoop/tareas/pig/3_peliculas/#consulta-1-filtrar-por-alta-puntuacion-y-popularidad","title":"Consulta 1: Filtrar por alta puntuaci\u00f3n y popularidad","text":"<ul> <li>Consulta original: Encuentra las pel\u00edculas con una puntuaci\u00f3n mayor a 8.5 y m\u00e1s de 2 millones de vistas.  </li> <li>Ejemplo similar: En un archivo de canciones de Spotify con columnas <code>id_cancion</code>, <code>titulo</code>, <code>artista</code>, <code>duracion_seg</code>, <code>puntuacion</code>, <code>reproducciones</code>, filtra las canciones con una puntuaci\u00f3n mayor a 4.5 y m\u00e1s de 1 mill\u00f3n de reproducciones. <pre><code>canciones = LOAD 'canciones_spotify.csv' USING PigStorage(',') \n            AS (id_cancion:INT, titulo:CHARARRAY, artista:CHARARRAY, duracion_seg:INT, puntuacion:FLOAT, reproducciones:INT);\nfiltradas = FILTER canciones BY puntuacion &gt; 4.5 AND reproducciones &gt; 1000000;\nDUMP filtradas;\n</code></pre></li> </ul>"},{"location":"ut3-ecosistema-hadoop/tareas/pig/3_peliculas/#consulta-2-duracion-promedio-por-genero","title":"Consulta 2: Duraci\u00f3n promedio por g\u00e9nero","text":"<ul> <li>Consulta original: Calcula la duraci\u00f3n promedio de las pel\u00edculas agrupadas por su g\u00e9nero.  </li> <li>Ejemplo similar: En un archivo de podcasts con columnas <code>id_podcast</code>, <code>titulo</code>, <code>categoria</code>, <code>duracion_min</code>, <code>puntuacion</code> y <code>oyentes</code>, agrupa por categor\u00eda y calcula la duraci\u00f3n promedio de los episodios. <pre><code>podcasts = LOAD 'podcasts.csv' USING PigStorage(',') \n           AS (id_podcast:INT, titulo:CHARARRAY, categoria:CHARARRAY, duracion_min:INT, puntuacion:FLOAT, oyentes:INT);\nagrupados = GROUP podcasts BY categoria;\npromedio_duracion = FOREACH agrupados GENERATE group AS categoria, AVG(podcasts.duracion_min) AS duracion_promedio;\nDUMP promedio_duracion;\n</code></pre></li> </ul>"},{"location":"ut3-ecosistema-hadoop/tareas/pig/3_peliculas/#consulta-3-top-5-de-canciones-mas-reproducidas","title":"Consulta 3: Top 5 de canciones m\u00e1s reproducidas","text":"<ul> <li>Consulta original: Lista las 5 pel\u00edculas con mayor cantidad de vistas, ordenadas de forma descendente.  </li> <li>Ejemplo similar: En un archivo de canciones de Spotify, encuentra las 5 canciones con m\u00e1s reproducciones. <pre><code>canciones = LOAD 'canciones_spotify.csv' USING PigStorage(',') \n            AS (id_cancion:INT, titulo:CHARARRAY, artista:CHARARRAY, duracion_seg:INT, puntuacion:FLOAT, reproducciones:INT);\nordenadas = ORDER canciones BY reproducciones DESC;\ntop_5 = LIMIT ordenadas 5;\nDUMP top_5;\n</code></pre></li> </ul>"},{"location":"ut3-ecosistema-hadoop/tareas/pig/3_peliculas/#consulta-4-analisis-por-ano","title":"Consulta 4: An\u00e1lisis por a\u00f1o","text":"<ul> <li>Consulta original: Agrupa las pel\u00edculas por a\u00f1o de estreno y calcula estad\u00edsticas.  </li> <li>Ejemplo similar: En un archivo de videojuegos con columnas <code>id_juego</code>, <code>titulo</code>, <code>genero</code>, <code>anio_lanzamiento</code>, <code>ventas</code> y <code>calificacion</code>, agrupa por a\u00f1o de lanzamiento y calcula:  <ul> <li>El n\u00famero total de juegos por a\u00f1o.  </li> <li>El promedio de calificaci\u00f3n.  </li> <li>El juego m\u00e1s vendido de cada a\u00f1o. <pre><code>videojuegos = LOAD 'videojuegos.csv' USING PigStorage(',') \n              AS (id_juego:INT, titulo:CHARARRAY, genero:CHARARRAY, anio_lanzamiento:INT, ventas:INT, calificacion:FLOAT);\nagrupados_anio = GROUP videojuegos BY anio_lanzamiento;\nestadisticas = FOREACH agrupados_anio GENERATE \n               group AS anio, \n               COUNT(videojuegos) AS total_juegos,\n               AVG(videojuegos.calificacion) AS promedio_calificacion,\n               MAX(videojuegos.ventas) AS ventas_maximas;\nDUMP estadisticas;\n</code></pre></li> </ul> </li> </ul>"},{"location":"ut3-ecosistema-hadoop/tareas/pig/3_peliculas/#consulta-5-comparativa-entre-generos","title":"Consulta 5: Comparativa entre g\u00e9neros","text":"<ul> <li>Consulta original: Calcula el total de vistas acumuladas para cada g\u00e9nero y ord\u00e9nalos de mayor a menor.  </li> <li>Ejemplo similar: En un archivo de aplicaciones m\u00f3viles con columnas <code>id_app</code>, <code>nombre</code>, <code>categoria</code>, <code>descargas</code>, calcula las descargas totales por cada categor\u00eda y ord\u00e9nalas en orden descendente. <pre><code>apps = LOAD 'apps.csv' USING PigStorage(',') \n       AS (id_app:INT, nombre:CHARARRAY, categoria:CHARARRAY, descargas:INT);\nagrupados_categoria = GROUP apps BY categoria;\ndescargas_totales = FOREACH agrupados_categoria GENERATE group AS categoria, SUM(apps.descargas) AS total_descargas;\nordenadas = ORDER descargas_totales BY total_descargas DESC;\nDUMP ordenadas;\n</code></pre></li> </ul>"},{"location":"ut3-ecosistema-hadoop/tareas/pig/3_peliculas/#consulta-6-distribucion-de-peliculas-por-duracion","title":"Consulta 6: Distribuci\u00f3n de pel\u00edculas por duraci\u00f3n","text":"<ul> <li>Consulta original: Clasifica las pel\u00edculas seg\u00fan su duraci\u00f3n.  </li> <li>Ejemplo similar: En un archivo de videos de YouTube con columnas <code>id_video</code>, <code>titulo</code>, <code>duracion_min</code>, clasifica los videos en:  <ul> <li>Cortos: Menos de 5 minutos.  </li> <li>Medios: Entre 5 y 20 minutos.  </li> <li>Largos: M\u00e1s de 20 minutos. <pre><code>videos = LOAD 'videos_youtube.csv' USING PigStorage(',') \n         AS (id_video:INT, titulo:CHARARRAY, duracion_min:INT);\nclasificados = FOREACH videos GENERATE \n               id_video, \n               titulo, \n               (CASE \n                 WHEN duracion_min &lt; 5 THEN 'Corto'\n                 WHEN duracion_min &gt;= 5 AND duracion_min &lt;= 20 THEN 'Medio'\n                 ELSE 'Largo' \n               END) AS categoria_duracion;\nagrupados_duracion = GROUP clasificados BY categoria_duracion;\nconteo = FOREACH agrupados_duracion GENERATE group AS categoria, COUNT(clasificados) AS total_videos;\nDUMP conteo;\n</code></pre></li> </ul> </li> </ul>"},{"location":"ut3-ecosistema-hadoop/tareas/pig/4_csv_eleccion/","title":"Enunciado de la Tarea: An\u00e1lisis de Datos con Apache Pig","text":""},{"location":"ut3-ecosistema-hadoop/tareas/pig/4_csv_eleccion/#objetivo","title":"Objetivo","text":"<p>El objetivo de esta tarea es que el estudiante practique las operaciones de carga, filtrado, agrupaci\u00f3n y ordenaci\u00f3n de datos utilizando Apache Pig, aplicando dichas t\u00e9cnicas a un dataset real extra\u00eddo de Kaggle.  </p>"},{"location":"ut3-ecosistema-hadoop/tareas/pig/4_csv_eleccion/#instrucciones","title":"Instrucciones","text":"<ol> <li> <p>B\u00fasqueda de un Dataset en Kaggle </p> <ul> <li>Busca un dataset en Kaggle relacionado con un tema de inter\u00e9s personal (por ejemplo, pel\u00edculas, deportes, tecnolog\u00eda, econom\u00eda, etc.).  </li> <li>Aseg\u00farate de que el dataset:<ul> <li>Contenga al menos 5 columnas relevantes para el an\u00e1lisis.</li> <li>Posea un tama\u00f1o manejable (menos de 100 MB).  </li> </ul> </li> <li>Descarga el archivo del dataset en formato CSV.  </li> </ul> </li> <li> <p>Consultas en Apache Pig     Realiza las siguientes operaciones sobre el dataset:  </p> <p>a. Carga del Dataset </p> <ul> <li>Escribe el script Pig para cargar el archivo CSV, especificando correctamente los tipos de datos y el delimitador del archivo.  </li> </ul> <p>b. Filtrados </p> <ul> <li> <p>Realiza al menos dos filtros sobre los datos. Por ejemplo:</p> <ul> <li>Filtrar filas que cumplan una condici\u00f3n espec\u00edfica (como valores mayores a un umbral o categor\u00edas espec\u00edficas).  </li> <li>Excluir valores nulos o irrelevantes.  </li> </ul> </li> </ul> <p>c. Agrupaciones </p> <ul> <li>Agrupa los datos por una columna de tu elecci\u00f3n (por ejemplo, g\u00e9nero, categor\u00eda, a\u00f1o) y realiza una operaci\u00f3n de agregaci\u00f3n (como <code>COUNT</code>, <code>AVG</code> o <code>MAX</code>).  </li> </ul> <p>d. Ordenaciones </p> <ul> <li>Ordena los resultados obtenidos por alguna m\u00e9trica de inter\u00e9s (por ejemplo, puntuaciones, cantidad de elementos, promedios).  </li> </ul> <p>e. Exportaci\u00f3n de Resultados </p> <ul> <li>Guarda los resultados de al menos una de las consultas en un archivo CSV usando la instrucci\u00f3n <code>STORE</code>.  </li> </ul> </li> </ol>"}]}